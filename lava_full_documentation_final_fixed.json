[
  {
    "url": "https://lava-nc.org/",
    "title": "Lava Software Framework — Lava  documentation",
    "content": "A software framework for neuromorphic computing\n\nLava is an open-source software framework for developing neuro-inspired applications and mapping them to neuromorphic hardware. Lava provides developers with the tools and abstractions to develop applications that fully exploit the principles of neural computation.  Constrained in this way, like the brain, Lava applications allow neuromorphic platforms to intelligently process, learn from, and respond to real-world data with great gains in energy efficiency and speed compared to conventional computer architectures.\n\nThe vision behind Lava is an open, community-developed code base that unites the full range of approaches pursued by the neuromorphic computing community. It provides a modular, composable, and extensible structure for researchers to integrate their best ideas into a growing algorithms library, while introducing new abstractions that allow others to build on those ideas without having to reinvent them.\n\nFor this purpose, Lava allows developers to define versatileprocessessuch as individual neurons, neural networks, conventionally coded programs, interfaces to peripheral devices, and bridges to other software frameworks. Lava allows collections of these processes to be encapsulated into modules and aggregated to form complex neuromorphic applications.  Communication between Lava processes uses event-based message passing, where messages can range from binary spikes to kilobyte-sized packets.\n\nThe behavior of Lava processes is defined by one or moreimplementation models, where different models may be specified for different execution platforms (“backends”), different degrees of precision, and for high-level algorithmic modeling purposes.  For example, an excitatory/inhibitory neural network process may have different implementation models for an analog neuromorphic chip compared to a digital neuromorphic chip, but the two models could share a common “E/I” process definition with each model’s implementations determined by common input parameters.\n\nLava is platform-agnostic so that applications can be prototyped on conventional CPUs/GPUs and deployed to heterogeneous system architectures spanning both conventional processors as well as a range of neuromorphic chips such as Intel’s Loihi. To compile and execute processes for different backends, Lava builds on a low-level interface calledMagmawith a powerful compiler and runtime library. Over time, the Lava developer community may enhance Magma to target additional neuromorphic platforms beyond its initial support for Intel’s Loihi chips.\n\nThe Lava framework currently supports (to be released soon):\n\nChannel-based message passing between asynchronous processes (the Communicating Sequential Processes paradigm)\n\nHyper-granular parallelism where computation emerges as the collective result of inter-process interactions\n\nHeterogeneous execution platforms with both conventional and neuromorphic components\n\nOffline backprop-based training of a wide range of neuron models and network topologies\n\nTools for generating complex spiking neural networks such asdynamic neural fieldsand networks that solve well-defined optimization problems\n\nIntegration with third-party frameworks\n\nFor maximum developer productivity, Lava blends a simple Python Interface with accelerated performance using underlying C/C++/CUDA code.\n\nFor more information, visit Lava on Github:[https://github.com/lava-nc](https://github.com/lava-nc)https://github.com/lava-nc\n\nProcesses are the fundamental building block in the Lava architecture from which all algorithms and applications are built. Processes are stateful objects with internal variables, input and output ports for message-based communication via channels and multiple behavioral models. This architecture is inspired from the Communicating Sequential Process (CSP) paradigm for asynchronous, parallel systems that interact via message passing. Lava processes implementing the CSP API can be compiled and executed via a cross-platform compiler and runtime that support execution on neuromorphic and conventional von-Neumann HW. Together, these components form the low-level Magma layer of Lava.\n\nAt a higher level, the process library contains a growing set of generic processes that implement various kinds of neuron models, neural network connection topologies, IO processes, etc. These execute on either CPU, GPU or neuromorphic HW such as Intel’s Loihi architecture.\n\nVarious algorithm and application libraries build on these these generic processes to create specialized processes and provide tools to train or configure processes for more advanced applications. A deep learning library, constrained optimization library, and dynamic neural field library are among the first to be released in Lava, with more libraries to come in future releases.\n\nLava is open to modification and extension to third-party libraries like Nengo, ROS, YARP and others. Additional utilities also allow users to profile power and performance of workloads, visualize complex networks, or help with the float to fixed point conversions required for many low-precision devices such as neuromorphic HW.\n\nAll of Lava’s core APIs and higher-level components are released, by default, with permissive BSD 3 licenses in order to encourage the broadest possible community contribution.  Lower-level Magma components needed for mapping processes to neuromorphic backends are generally released with more restrictive LGPL-2.1 licensing to discourage commercial proprietary forks of these technologies.  The specific components of Magma needed to compile processes specifically to Intel Loihi chips remains proprietary to Intel and is not provided through this GitHub site (see below).  Similar Magma-layer code for other future commercial neuromorphic platforms likely will also remain proprietary.\n\nA process has input and output ports to interact with other processes, internal variables may have different behavioral implementations in different programming languages or for different HW platforms.\n\nProcess models are used to provide different behavioral models of a process. This Python model implements the LIF process, the Loihi synchronization protocol and requires a CPU compute resource to run.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please[subscribe to our newsletter](http://eepurl.com/hJCyhb)subscribe to our newsletter.\n\n[Index](https://lava-nc.org/genindex.html)Index\n\n[Module Index](https://lava-nc.org/py-modindex.html)Module Index\n\n[Search Page](https://lava-nc.org/search.html)Search Page\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n# Instantiate Lava processes to build networkfromlava.proc.dense.processimportDensefromlava.proc.lif.processimportLIFlif1=LIF()dense=Dense()lif2=LIF()# Connect processes via their directional input and output portslif1.out_ports.s_out.connect(self.dense.in_ports.s_in)dense.out_ports.a_out.connect(self.lif2.in_ports.a_in)# Execute process lif1 and all processes connected to it for fixed number of stepsfromlava.magma.core.run_conditionsimportRunStepsfromlava.magma.core.run_configsimportRunConfiglif1.run(condition=RunSteps(num_steps=10),run_cfg=SimpleRunConfig(sync_domains=[]))lif1.stop()\n``````\n\n``````\nfromlava.magma.core.process.processimportAbstractProcessfromlava.magma.core.process.variableimportVarfromlava.magma.core.process.ports.portsimportInPort,OutPortclassLIF(AbstractProcess):\"\"\"Leaky-Integrate-and-Fire neural process with activation input and spikeoutput ports a_in and s_out.Realizes the following abstract behavior:u[t] = u[t-1] * (1-du) + a_inv[t] = v[t-1] * (1-dv) + u[t] + biass_out = v[t] > vthv[t] = v[t] - s_out*vth\"\"\"def__init__(self,**kwargs):super().__init__(**kwargs)shape=kwargs.get(\"shape\",(1,))self.a_in=InPort(shape=shape)self.s_out=OutPort(shape=shape)self.u=Var(shape=shape,init=0)self.v=Var(shape=shape,init=0)self.du=Var(shape=(1,),init=kwargs.pop(\"du\",0))self.dv=Var(shape=(1,),init=kwargs.pop(\"dv\",0))self.bias=Var(shape=shape,init=kwargs.pop(\"b\",0))self.vth=Var(shape=(1,),init=kwargs.pop(\"vth\",10))\n``````\n\n``````\nimportnumpyasnpfromlava.magma.core.sync.protocols.loihi_protocolimportLoihiProtocolfromlava.magma.core.model.py.portsimportPyInPort,PyOutPortfromlava.magma.core.model.py.typeimportLavaPyTypefromlava.magma.core.resourcesimportCPUfromlava.magma.core.decoratorimportimplements,requiresfromlava.magma.core.model.py.modelimportPyLoihiProcessModelfromlava.proc.lif.processimportLIF@implements(proc=LIF,protocol=LoihiProtocol)@requires(CPU)classPyLifModel(PyLoihiProcessModel):a_in:PyInPort=LavaPyType(PyInPort.VEC_DENSE,np.int16,precision=16)s_out:PyOutPort=LavaPyType(PyOutPort.VEC_DENSE,bool,precision=1)u:np.ndarray=LavaPyType(np.ndarray,np.int32,precision=24)v:np.ndarray=LavaPyType(np.ndarray,np.int32,precision=24)bias:np.ndarray=LavaPyType(np.ndarray,np.int16,precision=12)du:int=LavaPyType(int,np.uint16,precision=12)dv:int=LavaPyType(int,np.uint16,precision=12)vth:int=LavaPyType(int,int,precision=8)defrun_spk(self):self.u[:]=self.u*((2**12-self.du)//2**12)a_in_data=self.a_in.recv()self.u[:]+=a_in_dataself.v[:]=self.v*\\((2**12-self.dv)//2**12)+self.u+self.biass_out=self.v>self.vthself.v[s_out]=0# Reset voltage to 0. This is Loihi-1 compatible.self.s_out.send(s_out)\n``````"
  },
  {
    "url": "https://lava-nc.org/%20%22Lava%20Documentation%22",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/_sources/developer_guide.rst.txt",
    "title": "No Title",
    "content": "🛠️ Code Snippets:\n\n``````\n# Sign your commit and add a commit summary\n       git commit -sm \"\"\n\n- Push this branch to your fork\n\n    .. code-block:: bash\n\n       # If you cloned\n       git push -u origin# If you added remote lava-fork\n       git push -u lava-fork- `Open a pull request`_ to `lava-nc/lava main`_\n   \n  - Add a descriptive pull request title\n  - Describe the pull request in detail in the body\n  - Link an issue to the pull request\n  - Add a pull request label:\n\n  - :docs:`documentation`\n  - :enhance:`enhancement`\n  - :bug:`bug`\n\n- Add a review `label`_:\n  \n  - :needsr:`needs-review`\n\n- Work with committers on your code review\n  \n  - Answer any questions or comments\n  - Make updates as required\n  - Meet code requirements before merge\n\n    |        - Unit tests cover the pull request\n    |        - Code contains class and method doc strings\n    |        - The build and tests on the pull request pass\n    |        - Style Guidelines are met\n- Required before merge: 2 Code Reviews and the approval of a committer\n\nCoding Conventions\n******************\n\nCode Requirements\n=================\n- Code must be **styled** according to `PEP8`_.\n\n  - Line limit is 80 characters\n  - Use short descriptive variable & function names\n\n- Code must use **docstrings**:\n\n  - module docstring\n  - class docstring\n  - method docstring\n  - See :ref:`docstring format`\n\n- Code should be developed using TDD (Test Driven Development)\n\n  - Descriptive **unit tests** are *required*\n\n    - Tests prove your code works\n    - Tests help keep your code working when other contribute\n    - Write descriptive unit tests that explain the intent of the test\n    - Write minimal unit tests for just the feature you want to test\n\n  - Unit tests must cover the code in each pull request\n\n- All continuous integration checks must **pass** before pull requests are merged\n- Code must be reviewed twice and merged by a :ref:`committer`\n\nGuidelines\n==========\n- Before you embark on a big coding project, document it with an :ref:`issue` and discuss it with others including Lava Committers.\n- Use consistent :ref:`numpy docstring format`\n- Strive for a 100% linter score\n- Prefer short yet descriptive variable and function names\n- The more global a variable or function name the longer it may be. The more local the shorter the name should be\n- When something breaks, many tests may fail. But don't be overwhelmed. Fix the lowest level unit tests first. Chances are good these will also fix higher level unit tests.\n- Document everything\n\n  - module doc string\n  - class doc string\n  - method docstring\n\nDocstring Format\n===============\n\n.. code-block:: python\n\n   # Use numpy-style docstring formatting: https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard\n   def function(self, arg1, arg2):\n   \"\"\"Parameters\n   ----------\n   arg1\n   arg2\n\n   Returns\n   -------\n\n   \"\"\"\n\nContributors\n************\n\nContributor\n===========\nA contributor is someone who contributes to the Lava Community. Contributions can take many forms and include:\n\n- Create and update documentation\n- Contribute code\n- Contribute reviews of code and issues\n- Rate, comment on and give \"thumbs up\" on issues, pull requests etc.\n\nCommitter\n=========\nA Committer is a contributor who has the authority, and responsibility to review code and merge pull requests in the Lava Community. Committers are the leaders of the Lava Project and they have the following responsibilities:\n\n- Plan and decide the direction of the Lava Project\n- Facilitate community meetings and communication\n- Mentor new developers and community members\n- Review issues\n- Review pull requests, enforce requirements, merge code\n- Keep CI working\n\nList of lava-nc/lava Project Committers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n- `awintel`_\n- `joyeshmishra`_\n- `ysingh7`_\n- `mgkwill`_\n\nList of lava-nc/lava-dnf Project Committers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n- `mathisrichter`_\n- `awintel`_\n- `joyeshmishra`_\n- `ysingh7`_\n- `mgkwill`_\n\nList of lava-nc/lava-optimization Project Committers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n- `GaboFGuerra`_\n- `awintel`_\n- `joyeshmishra`_\n- `ysingh7`_\n- `mgkwill`_\n\nList of lava-nc/lava-dl Project Committers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n- `srrisbud`_\n- `bamsumit`_\n- `awintel`_\n- `joyeshmishra`_\n- `ysingh7`_\n- `mgkwill`_\n\n\nCommitter Promotion\n~~~~~~~~~~~~~~~~~~~\nCommitter promotion is the responsibility of the current committers.\n\nA current committer can nominate a contributor to become a committer based on contributions to the Lava Community.\n\nContributions that qualify a contributor should include:\n\n- Contributing significant document creation and document updates\n- Contributing significant amounts of code and high value features\n- Contributing reviews of code and issues\n- Facilitation of community planning, communication, and infrastructure\n\nUpon nomination of a new committer, all current committers will vote on the new committer nomination.\n\n- A quorum of 80% of committers must vote for a valid election\n- A nominee must not receive any vetos\n\nRepository Structure\n********************\nLava directory structure:\n\n| lava/\n| ├── lava\n| ├── lava-dl\n| ├── lava-dnf\n| ├── lava-docs\n| └── lava-optimization\n\nlava-nc/lava\n============\n.. epigraph::\n   Core repository containing the runtime, compiler and API.\n\nlava-nc/lava-dnf\n================\n.. epigraph::\n   Lava Dynamic Neural Fields Library\n\nlava-nc/lava-dl\n===============\n.. epigraph::\n   Lava Deep Learning Library\n\nlava-nc/lava-optimization\n=========================\n.. epigraph::\n   Lava Neuromorphic Constraint Optimization Library\n\nlava-nc/lava-docs\n=================\n.. epigraph::\n   Lava Documentation\n\n\nCode of Conduct\n***************\nYour behavior contributes to a successful community. As such community members should observe the following behaviors:\n\n- Welcome others, use inclusive and positive language.\n- Be truthful and transparent.\n- Be respectful of difference in viewpoint and experience.\n- Work towards the community's best interest.\n- Show empathy towards others.\n- Accept constructive criticism.\n\nAll Lava spaces are **professional interaction spaces** and *prohibit inappropriate behavior* or any behavior that could reasonably be thought to be inappropriate.\n\nInappropriate behavior that is **intolerable** includes:\n\n- Harassment in any form.\n- Sexual language or images.\n- Without permission, sharing private information of another, i.e., electronic, or physical address.\n- Political attacks, derogatory or insulting comments.\n- Conduct which could reasonably be considered inappropriate for the forum in which it occurs.\n\nLicenses\n********\nLava is licensed as *BSD 3* or *LGPL 2.1+*. Specific components are licensed as follows:\n\n| lava-nc/lava/magma/core:            **BSD 3-Clause**\n| lava-nc/lava/magma/compiler:        **LGPL 2.1 or later**\n| lava-nc/lava/magma/runtime:         **LGPL 2.1 or later**\n|\n| lava-nc/lava/proc:                  **BSD 3-Clause**\n| lava-nc/lava/utils:                 **BSD 3-Clause**\n| lava-nc/lava/tutorials:             **BSD 3-Clause**\n|\n| lava-nc/lava-dl:                    **BSD 3-Clause**\n| lava-nc/lava-dnf:                   **BSD 3-Clause**\n| lava-nc/lava-optimization:          **BSD 3-Clause**\n\nGo to :ref:`how to apply a license` for more information on using a license in your contribution.\n``````"
  },
  {
    "url": "https://lava-nc.org/_sources/index.rst.txt",
    "title": "No Title",
    "content": "A software framework for neuromorphic computing"
  },
  {
    "url": "https://lava-nc.org/_sources/lava-lib-dl/slayer/notebooks/pilotnet/\\\"https:/commons.wikimedia.org/wiki/File:Muybridge_race_horse_animated.gif\\\"",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/_sources/lava-lib-dl/slayer/notebooks/pilotnet/\\\"https:/github.com/lhzlhz/PilotNet/tree/master/data/datasets\\\"",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/_sources/lava-lib-dl/slayer/slayer.rst.txt",
    "title": "No Title",
    "content": ".. raw:: html"
  },
  {
    "url": "https://lava-nc.org/algorithms.html",
    "title": "Algorithms and Application Libraries — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/developer_guide.html",
    "title": "Developer Guide — Lava  documentation",
    "content": "Welcome to the Lava Developers Guide! Lava is an open-source software framework and community for developing neuro-inspired applications and mapping them to neuromorphic hardware.\n\nLava is an open, community-developed code base.\n\nThe purpose of this document is to provide a guide for contributing to Lava and explain how the Lava Project operates. The Lava Project can only grow through the contributions and work of this community.\n\nThanks for your interest in contributing to Lava!\n\nThe initial code for the Lava project ([github.com/lava-nc](https://github.com/lava-nc)github.com/lava-nc) was seeded by Intel’s Neuromorphic Computing Lab (NCL), a group within Intel Labs. NCL researchers continue to actively manage and expand the functionality of Lava, with the goal of building an active community of contributors and users.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please[subscribe to our newsletter](http://eepurl.com/hJCyhb)subscribe to our newsletter.\n\nEmail the Intel Lava Team at:[lava@intel.com](mailto:lava%40intel.com)lava@intel.com\n\nOur development roadmap will be published soon.\n\nComponent\n\nHW support\n\nFeatures\n\nMagma\n\nCPU, GPU\n\nThe generic high-level and HW-agnostic API supports creation of processes that execute asynchronously, in parallel and communicate via messages over channels to enable algorithm and application development.\n\nCompiler and Runtime initially only support execution or simulation on CPU and GPU platform.\n\nA series of basic examples and tutorials explain Lava’s key architectural and usage concepts\n\nProcess library\n\nCPU, GPU\n\nInitially supports basic processes to create spiking neural networks with different neuron models, connection topologies and input/output processes.\n\nDeep Learning library\n\nCPU, GPU\n\nAllows for direct training of stateful and event-based spiking neural networks with backpropagation via SLAYER 2.0 as well as inference through Lava. Training and inference will initially only be supported on CPU/GPU HW.\n\nOptimization library\n\nCPU, GPU\n\nOffers a variety of constraint optimization solvers such as constraint satisfaction (CSP) or quadratic unconstraint binary optimization (QUBO) and more.\n\nDynamic Neural Field library\n\nCPU, GPU\n\nAllows to build neural attractor networks for working memory, decision making, basic neuronal representations, and learning.\n\nMagma and Process library\n\nLoihi 1, 2\n\nCompiler, Runtime and the process library will be upgraded to support Loihi 1 and 2 architectures.\n\nProfiler\n\nCPU, GPU\n\nEnables power and performance measurements on neuromorphic HW as well as the ability to simulate power and performance of neuromorphic HW on CPU/GPU platforms. Initially only CPU/GPU support will be available.\n\nDL, DNF and Optimization library\n\nLoihi 1, 2\n\nAll algorithm libraries will be upgraded to support and be properly tested on neuromorphic HW.\n\nContributions to Lava are made through pull requests from personal forks of Lava ([https://github.com/lava-nc](https://github.com/lava-nc)https://github.com/lava-nc) on Github.\nWe welcome contributions at all levels of the software stack:\n\nRuntime\n\nCompiler\n\nAPI\n\nNew Processes\n\nAlgorithm or application libraries built on top of Lava\n\nSeperate utilities\n\n3rd party interfaces\n\nBefore you submit a pull request, please create an[issue](https://github.com/lava-nc/lava/issues)issuethat describes why the pull request is needed.\nPlease link your pull request to the issue covering the request upon pull request creation.\n\nIf you find a bug or would like to add additional functionality to Lava follow the steps below to create an issue.\n\nNote\n\nThese instructions are written using lava-nc/lava repository, however they can be used with any of the lava-nc/<repo> repositories.\n\nOpen an[issue](https://github.com/lava-nc/lava/issues)issue\n\nAdd a descriptive title\n\nDescribe the issue in detail in the body\n\nAdd an issue type[label](https://github.com/lava-nc/lava/labels)label:\n\ndocumentation\n\nenhancement\n\nbug\n\nAdd a review[label](https://github.com/lava-nc/lava/labels)label:\n\nneeds-review\n\nThe issue will be reviewed by a lava committer\n\nPlease participate in the review\n\nRespond to any questions\n\nUpdate the issue with changes as requested\n\nThe issue will be triaged with labels based on status:\n\nreviewed-approved\n\nreviewed-declined\n\nreviewed-needs-work\n\nduplicate\n\ninvalid\n\nwontfix\n\n<release version>-target\n\nIf ‘reviewed-approved’ a label of ‘<release version>-target’ will be added\n\nBefore you send your pull requests follow these steps:\n\nRead theCode of Conduct\n\nCheck if your changes are consistent withCoding Conventions\n\n[Apply a license](https://lava-nc.org/developer_guide.html#add-a-license)Apply a licenseto your contributions\n\nRun[linting and unit tests](https://lava-nc.org/developer_guide.html#lint-unit-tests)linting and unit tests\n\nWarning\n\nCode submissions must be original source code written by you.\n\nFor full coverage of how to create a fork and work with it see[Github Fork Procedures](https://docs.github.com/en/github/collaborating-with-pull-requests)Github Fork Procedures\n\nNote\n\nThese instructions are written using lava-nc/lava repository, however they can be used with any of the lava-nc/<repo> repositories.\n\nFork[lava-nc/lava](https://github.com/lava-nc/lava)lava-nc/lava\n\nClick on the ‘Fork’ button in the upper right corner\n\nGet the code locally\n\nClone your fork to your local machine\n\nAlternatively add a remote from your local repository to your fork\n\nCreate a new descriptive branch\n\nWrite your code\n\nMake code changes\n\nRun linting and unit tests\n\nFix any issues flagged by linting and unit tests and check again to ensure the issues are resolved\n\nNote\n\nPlease include, at the top of each source file, a BSD 3 or LGPL 2.1+ License. Check with Lava Committers if you have a question about licenses.\n\nFor Lava code contributionsexceptinglava-nc/magma/compilerandlava-nc/magma/runtime, use BSD 3. Example Intel BSD 3 License:\n\nForlava-nc/magma/compilerandlava-nc/magma/runtimeuse either BSD 3 or LGPL 2.1+. Example Intel LGPL 2.1+ License:\n\nCommit code changes to your branch\n\nPush this branch to your fork\n\n[Open a pull request](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork)Open a pull requestto[lava-nc/lava main](https://github.com/lava-nc/lava/tree/main)lava-nc/lava main\n\nAdd a descriptive pull request title\n\nDescribe the pull request in detail in the body\n\nLink an issue to the pull request\n\nAdd a pull request label:\n\ndocumentation\n\nenhancement\n\nbug\n\nAdd a review[label](https://github.com/lava-nc/lava/labels)label:\n\nneeds-review\n\nWork with committers on your code review\n\nAnswer any questions or comments\n\nMake updates as required\n\nMeet code requirements before merge\n\nRequired before merge: 2 Code Reviews and the approval of a committer\n\nCode must bestyledaccording to[PEP8](https://www.python.org/dev/peps/pep-0008/)PEP8.\n\nLine limit is 80 characters\n\nUse short descriptive variable & function names\n\nCode must usedocstrings:\n\nmodule docstring\n\nclass docstring\n\nmethod docstring\n\nSeedocstring format\n\nCode should be developed using TDD (Test Driven Development)\n\nDescriptiveunit testsarerequired\n\nTests prove your code works\n\nTests help keep your code working when other contribute\n\nWrite descriptive unit tests that explain the intent of the test\n\nWrite minimal unit tests for just the feature you want to test\n\nUnit tests must cover the code in each pull request\n\nAll continuous integration checks mustpassbefore pull requests are merged\n\nCode must be reviewed twice and merged by acommitter\n\nBefore you embark on a big coding project, document it with anissueand discuss it with others including Lava Committers.\n\nUse consistentnumpy docstring format\n\nStrive for a 100% linter score\n\nPrefer short yet descriptive variable and function names\n\nThe more global a variable or function name the longer it may be. The more local the shorter the name should be\n\nWhen something breaks, many tests may fail. But don’t be overwhelmed. Fix the lowest level unit tests first. Chances are good these will also fix higher level unit tests.\n\nDocument everything\n\nmodule doc string\n\nclass doc string\n\nmethod docstring\n\nA contributor is someone who contributes to the Lava Community. Contributions can take many forms and include:\n\nCreate and update documentation\n\nContribute code\n\nContribute reviews of code and issues\n\nRate, comment on and give “thumbs up” on issues, pull requests etc.\n\nA Committer is a contributor who has the authority, and responsibility to review code and merge pull requests in the Lava Community. Committers are the leaders of the Lava Project and they have the following responsibilities:\n\nPlan and decide the direction of the Lava Project\n\nFacilitate community meetings and communication\n\nMentor new developers and community members\n\nReview issues\n\nReview pull requests, enforce requirements, merge code\n\nKeep CI working\n\n[awintel](https://github.com/awintel)awintel\n\n[joyeshmishra](https://github.com/joyeshmishra)joyeshmishra\n\n[ysingh7](https://github.com/ysingh7)ysingh7\n\n[mgkwill](https://github.com/mgkwill)mgkwill\n\n[mathisrichter](https://github.com/mathisrichter)mathisrichter\n\n[awintel](https://github.com/awintel)awintel\n\n[joyeshmishra](https://github.com/joyeshmishra)joyeshmishra\n\n[ysingh7](https://github.com/ysingh7)ysingh7\n\n[mgkwill](https://github.com/mgkwill)mgkwill\n\n[GaboFGuerra](https://github.com/GaboFGuerra)GaboFGuerra\n\n[awintel](https://github.com/awintel)awintel\n\n[joyeshmishra](https://github.com/joyeshmishra)joyeshmishra\n\n[ysingh7](https://github.com/ysingh7)ysingh7\n\n[mgkwill](https://github.com/mgkwill)mgkwill\n\n[srrisbud](https://github.com/srrisbud)srrisbud\n\n[bamsumit](https://github.com/bamsumit)bamsumit\n\n[awintel](https://github.com/awintel)awintel\n\n[joyeshmishra](https://github.com/joyeshmishra)joyeshmishra\n\n[ysingh7](https://github.com/ysingh7)ysingh7\n\n[mgkwill](https://github.com/mgkwill)mgkwill\n\nCommitter promotion is the responsibility of the current committers.\n\nA current committer can nominate a contributor to become a committer based on contributions to the Lava Community.\n\nContributions that qualify a contributor should include:\n\nContributing significant document creation and document updates\n\nContributing significant amounts of code and high value features\n\nContributing reviews of code and issues\n\nFacilitation of community planning, communication, and infrastructure\n\nUpon nomination of a new committer, all current committers will vote on the new committer nomination.\n\nA quorum of 80% of committers must vote for a valid election\n\nA nominee must not receive any vetos\n\nLava directory structure:\n\nCore repository containing the runtime, compiler and API.\n\nLava Dynamic Neural Fields Library\n\nLava Deep Learning Library\n\nLava Neuromorphic Constraint Optimization Library\n\nLava Documentation\n\nYour behavior contributes to a successful community. As such community members should observe the following behaviors:\n\nWelcome others, use inclusive and positive language.\n\nBe truthful and transparent.\n\nBe respectful of difference in viewpoint and experience.\n\nWork towards the community’s best interest.\n\nShow empathy towards others.\n\nAccept constructive criticism.\n\nAll Lava spaces areprofessional interaction spacesandprohibit inappropriate behavioror any behavior that could reasonably be thought to be inappropriate.\n\nInappropriate behavior that isintolerableincludes:\n\nHarassment in any form.\n\nSexual language or images.\n\nWithout permission, sharing private information of another, i.e., electronic, or physical address.\n\nPolitical attacks, derogatory or insulting comments.\n\nConduct which could reasonably be considered inappropriate for the forum in which it occurs.\n\nLava is licensed asBSD 3orLGPL 2.1+. Specific components are licensed as follows:\n\nGo to[how to apply a license](https://lava-nc.org/developer_guide.html#add-a-license)how to apply a licensefor more information on using a license in your contribution.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\ngitclonegit@github.com:<user-name>/lava.git\n``````\n\n``````\ngitremoteaddlava-forkgit@github.com:<user-name>/lava.git\n``````\n\n``````\ngitcheckout-b<branch-name>\n``````\n\n``````\n# Install poetrypipinstall\"poetry>=1.1.13\"poetryconfigvirtualenvs.in-projecttruepoetryinstall\npoetryshell# Run lintingflakeheavenlintsrc/lavatests# Run unit testspytest# Run Secuity Lintingbandit-rsrc/lava/.#### If security linting fails run bandit directly#### and format failuresbandit-rsrc/lava/.--formatcustom--msg-template'{abspath}:{line}: {test_id}[bandit]: {severity}: {msg}'\n``````\n\n``````\ngitadd<code># Sign your commit and add a commit summarygitcommit-sm\"<title-description>\"\n``````\n\n``````\n# If you clonedgitpush-uorigin<branch-name># If you added remote lava-forkgitpush-ulava-fork<branch-name>\n``````\n\n``````\n# Use numpy-style docstring formatting: https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standarddeffunction(self,arg1,arg2):\"\"\" <Short description><Optional: Detailed description>Parameters----------arg1arg2Returns-------\"\"\"\n``````"
  },
  {
    "url": "https://lava-nc.org/dl.html",
    "title": "Deep Learning — Lava  documentation",
    "content": "Lava-DL (`lava-dl`lava-dl) is a library of deep learning tools within Lava that\nsupport offline training, online training and inference methods for\nvarious Deep Event-Based Networks.\n\nThere are two main strategies for training Deep Event-Based Networks:direct trainingandANN to SNN converison.\n\nDirectly training the network utilizes the information of precise timing of events. Direct training is very accurate and results in efficient networks. However, directly training networks take a lot of time and resources.\n\nOn the other hand, ANN to SNN conversion is especially suitable for rate\ncoded SNNs where we can leverage the fast training of ANN. These\nconverted SNNs, however, require increased latency compared to directly\ntrained SNNs.\n\nLava-DL provides an improved version of[SLAYER](https://github.com/bamsumit/slayerPytorch)SLAYERfor direct\ntraining of deep event based networks and a new ANN-SNN accelerated\ntraining approach called[Bootstrap](https://lava-nc.org/lava-lib-dl/bootstrap/bootstrap.html)Bootstrapto mitigate high latency issue of conventional ANN-SNN methods for training Deep\nEvent-Based Networks.\n\nThe lava-dl training libraries are independent of the core lava library since Lava Processes cannot be trained directly at this point. Instead, lava-dl is first used to train the model which can then be converted to a network of Lava processes using the netx library using platform independent hdf5 network description.\n\nThe library presently consists of\n\n`lava.lib.dl.slayer`lava.lib.dl.slayerfor natively training Deep Event-Based\nNetworks.\n\n`lava.lib.dl.bootstrap`lava.lib.dl.bootstrapfor training rate coded SNNs.\n\n`lava.lib.dl.netx`lava.lib.dl.netxfor training and deployment of event-based deep neural networks on traditional as well as neuromorphic backends.\n\nLava-dl also has the following external, fully compatible, plugin.\n\n[lava.lib.dl.decolle](https://github.com/kclip/lava-decolle)lava.lib.dl.decollefor training Deep SNNs with local learning and surrogate gradients. This extension is an implementation of[DECOLLE](https://github.com/nmi-lab/decolle-public)DECOLLElearning repo to be fully compatible to lava-dl training tools. Refer[here](https://github.com/kclip/lava-decolle)herefor the detailed description of the extension, examples and tutorials.:\n\nJ . Kaiser, H. Mostafa, and E. Neftci,Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE).Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE).pp 424,  Frontiers in Neuroscience 2020.\n\nMore tools will be added in the future.\n\nTypical Lava-DL workflow consists of:\n\nTraining:using`lava.lib.dl.{slayer/bootstrap}`lava.lib.dl.{slayer/bootstrap}which results in ahdf5 network\ndescription. Training usually consists of iterative cycle of\narchitecture design, hyperparameter tuning, and backpropagation\ntraining.\n\nInference:using`lava.lib.dl.netx`lava.lib.dl.netxwhich generates\nlava proces from the hdf5 network description of the trained network and\nenables inference on different backends.\n\nEnd to end training tutorials\n\n[Oxford spike train regression](https://lava-nc.org/lava-lib-dl/slayer/notebooks/oxford/train.html)Oxford spike train regression\n\n[MNIST digit classification](https://lava-nc.org/lava-lib-dl/bootstrap/notebooks/mnist/train.html)MNIST digit classification\n\n[NMNIST digit classification](https://lava-nc.org/lava-lib-dl/slayer/notebooks/nmnist/train.html)NMNIST digit classification\n\n[PilotNet steering angle prediction](https://lava-nc.org/lava-lib-dl/slayer/notebooks/pilotnet/train.html)PilotNet steering angle prediction\n\nDeep dive tutorials\n\n[Dynamics and Neurons](https://lava-nc.org/lava-lib-dl/slayer/notebooks/neuron_dynamics/dynamics.html)Dynamics and Neurons\n\nInference tutorials\n\n[Oxford Inference](https://lava-nc.org/lava-lib-dl/netx/notebooks/oxford/run.html)Oxford Inference\n\n[PilotNet SNN Inference](https://lava-nc.org/lava-lib-dl/netx/notebooks/pilotnet_snn/run.html)PilotNet SNN Inference\n\n[PilotNet SDNN Inference](https://lava-nc.org/lava-lib-dl/netx/notebooks/pilotnet_sdnn/run.html)PilotNet SDNN Inference\n\nSLAYER 2.0 (lava.lib.dl.slayerlava.lib.dl.slayer) is an enhanced version of[SLAYER](https://github.com/bamsumit/slayerPytorch)SLAYER. Most noteworthy\nenhancements are: support forrecurrent network structures, a wider\nvariety ofneuron modelsandsynaptic connections(a complete list\nof features is[here](https://lava-nc.org/lava-lib-dl/slayer/slayer.html)here).\nThis version of SLAYER is built on top of the[PyTorch](https://pytorch.org/)PyTorchdeep learning framework, similar to\nits predecessor. For smooth integration with Lava,lava.lib.dl.slayerlava.lib.dl.slayersupports exporting trained models using the\nplatform independenthdf5 network exchangeformat.\n\nIn future versions, SLAYER will get completely integrated into Lava to\ntrain Lava Processes directly. This will eliminate the need for\nexplicitly exporting and importing the trained networks.\n\nImport modules\n\nNetwork Description\n\nTraining\n\nExport the network\n\nIn general ANN-SNN conversion methods for rate based SNN result in high latency of the network during inference. This is because the rate interpretation of a spiking neuron using ReLU acitvation unit breaks down for short inference times. As a result, the network requires many time steps per sample to achieve adequate inference results.\n\nBootstrap (lava.lib.dl.bootstraplava.lib.dl.bootstrap) enables rapid training of rate based SNNs by translating them to an equivalent dynamic ANN representation which leads to SNN performance close to the equivalent ANN and low latency inference. More details[here](https://lava-nc.org/lava-lib-dl/bootstrap/bootstrap.html)here. It also supportshybrid traininga mixed ANN-SNN network to minimize the ANN to SNN performance gap. This method is independent of the SNN model being used.\n\nIt has similar API aslava.lib.dl.slayerlava.lib.dl.slayerand supports exporting\ntrained models using the platform independenthdf5 network exchangeformat.\n\nImport modules\n\nNetwork Description\n\nTraining\n\nExport the network\n\nFor inference using Lava, Network Exchange Library (lava.lib.dl.netxlava.lib.dl.netx) provides an\nautomated API for loading SLAYER-trained models as Lava Processes, which\ncan be directly run on a desired backend.`lava.lib.dl.netx`lava.lib.dl.netximports\nmodels saved via SLAYER using the hdf5 network exchange format. The\ndetails of hdf5 network description specification can be found[here](https://lava-nc.org/lava-lib-dl/netx/netx.html)here.\n\nImport modules\n\nLoad the trained network\n\nAttach Processes for Input Injection and Output Readout\n\nRun the network\n\nDetailed description:\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nimportlava.lib.dl.slayerasslayer\n``````\n\n``````\n# like any standard pyTorch networkclassNetwork(torch.nn.Module):def__init__(self):...self.blocks=torch.nn.ModuleList([# sequential network blocksslayer.block.sigma_delta.Input(sdnn_params),slayer.block.sigma_delta.Conv(sdnn_params,3,24,3),slayer.block.sigma_delta.Conv(sdnn_params,24,36,3),slayer.block.rf_iz.Conv(rf_params,36,64,3,delay=True),slayer.block.rf_iz.Conv(sdnn_cnn_params,64,64,3,delay=True),slayer.block.rf_iz.Flatten(),slayer.block.alif.Dense(alif_params,64*40,100,delay=True),slayer.block.cuba.Recurrent(cuba_params,100,50),slayer.block.cuba.KWTA(cuba_params,50,50,num_winners=5)])defforward(self,x):forblockinself.blocks:# forward computation is as simple as calling the blocks in a loopx=block(x)returnxdefexport_hdf5(self,filename):# network export to hdf5 formath=h5py.File(filename,'w')layer=h.create_group('layer')fori,binenumerate(self.blocks):b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\nnet=Network()assistant=slayer.utils.Assistant(net,error,optimizer,stats)...forepochinrange(epochs):fori,(input,ground_truth)inenumerate(train_loader):output=assistant.train(input,ground_truth)...fori,(input,ground_truth)inenumerate(test_loader):output=assistant.test(input,ground_truth)...\n``````\n\n``````\nnet.export_hdf5('network.net')\n``````\n\n``````\nimportlava.lib.dl.bootstrapasbootstrap\n``````\n\n``````\n# like any standard pyTorch networkclassNetwork(torch.nn.Module):def__init__(self):...self.blocks=torch.nn.ModuleList([# sequential network blocksbootstrap.block.cuba.Input(sdnn_params),bootstrap.block.cuba.Conv(sdnn_params,3,24,3),bootstrap.block.cuba.Conv(sdnn_params,24,36,3),bootstrap.block.cuba.Conv(rf_params,36,64,3),bootstrap.block.cuba.Conv(sdnn_cnn_params,64,64,3),bootstrap.block.cuba.Flatten(),bootstrap.block.cuba.Dense(alif_params,64*40,100),bootstrap.block.cuba.Dense(cuba_params,100,10),])defforward(self,x,mode):...forblock,minzip(self.blocks,mode):x=block(x,mode=m)returnxdefexport_hdf5(self,filename):# network export to hdf5 formath=h5py.File(filename,'w')layer=h.create_group('layer')fori,binenumerate(self.blocks):b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\nnet=Network()scheduler=bootstrap.routine.Scheduler()...forepochinrange(epochs):fori,(input,ground_truth)inenumerate(train_loader):mode=scheduler.mode(epoch,i,net.training)output=net.forward(input,mode)...loss.backward()fori,(input,ground_truth)inenumerate(test_loader):mode=scheduler.mode(epoch,i,net.training)output=net.forward(input,mode)...\n``````\n\n``````\nnet.export_hdf5('network.net')\n``````\n\n``````\nfromlava.lib.dl.netximporthdf5\n``````\n\n``````\n# Import the model as a Lava Processnet=hdf5.Network(net_config='network.net')\n``````\n\n``````\nfromlava.proc.ioimportInputLoader,BiasWriter,OutputReader# Instantiate the processesinput_loader=InputLoader(dataset=testing_set)bias_writer=BiasWriter(shape=input_shape)output=OutputReader()# Connect the input to the network:input_loader.data_out.connect(bias_writer.bias_in)bias_writer.bias_out.connect(net.in_layer.bias)# Connect network-output to the output processnet.out_layer.neuron.s_out.connect(output.net_output_in)fromlava.procimportio# Instantiate the processesdataloader=io.dataloader.SpikeDataloader(dataset=test_set)output_logger=io.sink.RingBuffer(shape=net.out_layer.shape,buffer=num_steps)gt_logger=io.sink.RingBuffer(shape=(1,),buffer=num_steps)# Connect the input to the network:dataloader.ground_truth.connect(gt_logger.a_in)dataloader.s_out.connect(net.in_layer.neuron.a_in)# Connect network-output to the output processnet.out_layer.out.connect(output_logger.a_in)\n``````\n\n``````\nfromlava.magmaimportrun_configsasrcfgfromlava.magmaimportrun_conditionsasrcndnet.run(condition=rcnd.RunSteps(total_run_time),run_cfg=rcfg.Loihi1SimCfg())\n``````"
  },
  {
    "url": "https://lava-nc.org/dnf.html",
    "title": "Dynamic Neural Fields — Lava  documentation",
    "content": "Dynamic Neural Fields (DNF) are neural attractor networks that generate\nstabilized activity patterns in recurrently connected populations of neurons.\nThese activity patterns form the basis of neural representations, decision\nmaking, working memory, and learning. DNFs are the fundamental\nbuilding block of[dynamic field theory](https://dynamicfieldtheory.org)dynamic field theorya mathematical and conceptual framework for modeling cognitive processes in\na closed behavioral loop.\n\nVoltage of a selective dynamic neural field tracking moving input\n\nlava-dnf is a library within the Lava software framework. The main building\nblocks in Lava are processes. lava-dnf provides\nprocesses and other software infrastructure to build architectures composed of\nDNFs. In particular, it provides functions that generate connectivity patterns\ncommon to DNF architectures.\n\nThe primary focus of lava-dnf today is on robotic applications: sensing and\nperception, motion control, behavioral organization, map formation, and\nautonomous (continual) learning. Neuromorphic hardware provides significant\ngains in both processing speed and energy efficiency compared to conventional\nimplementations of DNFs on a CPU or GPU (e.g., using[cedar](https://cedar.ini.rub.de)cedaror[cosivina](https://github.com/cosivina)cosivina).\n\nBuilding DNF architectures\n\nBased on spiking neurons\n\nDNF dimensionality support for 0D, 1D, 2D, and 3D\n\nRecurrent connectivity based on kernel functions\n\nForward connectivity to connect multiple DNFs\n\nStructured input from spike generators\n\nRunning DNF architectures\n\nOn CPU (Python simulation)\n\nOn Loihi 2\n\nExamples demonstrating basic DNF regimes and instabilities\n\nDetection of input\n\nSelection of input\n\nWorking memory of input\n\nRelational networks\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nfromlava.proc.lif.processimportLIFfromlava.lib.dnf.kernels.kernelsimportSelectiveKernelfromlava.lib.dnf.connect.connectimportconnectfromlava.lib.dnf.operations.operationsimportConvolution# Create a population of 20x20 spiking neurons.dnf=LIF(shape=(20,20))# Create a selective kernel.kernel=SelectiveKernel(amp_exc=18,width_exc=[4,4],global_inh=-15)# Apply the kernel to the population to create a DNF with a selective# regime.connect(dnf.s_out,dnf.a_in,[Convolution(kernel)])\n``````"
  },
  {
    "url": "https://lava-nc.org/genindex.html",
    "title": "Index — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/getting_started_with_lava.html",
    "title": "Getting Started with Lava — Lava  documentation",
    "content": "This guide to programming Lava will provide a growing collection of learning resources to help you become a Lava developer! It will cover all aspects of Lava enabling you to create, compile, execute and understand LavaProcesses. Review the[Lava Architecture](https://lava-nc.org/lava_architecture_overview.html#lava-architecture)Lava Architecturesection for an introduction to the fundamental architectural concepts of Lava.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/index.html",
    "title": "Lava Software Framework — Lava  documentation",
    "content": "A software framework for neuromorphic computing\n\nLava is an open-source software framework for developing neuro-inspired applications and mapping them to neuromorphic hardware. Lava provides developers with the tools and abstractions to develop applications that fully exploit the principles of neural computation.  Constrained in this way, like the brain, Lava applications allow neuromorphic platforms to intelligently process, learn from, and respond to real-world data with great gains in energy efficiency and speed compared to conventional computer architectures.\n\nThe vision behind Lava is an open, community-developed code base that unites the full range of approaches pursued by the neuromorphic computing community. It provides a modular, composable, and extensible structure for researchers to integrate their best ideas into a growing algorithms library, while introducing new abstractions that allow others to build on those ideas without having to reinvent them.\n\nFor this purpose, Lava allows developers to define versatileprocessessuch as individual neurons, neural networks, conventionally coded programs, interfaces to peripheral devices, and bridges to other software frameworks. Lava allows collections of these processes to be encapsulated into modules and aggregated to form complex neuromorphic applications.  Communication between Lava processes uses event-based message passing, where messages can range from binary spikes to kilobyte-sized packets.\n\nThe behavior of Lava processes is defined by one or moreimplementation models, where different models may be specified for different execution platforms (“backends”), different degrees of precision, and for high-level algorithmic modeling purposes.  For example, an excitatory/inhibitory neural network process may have different implementation models for an analog neuromorphic chip compared to a digital neuromorphic chip, but the two models could share a common “E/I” process definition with each model’s implementations determined by common input parameters.\n\nLava is platform-agnostic so that applications can be prototyped on conventional CPUs/GPUs and deployed to heterogeneous system architectures spanning both conventional processors as well as a range of neuromorphic chips such as Intel’s Loihi. To compile and execute processes for different backends, Lava builds on a low-level interface calledMagmawith a powerful compiler and runtime library. Over time, the Lava developer community may enhance Magma to target additional neuromorphic platforms beyond its initial support for Intel’s Loihi chips.\n\nThe Lava framework currently supports (to be released soon):\n\nChannel-based message passing between asynchronous processes (the Communicating Sequential Processes paradigm)\n\nHyper-granular parallelism where computation emerges as the collective result of inter-process interactions\n\nHeterogeneous execution platforms with both conventional and neuromorphic components\n\nOffline backprop-based training of a wide range of neuron models and network topologies\n\nTools for generating complex spiking neural networks such asdynamic neural fieldsand networks that solve well-defined optimization problems\n\nIntegration with third-party frameworks\n\nFor maximum developer productivity, Lava blends a simple Python Interface with accelerated performance using underlying C/C++/CUDA code.\n\nFor more information, visit Lava on Github:[https://github.com/lava-nc](https://github.com/lava-nc)https://github.com/lava-nc\n\nProcesses are the fundamental building block in the Lava architecture from which all algorithms and applications are built. Processes are stateful objects with internal variables, input and output ports for message-based communication via channels and multiple behavioral models. This architecture is inspired from the Communicating Sequential Process (CSP) paradigm for asynchronous, parallel systems that interact via message passing. Lava processes implementing the CSP API can be compiled and executed via a cross-platform compiler and runtime that support execution on neuromorphic and conventional von-Neumann HW. Together, these components form the low-level Magma layer of Lava.\n\nAt a higher level, the process library contains a growing set of generic processes that implement various kinds of neuron models, neural network connection topologies, IO processes, etc. These execute on either CPU, GPU or neuromorphic HW such as Intel’s Loihi architecture.\n\nVarious algorithm and application libraries build on these these generic processes to create specialized processes and provide tools to train or configure processes for more advanced applications. A deep learning library, constrained optimization library, and dynamic neural field library are among the first to be released in Lava, with more libraries to come in future releases.\n\nLava is open to modification and extension to third-party libraries like Nengo, ROS, YARP and others. Additional utilities also allow users to profile power and performance of workloads, visualize complex networks, or help with the float to fixed point conversions required for many low-precision devices such as neuromorphic HW.\n\nAll of Lava’s core APIs and higher-level components are released, by default, with permissive BSD 3 licenses in order to encourage the broadest possible community contribution.  Lower-level Magma components needed for mapping processes to neuromorphic backends are generally released with more restrictive LGPL-2.1 licensing to discourage commercial proprietary forks of these technologies.  The specific components of Magma needed to compile processes specifically to Intel Loihi chips remains proprietary to Intel and is not provided through this GitHub site (see below).  Similar Magma-layer code for other future commercial neuromorphic platforms likely will also remain proprietary.\n\nA process has input and output ports to interact with other processes, internal variables may have different behavioral implementations in different programming languages or for different HW platforms.\n\nProcess models are used to provide different behavioral models of a process. This Python model implements the LIF process, the Loihi synchronization protocol and requires a CPU compute resource to run.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please[subscribe to our newsletter](http://eepurl.com/hJCyhb)subscribe to our newsletter.\n\n[Index](https://lava-nc.org/genindex.html)Index\n\n[Module Index](https://lava-nc.org/py-modindex.html)Module Index\n\n[Search Page](https://lava-nc.org/search.html)Search Page\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n# Instantiate Lava processes to build networkfromlava.proc.dense.processimportDensefromlava.proc.lif.processimportLIFlif1=LIF()dense=Dense()lif2=LIF()# Connect processes via their directional input and output portslif1.out_ports.s_out.connect(self.dense.in_ports.s_in)dense.out_ports.a_out.connect(self.lif2.in_ports.a_in)# Execute process lif1 and all processes connected to it for fixed number of stepsfromlava.magma.core.run_conditionsimportRunStepsfromlava.magma.core.run_configsimportRunConfiglif1.run(condition=RunSteps(num_steps=10),run_cfg=SimpleRunConfig(sync_domains=[]))lif1.stop()\n``````\n\n``````\nfromlava.magma.core.process.processimportAbstractProcessfromlava.magma.core.process.variableimportVarfromlava.magma.core.process.ports.portsimportInPort,OutPortclassLIF(AbstractProcess):\"\"\"Leaky-Integrate-and-Fire neural process with activation input and spikeoutput ports a_in and s_out.Realizes the following abstract behavior:u[t] = u[t-1] * (1-du) + a_inv[t] = v[t-1] * (1-dv) + u[t] + biass_out = v[t] > vthv[t] = v[t] - s_out*vth\"\"\"def__init__(self,**kwargs):super().__init__(**kwargs)shape=kwargs.get(\"shape\",(1,))self.a_in=InPort(shape=shape)self.s_out=OutPort(shape=shape)self.u=Var(shape=shape,init=0)self.v=Var(shape=shape,init=0)self.du=Var(shape=(1,),init=kwargs.pop(\"du\",0))self.dv=Var(shape=(1,),init=kwargs.pop(\"dv\",0))self.bias=Var(shape=shape,init=kwargs.pop(\"b\",0))self.vth=Var(shape=(1,),init=kwargs.pop(\"vth\",10))\n``````\n\n``````\nimportnumpyasnpfromlava.magma.core.sync.protocols.loihi_protocolimportLoihiProtocolfromlava.magma.core.model.py.portsimportPyInPort,PyOutPortfromlava.magma.core.model.py.typeimportLavaPyTypefromlava.magma.core.resourcesimportCPUfromlava.magma.core.decoratorimportimplements,requiresfromlava.magma.core.model.py.modelimportPyLoihiProcessModelfromlava.proc.lif.processimportLIF@implements(proc=LIF,protocol=LoihiProtocol)@requires(CPU)classPyLifModel(PyLoihiProcessModel):a_in:PyInPort=LavaPyType(PyInPort.VEC_DENSE,np.int16,precision=16)s_out:PyOutPort=LavaPyType(PyOutPort.VEC_DENSE,bool,precision=1)u:np.ndarray=LavaPyType(np.ndarray,np.int32,precision=24)v:np.ndarray=LavaPyType(np.ndarray,np.int32,precision=24)bias:np.ndarray=LavaPyType(np.ndarray,np.int16,precision=12)du:int=LavaPyType(int,np.uint16,precision=12)dv:int=LavaPyType(int,np.uint16,precision=12)vth:int=LavaPyType(int,int,precision=8)defrun_spk(self):self.u[:]=self.u*((2**12-self.du)//2**12)a_in_data=self.a_in.recv()self.u[:]+=a_in_dataself.v[:]=self.v*\\((2**12-self.dv)//2**12)+self.u+self.biass_out=self.v>self.vthself.v[s_out]=0# Reset voltage to 0. This is Loihi-1 compatible.self.s_out.send(s_out)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/ann_sampler.html",
    "title": "ANN Statistics Sampler — Lava  documentation",
    "content": "ANN sampler module.\n\nANN data point sampler. It samples the weighted spike input ratez = \\langle z[t]\\rangleand\nneurons output spike rate (activation)a = \\langle s[t]\\rangledata points, manages the data points,\nand provides picewise linear ANN activation.\n\nnum_centers(int) – number of sampling centers. Defaults to 5.\n\nsample_range(list) – min and max range of sampling points. Defaults to [0.1, 0.9].\n\neps(float) – infinitesimal constant. Defaults to 1e-5.\n\nAppends new data points\n\na(torch.tensor) – output spike tensor.\n\nz(torch.tensor) – weighted spike tensor.\n\nClears all data points.\n\nFit piecewise linear model from sampled data.\n\nPicewise ANN activation\n\nz(torch tensor) – weighted input for ANN.\n\nequivalent ANN activation.\n\ntorch tensor\n\nPlots the piecewise ANN activation.\n\nfigsize(tupleof2) – width and height of figure. Defaults to None.\n\nRandomly clears half of the data points while retaining some of the\nhistory.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html",
    "title": "Block Module — Lava  documentation",
    "content": "Abstract bootstrap layer blocks.\n\nBases:`Module`Module\n\nAbstract bootstrap block\n\nFit the sampling points to estimate piecewise linear model.\n\nForward calculation block\n\nx(torch tensor) – input tensor.\n\nmode(optional) – forward operation mode. Can be one of`Mode.SNN`|`Mode.ANN`|`Mode.SAMPLE`Mode.SNN`|`Mode.ANN`|`Mode.SAMPLE.\nDefaults to Mode.ANN.\n\noutput tensor.\n\ntorch tensor\n\nBootstrap CUBA layer blocks.\n\nBases:[Affine](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.Affine)`Affine`Affine,[AbstractBlock](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.base.AbstractBlock)`AbstractBlock`AbstractBlock\n\nCUBA LIF affine transform class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights\nbefore synaptic operation. None means no transformation.\nDefaults to None.\n\ndynamics(bool,optional) – flag to enable neuron dynamics. If False, only the dendrite current\nis returned. Defaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None\nmeans no masking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nFit the sampling points to estimate piecewise linear model.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nBases:[Conv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.Conv)`Conv`Conv,[AbstractBlock](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.base.AbstractBlock)`AbstractBlock`AbstractBlock\n\nCUBA LIF convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[Dense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.Dense)`Dense`Dense,[AbstractBlock](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.base.AbstractBlock)`AbstractBlock`AbstractBlock\n\nCUBA LIF dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[Flatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.Flatten)`Flatten`Flatten,[AbstractBlock](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.base.AbstractBlock)`AbstractBlock`AbstractBlock\n\nCUBA LIF flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nBases:[Input](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.Input)`Input`Input,[AbstractBlock](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.base.AbstractBlock)`AbstractBlock`AbstractBlock\n\nCUBA LIF input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAffine\n``````\n\n``````\nAbstractBlock\n``````\n\n``````\nConv\n``````\n\n``````\nDense\n``````\n\n``````\nFlatten\n``````\n\n``````\nInput\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/block/modules.html",
    "title": "Blocks — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/bootstrap.html",
    "title": "Lava-DL Bootstrap — Lava  documentation",
    "content": "Rate based SNNs generated by ANN-SNN conversion methods require long runtimes to achieve high accuracy because the rate-based approximation of the ReLU activation unit breaks down for short runtimes.\n\n`lava.lib.dl.bootstrap`lava.lib.dl.bootstrapaccelerates rate coded Spiking Neural Network (SNN) training by dynamically estimating the equivalent ANN transfer function of a spiking layer with a picewise linear model at regular epochs interval and using the ANN equivlent network to train the original SNN.\n\nHighlight features\n\nAccelerated rate coded SNN training.\n\nLow latency inference of trained SNN made possible by close modeling\nof equivalent ANN dynamics.\n\nHybrid training with a mix of SNN layers and ANN layers for minimal\ndrop in SNN accuracy.\n\nScheduler for seamless switching between different bootstrap modes.\n\nThe underlying principle for ANN-SNN conversion is that the ReLU\nactivation function (or similar form) approximates the firing rate of an\nLIF spiking neuron. Consequently, an ANN trained with ReLU activation\ncan be mapped to an equivalent SNN with proper scaling of weights and\nthresholds. However, as the number of time-steps reduces, the alignment\nbetween ReLU activation and LIF spiking rate falls apart mainly due to\nthe following two reasons (especially, for discrete-in-time models like\nLoihi’s CUBA LIF):\n\nfit[](https://lava-nc.org/lava-lib-dl/bootstrap/bootstrap.html#id1)\n\nWith less time steps, the SNN can assume only a few discrete firing\nrates.\n\nLimited time steps mean that the spiking neuron activity rate often\nsaturates to maximum allowable firing rate.\n\nIn Bootstrap training. An SNN is used to jumpstart an equivalent ANN\nmodel which is then used to accelerate SNN training. There is no\nrestriction on the type of spiking neuron or it’s reset behavior. It\nconsists of following steps:\n\nInput output data points are first collected from the network running\nas an SNN:``bootstrap.mode.SNN``.\n\nThe data is used to estimate the corresponding ANN activation as a\npiecewise linear layer, unique to each layer:``bootstrap.mode.FIT``mode.\n\nThe training is accelerated using the piecewise linear ANN\nactivation:``bootstrap.mode.ANN``mode.\n\nThe network is seamlessly translated to an SNN:``bootstrap.mode.SNN``mode.\n\nSAMPLE mode and FIT mode are repeated for a few iterations every\ncouple of epochs, thus maintaining an accurate ANN estimate.\n\nThe dynamic estimation of ANN activation function may still not be\nenough to reduce the gap between SNN and it’s equivalent ANN, especially\nwhen the inference timesteps are low and the networks grow deep. In such\na scenario, one can look at a hybrid approach of directly training a\npart of the network as SNN layers/blocks while acclearating the rest of\nthe layers/blocks with bootstrap training.\n\nWith`bootstrap.block`bootstrap.blockinterface, some of the layers in the network\ncan be run in SNN and rest in ANN. We define acrossoverlayer which\nsplits layers earlier than it to always SNN and rest to ANN-SNN\nbootstrap mode.\n\n[MNIST digit classification](https://lava-nc.org/lava-lib-dl/bootstrap/notebooks/mnist/train.html)MNIST digit classification\n\nThe main modules are\n\nIt provides`lava.lib.dl.slayer.block`lava.lib.dl.slayer.blockbased network definition\ninterface.\n\nIt provides utilities for sampling SNN data points and pievewise linear\nANN fit.\n\n`bootstrap.routine.Scheduler`bootstrap.routine.Schedulerprovides an easy scheduling utility to\nseamlessly switch between SAMPLING | FIT | ANN | SNN mode. It also\nprovides ANN-SNN bootstraphybrid trainingutility as well\ndetermined by crossover point.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nbootstrap.block\n``````\n\n``````\nbootstrap.ann_sampler\n``````\n\n``````\nbootstrap.routine\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/index.html",
    "title": "Bootstrap (ANN-SNN training) — Lava  documentation",
    "content": "Contents:\n\n[Index](https://lava-nc.org/genindex.html)Index\n\n[Module Index](https://lava-nc.org/py-modindex.html)Module Index\n\n[Search Page](https://lava-nc.org/search.html)Search Page\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAnnSampler\n``````\n\n``````\nLayerMode\n``````\n\n``````\nMode\n``````\n\n``````\nScheduler\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/notebooks/mnist/train.html",
    "title": "Bootstrap SNN Training — Lava  documentation",
    "content": "The underlying principle for ANN-SNN conversion is that the ReLU activation function (or similar form) approximates the firing rate of an LIF spiking neuron. Consequently, an ANN trained with ReLU activation can be mapped to an equivalent SNN with proper scaling of weights and thresholds. However, as the number of time-steps reduces, the alignment between ReLU activation and LIF spiking rate falls apart mainly due to the following two reasons (especially, for discrete-in-time models like Loihi’s\nCUBA LIF):\n\nWith less time steps, the SNN can assume only a few discrete firing rates.\n\nLimited time steps mean that the spiking neuron activity rate often saturates to maximum allowable firing rate.\n\nIntroducingBootstrap training. An SNN is used to jumpstart an equivalent ANN model which is then used to accelerate SNN training. There is no restriction on the type of spiking neuron or it’s reset behavior. It consists of following steps:\n\nInput output data points are first collected from the network running as an SNN:SAMPLING mode.\n\nThe data is used to estimate the corresponding ANN activation as a piecewise linear layer, unique to each layer:FIT mode.\n\nThe training is accelerated using the piecewise linear ANN activation:ANN mode.\n\nThe network is seamlessly translated to an SNN:SNN mode.\n\nSAMPLING modeandFIT modeare repeated for a few iterations every couple of epochs, thus maintaining an accurate ANN estimate.\n\nBootstrap training is available as``lava.lib.dl.bootstrap``. The main modules are\n\n`block`block: provides`lava.lib.dl.slayer.block`lava.lib.dl.slayer.blockbased network definition interface.\n\n`ann_sampler`ann_sampler: provides utilities for sampling SNN data points and pievewise linear ANN fit.\n\n`routine`routine:`routine.Scheduler`routine.Schedulerprovides scheduling utility to seamlessly switch between SAMPLING | FIT | ANN | SNN mode.\n\nIt also provides ANN-SNN bootstrap hybrid traiing utility as well (Not demonstrated in this tutorial).\n\nHere, we will demonstrate botstrap SNN training on the well known MNIST classification problem.\n\nThe network definition follows standard PyTorch way using`torch.nn.Module`torch.nn.Module.\n\n`lava.lib.dl.bootstrap`lava.lib.dl.bootstrapprovidesblock interfacesimilar to`lava.lib.dl.slayer.block`lava.lib.dl.slayer.block- which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n\nHere we will use standardtorchvision datasetsto load MNIST data.\n\nTraining loop follows standard PyTorch training structure.`bootstrap.routine.Scheduler`bootstrap.routine.Schedulerhelps simplify the complex routine of periodically switching between different bootstrap modes during training.`scheduler.mode(epoch,i,net.training)`scheduler.mode(epoch,i,net.training)provides an iterator which orchestrates the mode of different blocks/layers.\n\nPlotting the learning curves is as easy as calling`stats.plot()`stats.plot().\n\nLoad the best model during training and export it as hdf5 network. It is supported by`lava.lib.dl.netx`lava.lib.dl.netxto automatically load the network as a lava process.\n\nHere, we will use`slayer.io.tensor_to_event`slayer.io.tensor_to_eventmethod to convert the torch output spike tensor into graded (non-binary)`slayer.io.Event`slayer.io.Eventobject and visualize a few input and output event pairs.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport os, sys\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\n\n# import slayer from lava-dl\nimport lava.lib.dl.slayer as slayer\nimport lava.lib.dl.bootstrap as bootstrap\n\nimport IPython.display as display\nfrom matplotlib import animation\n``````\n\n``````\n[2]:\n``````\n\n``````\nclass Network(torch.nn.Module):\n    def __init__(self, time_steps=16):\n        super(Network, self).__init__()\n        self.time_steps = time_steps\n\n        neuron_params = {\n                'threshold'     : 1.25,\n                'current_decay' : 1, # this must be 1 to use batchnorm\n                'voltage_decay' : 0.03,\n                'tau_grad'      : 1,\n                'scale_grad'    : 1,\n            }\n        neuron_params_norm = {\n                **neuron_params,\n                # 'norm'    : slayer.neuron.norm.MeanOnlyBatchNorm,\n            }\n\n        self.blocks = torch.nn.ModuleList([\n                bootstrap.block.cuba.Input(neuron_params, weight=1, bias=0), # enable affine transform at input\n                bootstrap.block.cuba.Dense(neuron_params_norm, 28*28, 512, weight_norm=True, weight_scale=2),\n                bootstrap.block.cuba.Dense(neuron_params_norm, 512, 512, weight_norm=True, weight_scale=2),\n                bootstrap.block.cuba.Affine(neuron_params, 512, 10, weight_norm=True, weight_scale=2),\n            ])\n\n    def forward(self, x, mode):\n        N, C, H, W = x.shape\n        if mode.base_mode == bootstrap.Mode.ANN:\n            x = x.reshape([N, C, H, W, 1])\n        else:\n            x = slayer.utils.time.replicate(x, self.time_steps)\n\n        x = x.reshape(N, -1, x.shape[-1])\n\n        for block, m in zip(self.blocks, mode):\n            x = block(x, mode=m)\n\n        return x\n\n    def export_hdf5(self, filename):\n        # network export to hdf5 format\n        h = h5py.File(filename, 'w')\n        simulation = h.create_group('simulation')\n        simulation['Ts'] = 1\n        simulation['tSample'] = self.time_steps\n        layer = h.create_group('layer')\n        for i, b in enumerate(self.blocks):\n            b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\n[3]:\n``````\n\n``````\ntrained_folder = 'Trained'\nos.makedirs(trained_folder, exist_ok=True)\n\n# device = torch.device('cpu')\ndevice = torch.device('cuda')\n\nnet = Network().to(device)\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\n# Dataset and dataLoader instances.\ntraining_set = datasets.MNIST(\n        root='data/',\n        train=True,\n        transform=transforms.Compose([\n            transforms.RandomAffine(\n                degrees=10,\n                translate=(0.05, 0.05),\n                scale=(0.95, 1.05),\n                shear=5,\n            ),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5), (0.5)),\n        ]),\n        download=True,\n    )\n\ntesting_set = datasets.MNIST(\n        root='data/',\n        train=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5), (0.5)),\n        ]),\n    )\n\ntrain_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)\n\nstats = slayer.utils.LearningStats()\nscheduler = bootstrap.routine.Scheduler()\n``````\n\n``````\n[4]:\n``````\n\n``````\nepochs = 100\nfor epoch in range(epochs):\n    for i, (input, label) in enumerate(train_loader, 0):\n        net.train()\n        mode = scheduler.mode(epoch, i, net.training)\n\n        input = input.to(device)\n        output = net.forward(input, mode)\n        rate = torch.mean(output, dim=-1).reshape((input.shape[0], -1))\n\n        loss = F.cross_entropy(rate, label.to(device))\n        prediction = rate.data.max(1, keepdim=True)[1].cpu().flatten()\n\n        stats.training.num_samples += len(label)\n        stats.training.loss_sum += loss.cpu().data.item() * input.shape[0]\n        stats.training.correct_samples += torch.sum( prediction == label ).data.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n\n    for i, (input, label) in enumerate(test_loader, 0):\n        net.eval()\n        mode = scheduler.mode(epoch, i, net.training)\n\n        with torch.no_grad():\n            input = input.to(device)\n            output = net.forward(input, mode=scheduler.mode(epoch, i, net.training))\n            rate = torch.mean(output, dim=-1).reshape((input.shape[0], -1))\n\n            loss = F.cross_entropy(rate, label.to(device))\n            prediction = rate.data.max(1, keepdim=True)[1].cpu().flatten()\n\n        stats.testing.num_samples += len(label)\n        stats.testing.loss_sum += loss.cpu().data.item() * input.shape[0]\n        stats.testing.correct_samples += torch.sum( prediction == label ).data.item()\n\n    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n\n    if mode.base_mode == bootstrap.routine.Mode.SNN:\n        scheduler.sync_snn_stat(stats.testing)\n        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n        print(mode)\n        print(f'[Epoch {epoch:2d}/{epochs}]\\nSNN Testing: {scheduler.snn_stat}')\n\n        if scheduler.snn_stat.best_accuracy:\n            torch.save(net.state_dict(), trained_folder + '/network.pt')\n        scheduler.update_snn_stat()\n\n    stats.update()\n    stats.save(trained_folder + '/')\n``````\n\n``````\nMode: SNN\n[Epoch  0/100]\nSNN Testing: loss =     0.18656                        accuracy = 0.96080\n\nMode: SNN\n[Epoch 10/100]\nSNN Testing: loss =     0.06348 (min =     0.18656)    accuracy = 0.98460 (max = 0.96080)\n\nMode: SNN\n[Epoch 20/100]\nSNN Testing: loss =     0.04227 (min =     0.06348)    accuracy = 0.98950 (max = 0.98460)\n\nMode: SNN\n[Epoch 30/100]\nSNN Testing: loss =     0.03778 (min =     0.04227)    accuracy = 0.98870 (max = 0.98950)\n\nMode: SNN\n[Epoch 40/100]\nSNN Testing: loss =     0.03424 (min =     0.03778)    accuracy = 0.98860 (max = 0.98950)\n\nMode: SNN\n[Epoch 50/100]\nSNN Testing: loss =     0.04215 (min =     0.03424)    accuracy = 0.98500 (max = 0.98950)\n\nMode: SNN\n[Epoch 60/100]\nSNN Testing: loss =     0.02876 (min =     0.03424)    accuracy = 0.99220 (max = 0.98950)\n\nMode: SNN\n[Epoch 70/100]\nSNN Testing: loss =     0.02592 (min =     0.02876)    accuracy = 0.99190 (max = 0.99220)\n\nMode: SNN\n[Epoch 80/100]\nSNN Testing: loss =     0.02771 (min =     0.02592)    accuracy = 0.99090 (max = 0.99220)\n\nMode: SNN\n[Epoch 90/100]\nSNN Testing: loss =     0.02705 (min =     0.02592)    accuracy = 0.99150 (max = 0.99220)\n[Epoch 99/100] Train loss =     0.01632 (min =     0.01561)    accuracy = 0.99425 (max = 0.99480) | Test  loss =     0.02458 (min =     0.02152)    accuracy = 0.99150 (max = 0.99280)\n``````\n\n``````\n[5]:\n``````\n\n``````\nstats.plot(figsize=(15, 5))\n``````\n\n``````\n[6]:\n``````\n\n``````\nnet.load_state_dict(torch.load(trained_folder + '/network.pt'))\nnet.export_hdf5(trained_folder + '/network.net')\n``````\n\n``````\n[7]:\n``````\n\n``````\noutput = net(input.to(device), mode=scheduler.mode(100, 0, False))\nfor i in range(5):\n    img = (2*input[i].reshape(28, 28).cpu().data.numpy()-1) * 255\n    Image.fromarray(img).convert('RGB').save(f'gifs/inp{i}.png')\n    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n    out_anim = out_event.anim(plt.figure(figsize=(10, 3.5)), frame_rate=2400)\n    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n``````\n\n``````\n[8]:\n``````\n\n``````\nimg_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 150px;\"/> </td>'\nhtml = '<table>'\nhtml += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\nfor i in range(5):\n    html += '<tr>'\n    html += img_td(f'gifs/inp{i}.png')\n    html += img_td(f'gifs/out{i}.gif')\n    html += '</tr>'\nhtml += '</tr></table>'\ndisplay.HTML(html)\n``````\n\n``````\n[8]:\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/bootstrap/routine.html",
    "title": "Routine — Lava  documentation",
    "content": "ANN-SNN mode switching routine helper.\n\nIterator that iterates layer/block’s mode of operation\n\ncrossover(int) – point below which layer is always in`mode.SNN`mode.SNN.\n\nbase_mode(enum) – global mode of operation. Options are`mode.{SNN|ANN|SAMPLE|FIT}`mode.{SNN|ANN|SAMPLE|FIT}.\n\nmode iterator\n\nEnum constants for different mode of operation. Valid modes areSNNSNN|ANNANN|SAMPLE | FITSAMPLE | FIT.\n\nHybrid mode iterator\n\nnum_sample_iter(int) – number of iteration to sample data. Defaults to 10.\n\nsample_period(int) – epoch interval to initiate data sampling. Defaults to 10.\n\ncrossover_epochs(list) – list of ints that define crossover landmarks. None means no landmarks.\nDefaults to None.\n\nBlock operation mode generator.\n\nepoch(int) – current epoch.\n\niteration(int) – current iteration.\n\ntrain(bool) – training or evaluation mode flag. Defaults to True.\n\noperation mode iterator\n\niterator\n\nSync SNN stat.\n\nstat([slayer.utils.LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.LearningStat)slayer.utils.LearningStat) – learning stat to sync.\n\nUpdate snn leraning statistics.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/index.html",
    "title": "Lava - Deep Learning — Lava  documentation",
    "content": "Welcome to the documentation of Lava - Deep Learning Library (Lava-DL).\nLava-DL is a library of Deep Learning tools within Lava that support offline\ntraining, online training and inference methods for various Deep Event-Based\nNetworks. Please refer[here](https://lava-nc.org/dl.html)herefor detailed description of the\nlibrary and a list of evergrowing modules.\n\nLava-DL is packaged as`lava.lib.dl`lava.lib.dl.\n\nContents:\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html",
    "title": "Blocks Module — Lava  documentation",
    "content": "Composable blocks processes in Lava.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nAbstract block definition.\n\nshape(tupleorlist) – shape of the block output in (x, y, z) or WHC format.\n\nneuron_params(dict,optional) – dictionary of neuron parameters. Defaults to None.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nDense Complex layer block.\n\nshape(tupleorlist) – shape of the layer block in (x, y, z)/WHC format.\n\nneuron_params(dict,optional) – dictionary of neuron parameters. Defaults to None.\n\nweight_real(np.ndarray) – synaptic real weight.\n\nweight_imag(np.ndarray) – synaptic imag weight.\n\nhas_graded_input(dict) – flag for graded spikes at input. Defaults to False.\n\nnum_weight_bits_real(int) – number of real weight bits. Defaults to 8.\n\nnum_weight_bits_imag(int) – number of imag weight bits. Defaults to 8.\n\nweight_exponent_real(int) – real weight exponent value. Defaults to 0.\n\nweight_exponent_imag(int) – imag weight exponent value. Defaults to 0.\n\nsparse_synapse(bool) – connection is sparse\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\n`None`None\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nInput layer block.\n\nshape(tupleorlist) – shape of the layer block in (x, y, z)/WHC format.\n\nneuron_params(dict,optional) – dictionary of neuron parameters. Defaults to None.\n\n`None`None\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nConv layer block.\n\nshape(tupleorlist) – shape of the layer block in (x, y, z)/WHC format.\n\ninput_shape(tupleorlist) – shape of input layer in (x, y, z)/WHC format.\n\nneuron_params(dict,optional) – dictionary of neuron parameters. Defaults to None.\n\nweight(np.ndarray) – kernel weight.\n\nbias(np.ndarrayorNone) – bias of neuron. None means no bias. Defaults to None.\n\nstride(intortupleofints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleofints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleofints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int) – convolution groups. Defaults to 1.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\n`None`None\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nDense layer block.\n:param shape: shape of the layer block in (x, y, z)/WHC format.\n:type shape: tuple or list\n:param neuron_params: dictionary of neuron parameters. Defaults to None.\n:type neuron_params: dict, optional\n:param weight: synaptic weight.\n:type weight: np.ndarray\n:param delay: synaptic delay.\n:type delay: np.ndarray\n:param bias: bias of neuron. None means no bias. Defaults to None.\n:type bias: np.ndarray or None\n:param has_graded_input: flag for graded spikes at input. Defaults to False.\n:type has_graded_input: dict\n:param num_weight_bits: number of weight bits. Defaults to 8.\n:type num_weight_bits: int\n:param weight_exponent: weight exponent value. Defaults to 0.\n:type weight_exponent: int\n:param sparse_synapse: connection is sparse\n:type sparse_synapse: bool\n:param input_message_bits: number of message bits in input spike. Defaults to 0 meaning unary\n\nspike.\n\n`None`None\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nInput layer block.\n\nshape(tupleorlist) – shape of the layer block in (x, y, z)/WHC format.\n\nneuron_params(dict,optional) – dictionary of neuron parameters. Defaults to None.\n\ntransform(fx pointerorlambda) – input transform to be applied. Defualts to`lambdax:x`lambdax:x.\n\nbias(np.ndarrayorNone) – bias of input neuron. None means no bias. Defaults to None.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\n`None`None\n\nBases:[AbstractBlock](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.AbstractBlock)`AbstractBlock`AbstractBlock\n\nRecurrentDense layer block.\n:param shape: shape of the layer block in (x, y, z)/WHC format.\n:type shape: tuple or list\n:param neuron_params: dictionary of neuron parameters. Defaults to None.\n:type neuron_params: dict, optional\n:param weight: synaptic weight.\n:type weight: np.ndarray\n:param weight_rec: recurrent synaptic weight.\n:type weight_rec: np.ndarray\n:param delay: synaptic delay.\n:type delay: np.ndarray\n:param bias: bias of neuron. None means no bias. Defaults to None.\n:type bias: np.ndarray or None\n:param has_graded_input: flag for graded spikes at input. Defaults to False.\n:type has_graded_input: dict\n:param num_weight_bits: number of weight bits. Defaults to 8.\n:type num_weight_bits: int\n:param weight_exponent: weight exponent value. Defaults to 0.\n:type weight_exponent: int\n:param sparse_synapse: connection is sparse\n:type sparse_synapse: bool\n:param input_message_bits: number of message bits in input spike. Defaults to 0 meaning unary\n\nspike.\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nAbstract Block model. A block typically encapsulates at least a\nsynapse and a neuron in a layer. It could also include recurrent\nconnection as well as residual connection. A minimal example of a\nblock is a feedforward layer.\n\nBases:[AbstractPyBlockModel](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.models.AbstractPyBlockModel)`AbstractPyBlockModel`AbstractPyBlockModel\n\nalias of[ComplexDense](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.ComplexDense)`ComplexDense`ComplexDense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyBlockModel](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.models.AbstractPyBlockModel)`AbstractPyBlockModel`AbstractPyBlockModel\n\nalias of[ComplexInput](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.ComplexInput)`ComplexInput`ComplexInput\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyBlockModel](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.models.AbstractPyBlockModel)`AbstractPyBlockModel`AbstractPyBlockModel\n\nalias of[Conv](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Conv)`Conv`Conv\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyBlockModel](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.models.AbstractPyBlockModel)`AbstractPyBlockModel`AbstractPyBlockModel\n\nalias of[Dense](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Dense)`Dense`Dense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyBlockModel](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.models.AbstractPyBlockModel)`AbstractPyBlockModel`AbstractPyBlockModel\n\nalias of[Input](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Input)`Input`Input\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractProcess\n``````\n\n``````\nAbstractBlock\n``````\n\n``````\nAbstractSubProcessModel\n``````\n\n``````\nAbstractPyBlockModel\n``````\n\n``````\nComplexDense\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nComplexInput\n``````\n\n``````\nConv\n``````\n\n``````\nDense\n``````\n\n``````\nInput\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/blocks/modules.html",
    "title": "Blocks — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/hdf5.html",
    "title": "HDF5 — Lava  documentation",
    "content": "HDF5 network exchange module.\n\nGenerates a Lava process for the network described in hdf5 config.\n\nnet_config(str) – name of the hdf5 config filename.\n\nnum_layers(int,optional) – number of blocks to generate. An integer value will only generate the\nfirst`num_layers`num_layersblocks in the description. The actual number of\ngenerated layers may be less than`num_layers`num_layers. If it is None, all\nthe layers are generated. Defaults to None.\n\nskip_layers(int) – number of blocks to skip. The nework generator will skip the first`skip_layers`skip_layersblocks that have been generated accoridng to`num_layers`num_layersparametr. If it is None, no layers are skipped.\nDefaults to None.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\ninput_shape(tupleofints,optional) – shape of input to the network. If None, input layer is assumed to be\nthe first layer. Defaults to None.\n\nreset_interval(int,optional) – determines how often the network needs to be reset. Reset is\norchestrated with a timestep offset as per the layer number. If that is\nnot desired, it can be changed by directly accessing the layer’s\nneuron’s reset parameter. None means no reset. Defaults to None.\n\nreset_offset(int) – determines the phase shift of network reset if enabled. Defaults to 0.\n\nspike_exp(int) – determines the decimal place of graded spike. Defaults to 6.\n\nsparse_fc_layer(boolean,optional) – If True, all fully-connected layer synapses will be interpreted as\nSparse types in Lava.\n\nNumber of layers in the network.\n\n`int`int\n\nNetwork description string.\n\n`str`str\n\nCreates conv layer from layer configuration\n\nlayer_config(h5py.Group) – hdf5 handle to layer description.\n\ninput_shape(tupleof3 ints) – shape of input to the block.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\nreset_interval(int,optional) – the reset interval. None means no reset. Default is None.\n\nreset_offset(int) – the offset/phase of reset. It is only valid of reset_interval is\nnot None.\n\nspike_exp(int) – determines the decimal place of graded spike. Defaults to 6.\n\n`Tuple`Tuple[[Conv](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Conv)`Conv`Conv,`str`str]\n\nAbstractProcess– dense block process.str– table entry string for process.\n\nAbstractProcess– dense block process.\n\nstr– table entry string for process.\n\nCreates dense layer from layer configuration\n\nlayer_config(h5py.Group) – hdf5 handle to layer description.\n\ninput_message_bits(int,optional) – number of message bits in input spike. Defaults to 0 meaning unary\nspike.\n\nreset_interval(int,optional) – the reset interval. None means no reset. Default is None.\n\nreset_offset(int) – the offset/phase of reset. It is only valid of reset_interval is\nnot None.\n\nspike_exp(int) – determines the decimal place of graded spike. Defaults to 6.\n\n`Tuple`Tuple[[Dense](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Dense)`Dense`Dense,`str`str]\n\nAbstractProcess– dense block process.str– table entry string for process.\n\nAbstractProcess– dense block process.\n\nstr– table entry string for process.\n\nCreates input layer from layer configuration.\n\nlayer_config(h5py.Group) – hdf5 handle to layer description.\n\nreset_interval(int,optional) – the reset interval. None means no reset. Default is None.\n\nreset_offset(int) – the offset/phase of reset. It is only valid of reset_interval is\nnot None.\n\nspike_exp(int) – determines the decimal place of graded spike. Defaults to 6.\n\n`Tuple`Tuple[[Input](https://lava-nc.org/lava-lib-dl/netx/blocks/blocks.html#lava.lib.dl.netx.blocks.process.Input)`Input`Input,`str`str]\n\nAbstractProcess– input block process.str– table entry string for process.\n\nAbstractProcess– input block process.\n\nstr– table entry string for process.\n\nProvides the correct neuron configuration process and parameters\nfrom the neuron description in hdf5 config.\n\nneuron_config(h5py.Group) – hdf5 object describing the neuron configuration\n\ninput(bool) – flag to indicate if the layer is input. For some cases special\nprocessing may be done.\n\nreset_interval(int,optional) – the reset interval. None means no reset. Default is None.\n\nreset_offset(int) – the offset/phase of reset. It is only valid of reset_interval is\nnot None.\n\nspike_exp(int) – determines the decimal place of graded spike. Defaults to 6.\n\nThe Lava process that implements the neuron described.\n\n[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess\n\nalias of[Network](https://lava-nc.org/lava-lib-dl/netx/hdf5.html#lava.lib.dl.netx.hdf5.Network)`Network`Network\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nConv\n``````\n\n``````\nDense\n``````\n\n``````\nInput\n``````\n\n``````\nNetwork\n``````\n\n``````\nLoihiProtocol\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/index.html",
    "title": "Lava-DL NetX — Lava  documentation",
    "content": "Contents:\n\n[Index](https://lava-nc.org/genindex.html)Index\n\n[Module Index](https://lava-nc.org/py-modindex.html)Module Index\n\n[Search Page](https://lava-nc.org/search.html)Search Page\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nNetwork\n``````\n\n``````\nPyNetworkModel\n``````\n\n``````\nNetDict\n``````\n\n``````\nSYNAPSE_SIGN_MODE\n``````\n\n``````\nnum_delay_bits()\n``````\n\n``````\noptimize_weight_bits()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/netx.html",
    "title": "Lava-DL NetX — Lava  documentation",
    "content": "Lava-DL NetX library`lava.lib.dl.netx`lava.lib.dl.netxautomates deep learning network model exchange\nto/from LAVA from/to other frameworks. At the moment, we support a\nsimple platform independent hdf5 network description protocol. In\nfurture we will extend network exchange support to other neural network\nexchange formats.\n\nLoading a model to Lava is as simple as:\n\nThe hdf5 network description protocol is described below:\n\nThe computational graph is represented layer-wise in the`layer`layerfield of the hdf5 file.\n\nThe layer id is assigned serially from`0`0to`n-1`n-1starting from\ninput to output.\n\nBy default, a sequential connection is assumed.\n\nSkip/recurrent connections are preceded by a concatenate layer\nthat connects to pervious layer plus a list of non-sequential\nlayers identified by their id.\n\nEach layer entry consts a minimum of`shape`shapeand`type`typefield.\nOther relevant fileds can be addes as necessary.\n\n`shape`shapeentry is a tuple/list in (x, y, z) format.\n\n`type`typeentry is a string that describes the layer type. See\nbelow for a list of supported types.\n\n`neuron`neuronfield describes the compartment model and it’s\nparameter.\n\ndefault`neuron`neurontype is`CUBA-LIF`CUBA-LIF.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nfromlava.lib.dl.netximporthdf5# Import the model as a Lava Processnet=hdf5.Network(net_config='network.net')\n``````\n\n``````\n||->layer# description of network layer blocks such as input, dense, conv, pool, flatten, average||->0|||->{shape,type,...}# each layer description has at least shape and type attribute||->1|||->{shape,type,...}|:||->n-1||->{shape,type,...}||# other fields (not used for network exchange)|->simulation# simulation description||->Ts# sampling time. Usually 1||->tSample# length of the sample to run\n``````\n\n``````\ninput:{shape,type}flatten:{shape,type}average:{shape,type}concat:{shape,type,layers}dense:{shape,type,neuron,inFeatures,outFeatures,weight,delay(ifavailable)}pool:{shape,type,neuron,kernelSize,stride,padding,dilation,weight}conv:{shape,type,neuron,inChannels,outChannels,kernelSize,stride,|padding,dilation,groups,weight,delay(ifavailable)}||->thisisthedescriptionofthecompartmentparameters|->{iDecay,vDecay,vThMant,refDelay,...(otheradditionalparams)}\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/notebooks/oxford/run.html",
    "title": "Oxford example — Lava  documentation",
    "content": "This tutorial demonstrates the`lava.lib.dl.netx`lava.lib.dl.netxapi for running Oxford network trained using`lava.lib.dl.slayer`lava.lib.dl.slayer. The training example can be found here[here](https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/oxford/train.ipynb)here\n\nThe task is to learn to transform a random Poisson spike train to an output spike pattern that resembles The Radcliffe Camera building of Oxford University, England. The input and output both consist of 200 neurons each and the spikes span approximately 1900ms. The input and output pair are converted from[SuperSpike](https://github.com/fzenke/pub2018superspike)SuperSpike(© GPL-3).\n\nInput\n\nOutput\n\nThe NetX api allows automatic creation of Lava process from the network specification. It is available as a part of the lava-dl library as`lava.lib.dl.netx`lava.lib.dl.netx\n\nCheck if Loihi2 compiker is available and import related modules.\n\nA lava process describing the network can be created by simply instantiating`netx.hdf5.Network`netx.hdf5.Networkwith the path of the desired hdf5 network description file. * The input layer is accessible as`net.in_layer`net.in_layer. * The output layer is accessible as`net.out_layer`net.out_layer. * All the constituent layers are accessible as a list:`net.layers`net.layers.\n\nHere, we will use`RingBuffer`RingBufferprocesses in`lava.proc.io.{source/sink}`lava.proc.io.{source/sink}to generate spike that is sent to the network and record the spike output from the network.\n\nThere are 200 neurons and the input spikes span apprximately 2000 steps.\n\nWe will use`slayer.io`slayer.ioutilities to read the event data and convert them to dense spike data.\n\nWe will run the network for 2000 steps and read the network’s output.\n\nSwitching between Loihi 2 hardware and CPU simulation is as simple as changing the run configuration settings.\n\nFinally, convert output spike data into an event and plot them.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport logging\n\nfrom lava.magma.core.run_configs import Loihi2HwCfg, Loihi2SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.proc import io\n\nfrom utils import InputAdapter, OutputAdapter, OxfordMonitor\n``````\n\n``````\n[2]:\n``````\n\n``````\nfrom lava.lib.dl import netx\nfrom lava.lib.dl import slayer\n``````\n\n``````\n[3]:\n``````\n\n``````\nfrom lava.utils.system import Loihi2\nLoihi2.preferred_partition = 'oheogulch'\nloihi2_is_available = Loihi2.is_loihi2_available\n\nif loihi2_is_available:\n    from lava.proc import embedded_io as eio\n    print(f'Running on {Loihi2.partition}')\nelse:\n    print(\"Loihi2 compiler is not available in this system. \"\n          \"This tutorial will execute on CPU backend.\")\n``````\n\n``````\nRunning on kp_stack\n``````\n\n``````\n[4]:\n``````\n\n``````\nnet = netx.hdf5.Network(net_config='Trained/network.net')\nprint(net)\n``````\n\n``````\n|   Type   |  W  |  H  |  C  | ker | str | pad | dil | grp |delay|\n|Dense     |    1|    1|  256|     |     |     |     |     |False|\n|Dense     |    1|    1|  200|     |     |     |     |     |False|\n``````\n\n``````\n[5]:\n``````\n\n``````\nprint(f'There are {len(net)} layers in the network:')\n\nfor l in net.layers:\n    print(f'{l.block:5s} : {l.name:10s}, shape : {l.shape}')\n``````\n\n``````\nThere are 2 layers in network:\nDense : Process_1 , shape : (256,)\nDense : Process_4 , shape : (200,)\n``````\n\n``````\n[6]:\n``````\n\n``````\ninput = slayer.io.read_np_spikes('input.npy')\ntarget = slayer.io.read_np_spikes('output.npy')\nsource = io.source.RingBuffer(data=input.to_tensor(dim=(1, 200, 2000)).squeeze())\nsink = io.sink.RingBuffer(shape=net.out.shape, buffer=2000)\ninp_adapter = InputAdapter(shape=net.inp.shape)\nout_adapter = OutputAdapter(shape=net.out.shape)\n\nsource.s_out.connect(inp_adapter.inp)\ninp_adapter.out.connect(net.inp)\nnet.out.connect(out_adapter.inp)\nout_adapter.out.connect(sink.a_in)\n``````\n\n``````\n[7]:\n``````\n\n``````\nif loihi2_is_available:\n    from utils import CustomHwRunConfig\n    run_config = CustomHwRunConfig()\nelse:\n    from utils import CustomSimRunConfig\n    run_config = CustomSimRunConfig()\nrun_condition = RunSteps(num_steps=2000)\nnet._log_config.level = logging.INFO\nnet.run(condition=run_condition, run_cfg=run_config)\noutput = sink.data.get()\nnet.stop()\n``````\n\n``````\nViolation core_id=0 reg_name='SynMem' allocation=16128 self.cost_db.registers[reg_name]=12000\nViolation core_id=1 reg_name='SynMem' allocation=16200 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=0 reg_name='SynMem' allocation=16128 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=1 reg_name='SynMem' allocation=16200 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=0 reg_name='SynMem' allocation=16128 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=1 reg_name='SynMem' allocation=16200 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=0 reg_name='SynMem' allocation=16128 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=1 reg_name='SynMem' allocation=16200 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=0 reg_name='SynMem' allocation=16128 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Violation core_id=1 reg_name='SynMem' allocation=16000 self.cost_db.registers[reg_name]=12000\nFinal max_ratio=2, Per core distribution:\n----------------------------------------------------------------\n| AxonIn |NeuronGr| Neurons|Synapses| AxonMap| AxonMem|  Cores |\n|--------------------------------------------------------------|\n|     256|       1|     100|    8064|     100|       0|       2|\n|     200|       1|      85|    5400|      85|       0|       3|\n|--------------------------------------------------------------|\n| Total                                               |       5|\n----------------------------------------------------------------INFO:DRV:  SLURM is being run in backgroundINFO:DRV:  Connecting to 10.54.73.72:41019INFO:DRV:      Host server up..............Done 0.49sINFO:DRV:      Mapping chipIds.............Done 0.02msINFO:DRV:      Mapping coreIds.............Done 0.07msINFO:DRV:      Partitioning neuron groups..Done 1.38msINFO:DRV:      Mapping axons...............Done 2.52msINFO:DRV:      Partitioning MPDS...........Done 0.61msINFO:DRV:      Creating Embedded Snips and ChannelsDone 0.02sINFO:DRV:      Compiling Embedded snips....Done 2.90sINFO:DRV:      Compiling Host snips........Done 0.06msINFO:DRV:      Compiling Register Probes...Done 0.09msINFO:DRV:      Compiling Spike Probes......Done 0.01msINFO:HST:  Args chip=0 cpu=0 /home/sshresth/lava-nc/frameworks.ai.nx.nxsdk/nxcore/arch/base/pre_execution/../../../../temp/27a4c4ec-3a0e-11ed-bc43-dd9a32d49015/launcher_chip0_lmt0.bin --chips=1 --remote-relay=0INFO:HST:  Args chip=0 cpu=1 /home/sshresth/lava-nc/frameworks.ai.nx.nxsdk/nxcore/arch/base/pre_execution/../../../../temp/27a4c4ec-3a0e-11ed-bc43-dd9a32d49015/launcher_chip0_lmt1.bin --chips=1 --remote-relay=0INFO:HST:  Nx...INFO:DRV:      Booting up..................Done 1.57sINFO:DRV:      Encoding probes.............Done 0.01msINFO:DRV:      Transferring probes.........Done 7.08msINFO:DRV:      Configuring registers.......Done 0.61sINFO:DRV:      Transferring spikes.........Done 0.00msINFO:HST:  chip=0 msg=00018114 00ffff00INFO:DRV:      Executing...................Done 16.09sINFO:DRV:      Processing timeseries.......Done 0.01msINFO:DRV:  Executor: 2000 timesteps........Done 16.85sINFO:HST:  Execution has not started yet or has finished.INFO:HST:  Stopping Execution : at 2000INFO:HST:  chip=0 cpu=1 halted, status=0x0INFO:HST:  chip=0 cpu=0 halted, status=0x0\n``````\n\n``````\n[8]:\n``````\n\n``````\nout_event = slayer.io.tensor_to_event(output.reshape((1,) + output.shape))\ngt_event = slayer.io.read_np_spikes('ground_truth.npy')\n``````\n\n``````\n[9]:\n``````\n\n``````\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nax[0].plot(gt_event.t, gt_event.x, '.', markersize=2)\nax[1].plot(out_event.t, out_event.x, '.', markersize=2)\nax[0].set_title('Slayer Reference')\nax[0].set_ylabel('Neuron ID')\nax[0].set_xlabel('time')\nax[1].set_title('Lava')\nax[1].set_xlabel('time')\n``````\n\n``````\n[9]:\n``````\n\n``````\nText(0.5, 0, 'time')\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/notebooks/pilotnet_sdnn/run.html",
    "title": "PilotNet SDNN Example — Lava  documentation",
    "content": "This tutorial demonstrates how to uselavato perform inference on a PilotNet SDNN on both CPU and Loihi 2 neurocore.\n\nThe network receives video input, recorded from a dashboard camera of a driving car (Dataloader). The data is encoded efficiently as the difference between individual frames (Encoder). The data passes through the PilotNet SDNN, which was trained withlava-dland is built using itsNetwork Exchangemodule (netx.hdf5.Network), which automatically generates a Lava process from the training artifact. The network estimates the angle of the steering wheel of the car, which is decoded\nfrom the network’s raw output (Decoder) and sent to a visualization (Monitor) and logging system (Logger).\n\nThe core of the tutorial is lava-dl’s Network Exchange module, which is available as`lava.lib.dl.netx.{hdf5,blocks,utils}`lava.lib.dl.netx.{hdf5,blocks,utils}. *`hdf5`hdf5implements automatic network generation. *`blocks`blocksimplements individual layer blocks. *`utils`utilsimplements hdf5 reading utilities.\n\nIn addition, it also demonstrates how different lava processes can be connected with each other for real time interaction between them even though the underlying processes can be run on various backends, including Loihi 2.\n\nSwitching between Loihi 2 hardware and CPU simulation is as simple as changing the run configuration settings.\n\nCheck if Loihi2 compiler is available and import related modules.\n\nPilotNet SDNN is described by the hdf5 file interface`network.net`network.netexported after training. You can refer to the training tutorial that trains the networks and exports hdf5 file interface at``tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb``tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb<[https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb](https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb)https://github.com/lava-nc/lava-dl/blob/main/tutorials/lava/lib/dl/slayer/pilotnet/train.ipynb>`__\n\nA network block can be created by simply instantiating`netx.hdf5.Network`netx.hdf5.Networkwith the path of the desired hdf5 network description file. * The input layer is accessible as`net.in_layer`net.in_layer. * The output layer is accessible as`net.out_layer`net.out_layer. * All the constituent layers are accessible as a list:`net.layers`net.layers.\n\nConfigure number of samples, execution timesteps, and readout offset.\n\nTypically the user would write it or provide it.\n\nThe dataloader process reads data from the dataset objects and sends out the input frame and ground truth as spikes.\n\nThe input encoder process does frame difference of subsequent frames to sparsify the input to the network.\n\nFor Loihi execution, it additionally compresses and sends the input data to the Loihi 2 chip.\n\nThe output decoder process receives the output from the network and applies proper scaling to decode the steering angle prediction.\n\nFor Loihi execution, it additionally communicates the network’s output spikes from the Loihi 2 chip.\n\nMonitor is a lava process that visualizes the PilotNet network prediction in real-time. In addition, datalogger processes store the network predictions and ground truths.\n\nSwitching between Loihi 2 hardware and CPU simulation is as simple as changing the run configuration settings.\n\nPlot and compare the results with the dataset ground truth.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.proc import io\n\nfrom lava.lib.dl import netx\nfrom dataset import PilotNetDataset\nfrom utils import (\n    PilotNetEncoder, PilotNetDecoder, PilotNetMonitor,\n    CustomHwRunConfig, CustomSimRunConfig,\n    get_input_transform\n)\n``````\n\n``````\n[2]:\n``````\n\n``````\nfrom lava.utils.system import Loihi2\nLoihi2.preferred_partition = 'oheogulch'\nloihi2_is_available = Loihi2.is_loihi2_available\n\nif loihi2_is_available:\n    print(f'Running on {Loihi2.partition}')\n    compression = io.encoder.Compression.DELTA_SPARSE_8\nelse:\n    print(\"Loihi2 compiler is not available in this system. \"\n          \"This tutorial will execute on CPU backend.\")\n    compression = io.encoder.Compression.DENSE\n``````\n\n``````\nRunning on oheogulch\n``````\n\n``````\n[3]:\n``````\n\n``````\nnet = netx.hdf5.Network(net_config='network.net', skip_layers=1)\nprint(net)\n``````\n\n``````\n|   Type   |  W  |  H  |  C  | ker | str | pad | dil | grp |delay|\n|Conv      |   99|   32|   24| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   49|   15|   36| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   24|    7|   48| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   22|    4|   64| 3, 3| 1, 2| 0, 1| 1, 1|    1|False|\n|Conv      |   20|    2|   64| 3, 3| 1, 1| 0, 0| 1, 1|    1|False|\n|Dense     |    1|    1|  100|     |     |     |     |     |False|\n|Dense     |    1|    1|   50|     |     |     |     |     |False|\n|Dense     |    1|    1|   10|     |     |     |     |     |False|\n|Dense     |    1|    1|    1|     |     |     |     |     |False|\n``````\n\n``````\n[4]:\n``````\n\n``````\nprint(f'There are {len(net)} layers in the network:')\n\nfor l in net.layers:\n    print(f'{l.__class__.__name__:5s} : {l.name:10s}, shape : {l.shape}')\n``````\n\n``````\nThere are 9 layers in the network:\nConv  : in_layer  , shape : (99, 32, 24)\nConv  : Process_6 , shape : (49, 15, 36)\nConv  : Process_9 , shape : (24, 7, 48)\nConv  : Process_12, shape : (22, 4, 64)\nConv  : Process_15, shape : (20, 2, 64)\nDense : Process_18, shape : (100,)\nDense : Process_21, shape : (50,)\nDense : Process_24, shape : (10,)\nDense : out_layer , shape : (1,)\n``````\n\n``````\n[5]:\n``````\n\n``````\nnum_samples = 200\nsteps_per_sample = 1\nnum_steps = num_samples + len(net.layers)\n# Output appears delayed due to encoding and spike communication\nout_offset = len(net.layers) + 3\n``````\n\n``````\n[6]:\n``````\n\n``````\ntransform = get_input_transform(net.net_config)\nfull_set = PilotNetDataset(\n    path='../data',\n    size=net.inp.shape[:2],\n    transform=transform,  # input transform\n    visualize=True,  # visualize ensures the images are returned in sequence\n    sample_offset=10550,\n)\ntrain_set = PilotNetDataset(\n    path='../data',\n    size=net.inp.shape[:2],\n    transform=transform,  # input transform\n    train=True,\n)\ntest_set = PilotNetDataset(\n    path='../data',\n    size=net.inp.shape[:2],\n    transform=transform,  # input transform\n    train=False,\n)\n``````\n\n``````\n[7]:\n``````\n\n``````\ndataloader = io.dataloader.SpikeDataloader(dataset=full_set)\n``````\n\n``````\n[8]:\n``````\n\n``````\ninput_encoder = PilotNetEncoder(shape=net.inp.shape,\n                                net_config=net.net_config,\n                                compression=compression)\n``````\n\n``````\n[9]:\n``````\n\n``````\noutput_decoder = PilotNetDecoder(shape=net.out.shape)\n``````\n\n``````\n[10]:\n``````\n\n``````\nmonitor = PilotNetMonitor(shape=net.inp.shape,\n                          transform=transform,\n                          output_offset=out_offset)\ngt_logger = io.sink.RingBuffer(shape=(1,), buffer=num_steps)\noutput_logger = io.sink.RingBuffer(shape=net.out_layer.shape, buffer=num_steps)\n``````\n\n``````\n[11]:\n``````\n\n``````\ndataloader.ground_truth.connect(gt_logger.a_in)\ndataloader.s_out.connect(input_encoder.inp)\n\ninput_encoder.out.connect(net.inp)\nnet.out.connect(output_decoder.inp)\noutput_decoder.out.connect(output_logger.a_in)\n\ndataloader.s_out.connect(monitor.frame_in)\ndataloader.ground_truth.connect(monitor.gt_in)\noutput_decoder.out.connect(monitor.output_in)\n``````\n\n``````\n[12]:\n``````\n\n``````\nif loihi2_is_available:\n    run_config = CustomHwRunConfig()\nelse:\n    run_config = CustomSimRunConfig()\nnet.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_config)\noutput = output_logger.data.get().flatten()\ngts = gt_logger.data.get().flatten()\nnet.stop()\n``````\n\n``````\n[13]:\n``````\n\n``````\nplt.figure(figsize=(7, 5))\nplt.plot(np.array(gts), label='Ground Truth')\nplt.plot(np.array(output[out_offset:]).flatten(), label='Lava output')\nplt.xlabel(f'Sample frames (+10550)')\nplt.ylabel('Steering angle (radians)')\nplt.legend()\n``````\n\n``````\n[13]:\n``````\n\n``````\n<matplotlib.legend.Legend at 0x7f86f1e55ca0>\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/notebooks/pilotnet_snn/run.html",
    "title": "PilotNet LIF Example — Lava  documentation",
    "content": "This tutorial demonstrates how to uselavato perform inference on a PilotNet LIF on both CPU and Loihi 2 neurocore.\n\nThe network receives video input, recorded from a dashboard camera of a driving car (Dataloader). The data is encoded efficiently as the difference between individual frames (Encoder). The data passes through the PilotNet LIF, which was trained withlava-dland is built using itsNetwork Exchangemodule (netx.hdf5.Network), which automatically generates a Lava process from the training artifact. The network estimates the angle of the steering wheel of the car, is read from the\noutput layer neuron’s voltage state, decoded using proper scaling (Decoder) and sent to a visualization (Monitor) and logging system (Logger).\n\nThe PilotNet LIF network predicts the steerting angle every 16th timestep. The input is sent from the dataloader at the same frequency and the PilotNet LIF network resets it’s internal state every 16th timestep to prcoess the new input frame.\n\nThe core of the tutorial is lava-dl’s Network Exchange module, which is available as`lava.lib.dl.netx.{hdf5,blocks,utils}`lava.lib.dl.netx.{hdf5,blocks,utils}. *`hdf5`hdf5implements automatic network generation. *`blocks`blocksimplements individual layer blocks. *`utils`utilsimplements hdf5 reading utilities.\n\nIn addition, it also demonstrates how different lava processes can be connected with each other for real time interaction between them even though the underlying processes can be run on various backends, including Loihi 2.\n\nSwitching between Loihi 2 hardware and CPU simulation is as simple as changing the run configuration settings.\n\nCheck if Loihi2 compiker is available and import related modules.\n\nPilotNet LIF is described by the hdf5 file inference`network.net`network.net.\n\nA network block can be created by simply instantiating`netx.hdf5.Network`netx.hdf5.Networkwith the path of the desired hdf5 network description file. * The input layer is accessible as`net.in_layer`net.in_layer. * The output layer is accessible as`net.out_layer`net.out_layer. * All the constituent layers are accessible as as a list:`net.layers`net.layers.\n\nThe PilotNet LIF needs to be reset for every input sample. The reset needs to be orchestrated at different time steps for each layer of the network for fastest possible throughput.`netx.hdf5.Network`netx.hdf5.Networkfeatures pipelined orchestration of layer reset where each subsequent layer is reset a time step later than it’s input.\n\nConfigure number of samples, execution timesteps, and readout offset.\n\nTypically the user would write it or provide it.\n\nThe dataloader process reads data from the dataset objects and sends out the input frame and ground truth as spikes. The dataloader injects new input sample every`steps_per_sample`steps_per_sample.\n\nThe PilotNet LIF network’s input layer does bias integration. Bias input is a slow process compared to graded spike input. Therefore, we tweak the input layer of PilotNet LIF to receive graded spike and integrate it on neuron’s current (u) state to achieve effective bias input.\n\nThe input encoder process does frame difference of subsequent frames to sparsify the input to the network.\n\nFor Loihi execution, it additionally compresses and sends the input data to the Loihi 2 chip.\n\nThe output of PilotNet LIF network is the output layer neuron’s voltage. We use a`VoltageReader`VoltageReaderto read the neuron voltage and scale the input appropriately using`AffineTransformer`AffineTransformer.\n\nFor Loihi execution,`VoltageReader`VoltageReaderadditionally communicates the read values from the Loihi 2 chip.\n\nMonitor is a lava process that visualizes the PilotNet network prediction in real-time. In addition, datalogger processes store the network predictions and ground truths.\n\nSwitching between Loihi 2 hardware and CPU simulation is as simple as changing the run configuration settings.\n\nPlot and compare the results with the dataset ground truth.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom lava.magma.core.run_configs import Loihi2SimCfg, Loihi2HwCfg\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.proc import io\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import RefPort\n\nfrom lava.lib.dl import netx\nfrom dataset import PilotNetDataset\nfrom utils import (\n    PilotNetEncoder, PilotNetDecoder, VoltageReader, PilotNetMonitor,\n    loihi2hw_exception_map, loihi2sim_exception_map\n)\n``````\n\n``````\n[2]:\n``````\n\n``````\nfrom lava.utils.system import Loihi2\nLoihi2.preferred_partition = 'oheogulch'\nloihi2_is_available = Loihi2.is_loihi2_available\n\nif loihi2_is_available:\n    print(f'Running on {Loihi2.partition}')\n    compression = io.encoder.Compression.DELTA_SPARSE_8\n    from lava.proc import embedded_io as eio\n    from lava.proc.embedded_io.state import Read as VoltageReader\nelse:\n    print(\"Loihi2 compiler is not available in this system. \"\n          \"This tutorial will execute on CPU backend.\")\n    compression = io.encoder.Compression.DENSE\n``````\n\n``````\nRunning on oheogulch\n``````\n\n``````\n[3]:\n``````\n\n``````\n# The input spike loads to dataloader at t=0\n# Gets transmitted through the embedded processor at t=1\n# and appears at the input layer at t=2\nnet = netx.hdf5.Network(net_config='network.net',\n                        reset_interval=16,\n                        reset_offset=3)\nprint(net)\n``````\n\n``````\n|   Type   |  W  |  H  |  C  | ker | str | pad | dil | grp |delay|\n|Input     |  200|   66|    3|     |     |     |     |     |False|\n|Conv      |   99|   32|   24| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   49|   15|   36| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   24|    7|   48| 3, 3| 2, 2| 0, 0| 1, 1|    1|False|\n|Conv      |   22|    4|   64| 3, 3| 1, 2| 0, 1| 1, 1|    1|False|\n|Conv      |   20|    2|   64| 3, 3| 1, 1| 0, 0| 1, 1|    1|False|\n|Dense     |    1|    1|  100|     |     |     |     |     |False|\n|Dense     |    1|    1|   50|     |     |     |     |     |False|\n|Dense     |    1|    1|   10|     |     |     |     |     |False|\n|Dense     |    1|    1|    1|     |     |     |     |     |False|\n``````\n\n``````\n[4]:\n``````\n\n``````\nprint(f'There are {len(net)} layers in network:')\n\nfor l in net.layers:\n    print(f'{l.__class__.__name__:5s} : {l.name:10s}, shape : {l.shape}')\n``````\n\n``````\nThere are 10 layers in network:\nInput : Process_1 , shape : (200, 66, 3)\nConv  : Process_3 , shape : (99, 32, 24)\nConv  : Process_6 , shape : (49, 15, 36)\nConv  : Process_9 , shape : (24, 7, 48)\nConv  : Process_12, shape : (22, 4, 64)\nConv  : Process_15, shape : (20, 2, 64)\nDense : Process_18, shape : (100,)\nDense : Process_21, shape : (50,)\nDense : Process_24, shape : (10,)\nDense : Process_27, shape : (1,)\n``````\n\n``````\n[5]:\n``````\n\n``````\nnum_samples = 201\nsteps_per_sample = net.reset_interval\nreadout_offset = len(net) + 2\nnum_steps = num_samples * steps_per_sample + 1\n``````\n\n``````\n[6]:\n``````\n\n``````\nfull_set = PilotNetDataset(\n    path='../data',\n    transform=net.in_layer.transform,  # input transform\n    visualize=True,  # visualize ensures the images are returned in sequence\n    sample_offset=10550,\n)\ntrain_set = PilotNetDataset(\n    path='../data',\n    transform=net.in_layer.transform,  # input transform\n    train=True,\n)\ntest_set = PilotNetDataset(\n    path='../data',\n    transform=net.in_layer.transform,  # input transform\n    train=False,\n)\n``````\n\n``````\n[7]:\n``````\n\n``````\ndataloader = io.dataloader.SpikeDataloader(dataset=full_set,\n                                           interval=steps_per_sample)\n``````\n\n``````\n[8]:\n``````\n\n``````\nnet.in_layer.neuron.du.init = -1  # Make current state persistent\n``````\n\n``````\n[9]:\n``````\n\n``````\ninput_encoder = PilotNetEncoder(shape=net.in_layer.shape,\n                                interval=steps_per_sample,\n                                offset=1,\n                                compression=compression)\n``````\n\n``````\n[10]:\n``````\n\n``````\noutput_adapter = VoltageReader(shape=net.out.shape,\n                               interval=steps_per_sample,\n                               offset=len(net) + 1)\noutput_decoder = PilotNetDecoder(shape=net.out.shape,\n                                 weight=1 / steps_per_sample / 32 / 64,\n                                 interval=steps_per_sample,\n                                 offset=len(net) + 2)\n``````\n\n``````\n[11]:\n``````\n\n``````\nmonitor = PilotNetMonitor(shape=net.inp.shape,\n                          transform=net.in_layer.transform,\n                          interval=steps_per_sample)\ngt_logger = io.sink.RingBuffer(shape=(1,), buffer=num_steps)\noutput_logger = io.sink.RingBuffer(shape=net.out.shape, buffer=num_steps)\n``````\n\n``````\n[12]:\n``````\n\n``````\ndataloader.ground_truth.connect(gt_logger.a_in)\ndataloader.s_out.connect(input_encoder.inp)\ninput_encoder.out.connect(net.in_layer.neuron.a_in)\n\noutput_adapter.connect_var(net.out_layer.neuron.v)\noutput_adapter.out.connect(output_decoder.inp)\noutput_decoder.out.connect(output_logger.a_in)\n\ndataloader.s_out.connect(monitor.frame_in)\ndataloader.ground_truth.connect(monitor.gt_in)\noutput_decoder.out.connect(monitor.output_in)\n``````\n\n``````\n[13]:\n``````\n\n``````\nif loihi2_is_available:\n    run_config = Loihi2HwCfg(exception_proc_model_map=loihi2hw_exception_map)\nelse:\n    run_config = Loihi2SimCfg(select_tag='fixed_pt',\n                              exception_proc_model_map=loihi2sim_exception_map)\nnet.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_config)\noutput = output_logger.data.get().flatten()\ngts = gt_logger.data.get().flatten()[::steps_per_sample]\nnet.stop()\nresult = output[readout_offset::steps_per_sample]\n``````\n\n``````\n[14]:\n``````\n\n``````\nplt.figure(figsize=(7, 5))\nplt.plot(np.array(gts), label='Ground Truth')\nplt.plot(result[1:].flatten(), label='Lava output')\nplt.xlabel(f'Sample frames (+10550)')\nplt.ylabel('Steering angle (radians)')\nplt.legend()\n``````\n\n``````\n[14]:\n``````\n\n``````\n<matplotlib.legend.Legend at 0x7ff370688850>\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/netx/utils.html",
    "title": "Utils — Lava  documentation",
    "content": "HDF5 net description manipulation utilities.\n\nProvides dictionary like access to h5py object without the h5py quirks\n\nfilename(strorNone,optional) – filename of h5py file to be loaded. It is only invoked if hdf5 file\nhandle`f`fis`None`None. Default is None.\n\nmode(str,optional) – file open mode, by default ‘r’.\n\nf(h5py.Fileorh5py.Group,optional) – hdf5 file object handle. Overwrites the function of filename if it is\nnot`None`None. Default is None.\n\nEnum for synapse sign mode. Options are {`MIXED:1`MIXED:1,`EXCITATORY:2`EXCITATORY:2and`INHIBITORY:2`INHIBITORY:2}.\n\nCalculates the number of delay bits required.\n\ndelays(np.ndarray) – delay vector\n\nnumber of delay bits.\n\nint\n\nOptimizes the weight matrix to best fit in Loihi’s synapse.\n\nweight(np.ndarray) – standard 8 bit signed weight matrix.\n\n`Tuple`Tuple[`ndarray`ndarray,`int`int,`int`int,[SYNAPSE_SIGN_MODE](https://lava-nc.org/lava-lib-dl/netx/utils.html#lava.lib.dl.netx.utils.SYNAPSE_SIGN_MODE)`SYNAPSE_SIGN_MODE`SYNAPSE_SIGN_MODE]\n\nnp.ndarray– optimized weight matrixint– weight bitsint– weight_exponentSYNAPSE_SIGN_MODE– synapse sign mode\n\nnp.ndarray– optimized weight matrix\n\nint– weight bits\n\nint– weight_exponent\n\nSYNAPSE_SIGN_MODE– synapse sign mode\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nSYNAPSE_SIGN_MODE\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/auto.html",
    "title": "Auto — Lava  documentation",
    "content": "Auto network generation module form network description. We support hdf5\nnetwork description for now. It is intended to load a model and perform\nfine tuning or use it as a pretrained feature extractor.\n\nCreates sequential network from hdf5 network description.\n\nnetwork_config(str) – name of network configuration description.\n\npersistent_state(bool) – flag for persistent state. Defaults to False.\n\nreduction(strorNone) – Reduction of output spike. Options are ‘sum’ or ‘mean’.\nNone means no reduction. Defaults to None.\n\nweight_norm(bool) – flag to enable weight norm. Defaults to False.\n\ncount_log(bool) – flag to enable count statistics. Defaults to False.\n\nnetwork module.\n\ntorch module\n\nMaps slayer class from neuron type.\n\nneuron_type(string) – neuron type description. None means cuba neuron. Defaults to None.\n\nneuron class and block class.\n\nneuron_class, block_class\n\nGets neuron parameters from the hdf5 description handle.\n\nneuron_handle(hdf5 handle) – handle to hdf5 object that describes the neuron.\n\nneuron_class(slayer.neuron.*) – neuron class type\n\ndictionary of neuron parameters.\n\ndict\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/axon/axon.html",
    "title": "Axon Module — Lava  documentation",
    "content": "Axon delay implementation.\n\nBases:`Module`Module\n\nLearnable axonal delay module. The delays operate on channel dimension.\n\nsampling_time(int) – Sampling time of delay. Defaults to 1.\n\nmax_delay(int) – Maximum allowable delay. Defaults to None.\n\ngrad_scale(float) – gradient scale parameter. Defaults to 1.\n\nthe delay parameter.\n\ntorch parameter\n\nExamples\n\nClamps delay to allowable range. Typically it is not needed to be\ncalled explicitly.\n\nApply delay to input tensor.\n\ninput(torch.tensor) – input tensor.\n\ndelayed tensor.\n\ntorch.tensor\n\nShape of the delay.\n\nDelay the signal in time.\n\ninput(torch.tensor) – Input signal. The last dimension is assumed to be time dimension.\n\ndelay(int) – Amount of delay to apply. Defaults to 1.\n\nsampling_time(int) – Sampling time of delay operation. Defaults to 1.\n\ndelayed signal\n\ntorch.tensor\n\nExamples\n\nDelta encoder implementation.\n\nBases:`Module`Module\n\nImplements delta differential encoding followed by thresholding.\nThe thresholds are learnable, individually or as a group.\n\n\\Delta x[t] &= x[t] - x[t-1] + r[t-1] \\\\\ny[t] &= \\begin{cases}\n    \\Delta x[t] &\\text{ if } \\Delta x[t] \\geq \\vartheta \\\\\n    0 &\\text{ otherwise}\n\\end{cases}\\\\\nr[t] &= \\Delta x[t] - y[t]\n\nFor cumulative error, output evaluation is changed to\n\ne[t] &= e[t] + \\Delta x[t]\\\\\ny[t] &= \\begin{cases}\n    \\Delta x[t] &\\text{ if } e[t] \\geq \\vartheta \\\\\n    0 &\\text{ otherwise}\\\\\ne[t] &= e[t] * (1 - \\mathcal{H}(|y[t]|))\n\\end{cases}\n\nthreshold(float) – threshold value.\n\nscale(int) – quantization step size. Defaults to 64.\n\ntau_grad(float) – threshold gradient relaxation parameter. Defaults to 1.\n\nscale_grad(float) – threshold gradient scaling parameter. Defaults to 1.\n\ncum_error(bool) – flag to enable cumulative error before thresholding.\nDefaults to False.\n\nshared_param(bool) – flag to enable shared threshold. Defaults to True.\n\npersistent_state(bool) – flag to enable persistent delta states. Defaults to False.\n\nrequires_grad(bool) – flag to enable threshold gradient. Defaults to False.\n\nshape of delta block. It is identified on runtime. The value is None\nbefore that.\n\ntorch shape\n\nprevious state of delta unit.\n\ntorch tensor\n\nresidual state of delta unit.\n\ntorch tensor\n\nerror state of delta unit.\n\ntorch tensor\n\nExamples\n\n>> delta = Delta(threshold=1)\n>> y = delta(x) # differential threshold encoding\n\nClamps the threshold value to[\\verb~1/scale~, \\infty).\n\nDevice property of object\n\nreturns the device memory where the object lives.\n\ntorch.device\n\nBases:`Module`Module\n\nLearnable axonal delay module. The delays operate on channel dimension.\n\nsampling_time(int) – Sampling time of delay. Defaults to 1.\n\nmax_delay(int) – Maximum allowable delay. Defaults to None.\n\ngrad_scale(float) – gradient scale parameter. Defaults to 1.\n\nthe delay parameter.\n\ntorch parameter\n\nExamples\n\nClamps delay to allowable range. Typically it is not needed to be\ncalled explicitly.\n\nApply delay to input tensor.\n\ninput(torch.tensor) – input tensor.\n\ndelayed tensor.\n\ntorch.tensor\n\nShape of the delay.\n\nBases:`Module`Module\n\nImplements delta differential encoding followed by thresholding.\nThe thresholds are learnable, individually or as a group.\n\n\\Delta x[t] &= x[t] - x[t-1] + r[t-1] \\\\\ny[t] &= \\begin{cases}\n    \\Delta x[t] &\\text{ if } \\Delta x[t] \\geq \\vartheta \\\\\n    0 &\\text{ otherwise}\n\\end{cases}\\\\\nr[t] &= \\Delta x[t] - y[t]\n\nFor cumulative error, output evaluation is changed to\n\ne[t] &= e[t] + \\Delta x[t]\\\\\ny[t] &= \\begin{cases}\n    \\Delta x[t] &\\text{ if } e[t] \\geq \\vartheta \\\\\n    0 &\\text{ otherwise}\\\\\ne[t] &= e[t] * (1 - \\mathcal{H}(|y[t]|))\n\\end{cases}\n\nthreshold(float) – threshold value.\n\nscale(int) – quantization step size. Defaults to 64.\n\ntau_grad(float) – threshold gradient relaxation parameter. Defaults to 1.\n\nscale_grad(float) – threshold gradient scaling parameter. Defaults to 1.\n\ncum_error(bool) – flag to enable cumulative error before thresholding.\nDefaults to False.\n\nshared_param(bool) – flag to enable shared threshold. Defaults to True.\n\npersistent_state(bool) – flag to enable persistent delta states. Defaults to False.\n\nrequires_grad(bool) – flag to enable threshold gradient. Defaults to False.\n\nshape of delta block. It is identified on runtime. The value is None\nbefore that.\n\ntorch shape\n\nprevious state of delta unit.\n\ntorch tensor\n\nresidual state of delta unit.\n\ntorch tensor\n\nerror state of delta unit.\n\ntorch tensor\n\nExamples\n\n>> delta = Delta(threshold=1)\n>> y = delta(x) # differential threshold encoding\n\nClamps the threshold value to[\\verb~1/scale~, \\infty).\n\nDevice property of object\n\nreturns the device memory where the object lives.\n\ntorch.device\n\nDelay the signal in time.\n\ninput(torch.tensor) – Input signal. The last dimension is assumed to be time dimension.\n\ndelay(int) – Amount of delay to apply. Defaults to 1.\n\nsampling_time(int) – Sampling time of delay operation. Defaults to 1.\n\ndelayed signal\n\ntorch.tensor\n\nExamples\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>axon_delay=Delay()>>>x_delayed=axon_delay(x)\n``````\n\n``````\n>>>x_delayed=delay(x,2)# delay x by 2 timesteps\n``````\n\n``````\n>>>axon_delay=Delay()>>>x_delayed=axon_delay(x)\n``````\n\n``````\n>>>x_delayed=delay(x,2)# delay x by 2 timesteps\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/axon/modules.html",
    "title": "Axon — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/block/block.html",
    "title": "Block Module — Lava  documentation",
    "content": "Base block class\n\nBases:`Module`Module\n\nAbstract affine transform class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights\nbefore synaptic operation. None means no transformation.\nDefaults to None.\n\ndynamics(bool,optional) – flag to enable neuron dynamics. If False, only the dendrite current\nis returned. Defaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None\nmeans no masking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input must be in`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract dense block class. This should never be instantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nBases:`Module`Module\n\nAbstract input block class. This should never be instantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract input block class. This should never be instantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input must be in`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nBases:`Module`Module\n\nAbstract time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nForward computation method. The input can be either of`NCT`NCTor`NCHWT`NCHWTformat.\n\nShape of the block.\n\nBases:`Module`Module\n\nAbstract Unpool block class. This should never be instantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nShape of the block.\n\nStep delay computation. This simulates the 1 timestep delay needed\nfor communication between layers.\n\nmodule(module) – python module instance\n\nx(torch.tensor) – Tensor data to be delayed.\n\nCUBA-LIF layer blocks\n\nBases:`Module`Module\n\nAbstract block class for Current Based Leaky Integrator neuron. This\nshould never be instantiated on it’s own.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractAffine](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAffine)`AbstractAffine`AbstractAffine\n\nCUBA LIF affine transform class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights\nbefore synaptic operation. None means no transformation.\nDefaults to None.\n\ndynamics(bool,optional) – flag to enable neuron dynamics. If False, only the dendrite current\nis returned. Defaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None\nmeans no masking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nCUBA LIF average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nCUBA LIF convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nCUBA LIF convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nCUBA LIF dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nCUBA LIF flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nCUBA LIF input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nCUBA LIF K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nCUBA LIF input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nCUBA LIF recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nCUBA LIF time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractCuba](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.cuba.AbstractCuba)`AbstractCuba`AbstractCuba,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nCUBA LIF Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nResonate and Fire layer blocks.\n\nBases:`Module`Module\n\nAbstract Resonate and Fire block class. This should never be\ninstantiated on it’s own.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractAffine](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAffine)`AbstractAffine`AbstractAffine\n\nResonate & Fire affine transform class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights\nbefore synaptic operation. None means no transformation.\nDefaults to None.\n\ndynamics(bool,optional) – flag to enable neuron dynamics. If False, only the dendrite current\nis returned. Defaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None\nmeans no masking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nResonate & Fire average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nResonate & Fire convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nResonate & Fire convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nResonate & Fire dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nResonate & Fire flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nResonate & Fire input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nResonate & Fire K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nResonate & Fire input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nResonate & Fire recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of RF parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nResonate & Fire time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf.AbstractRF)`AbstractRF`AbstractRF,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nResonate & Fire Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nResonate and Fire - Izhikevich layer blocks.\n\nBases:`Module`Module\n\nAbstract Resonate and Fire - Izhikevich block class. This should never\nbe instantiated on it’s own.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractAffine](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAffine)`AbstractAffine`AbstractAffine\n\nResonate & Fire Izhikevich affine transform class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights\nbefore synaptic operation. None means no transformation.\nDefaults to None.\n\ndynamics(bool,optional) – flag to enable neuron dynamics. If False, only the dendrite current\nis returned. Defaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None\nmeans no masking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nResonate & Fire Izhikevich average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nResonate & Fire Izhikevich convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nResonate & Fire Izhikevich convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nResonate & Fire Izhikevich dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nResonate & Fire Izhikevich flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nResonate & Fire Izhikevich input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nResonate & Fire Izhikevich K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nResonate & Fire Izhikevich input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nResonate & Fire Izhikevich recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nResonate & Fire Izhikevich time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractRFIz](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.rf_iz.AbstractRFIz)`AbstractRFIz`AbstractRFIz,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nResonate & Fire Izhikevich Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of RF-Izhikevich neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nAdaptive Leaky Integrate and Fire block layers.\n\nBases:`Module`Module\n\nAbstract Leaky Integrae and Fire block. This should never be\ninstantiated on it’s own.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nAdaptive LIF average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nAdaptive LIF convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nAdaptive LIF convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nAdaptive LIF dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nAdaptive LIF flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nAdaptive LIF input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nAdaptive LIF K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nAdaptive LIF input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nAdaptive LIF recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nAdaptive LIF time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractALIF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.alif.AbstractALIF)`AbstractALIF`AbstractALIF,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nAdaptive LIF Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive LIF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nAdaptive Resonate and Fire - Phase Threshold layer\n\nBases:`Module`Module\n\nAbstract Adaptive Resonate and Fire block. This should\nnever be instantiated on its own.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nAdaptive Resonate & Fire average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nAdaptive Resonate & Fire convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nAdaptive Resonate & Fire convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nAdaptive Resonate & Fire dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nAdaptive Resonate & Fire flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nAdaptive Resonate & Fire input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nAdaptive Resonate & Fire K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nAdaptive Resonate & Fire input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nAdaptive Resonate & Fire recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nAdaptive Resonate & Fire time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRF](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf.AbstractADRF)`AbstractADRF`AbstractADRF,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nAdaptive Resonate & Fire Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nAdaptive Resonate and Fire - Izhikevich layer blocks\n\nBases:`Module`Module\n\nAbstract Adaptive Resonate and Fire - Izhikevich block. This should\nnever be instantiated on its own.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nAdaptive Resonate & Fire Izhikevich average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nAdaptive Resonate & Fire Izhikevich convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nAdaptive Resonate & Fire Izhikevich convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nAdaptive Resonate & Fire Izhikevich dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nAdaptive Resonate & Fire Izhikevich flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nAdaptive Resonate & Fire Izhikevich input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractKWTA](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractKWTA)`AbstractKWTA`AbstractKWTA\n\nAdaptive Resonate & Fire Izhikevich K-Winner-Takes-All block class. This should never be\ninstantiated on its own. The formulation is described as below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                   + \\mathbf{R}\\,s_{out}[t-1]\n                   + \\alpha\\,(N-2K)\\right)\\\\\n\\mathbf{R} = \\begin{bmatrix}\na &-1 &\\cdots &-1\\\\\n-1 & a &\\cdots &-1\\\\\n\\vdots &\\vdots &\\ddots &\\vdots\\\\\n-1 &-1 &\\cdots & a\n\\end{bmatrix},\\qquad |a| < 1\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nnum_winners([type]) – number of winners.\n\nself_excitation(float,optional) – self excitation factor. Defaults to 0.5.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nAdaptive Resonate & Fire Izhikevich input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractRecurrent](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractRecurrent)`AbstractRecurrent`AbstractRecurrent\n\nAdaptive Resonate & Fire Izhikevich recurrent block class. This should never be instantiated on its\nown. The recurrent formulation is described below:\n\ns_\\text{out}[t] = f_s\\left(\\mathbf{W}\\,s_\\text{in}[t]\n                + \\mathbf{R}\\,s_{out}[t-1]\\right)\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter.Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\nrequires_grad(bool,optional) – flag for learnable recurrent synapse. Defaults to True.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractTimeDecimation](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractTimeDecimation)`AbstractTimeDecimation`AbstractTimeDecimation\n\nAdaptive Resonate & Fire Izhikevich time decimation block class. This should never be instantiated\non its own.\n\nfactor(int,optional) – number of time units to decimate in a single bin. Must be in\npowers of 2.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractADRFIZ](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.adrf_iz.AbstractADRFIZ)`AbstractADRFIZ`AbstractADRFIZ,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nAdaptive Resonate & Fire Izhikevich Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Adaptive RF-Izhikevich neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nSigma Delta layer blocks.\n\nBases:`Module`Module\n\nAbstract Sigma Delta block class. This should never be instantiated on\nit’s own.\n\nBases:[AbstractAverage](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractAverage)`AbstractAverage`AbstractAverage\n\nSigma Delta average block class. This should never be instantiated on its\nown.\n\nnum_outputs(int) – number of output population groups.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractConv](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConv)`AbstractConv`AbstractConv\n\nSigma Delta convolution block class. This should never be instantiated on\nits own.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolution stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolution padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolution dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractConvT](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractConvT)`AbstractConvT`AbstractConvT\n\nSigma Delta convolution Traspose block class. This should never be\ninstantiated on its own.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(int) – kernel size.\n\nstride(intortupleoftwo ints,optional) – convolutionT stride. Defaults to 1.\n\npadding(intortupleoftwo ints,optional) – convolutionT padding. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – convolutionT dilation. Defaults to 1.\n\ngroups(int,optional) – number of blocked connections. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractDense](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractDense)`AbstractDense`AbstractDense\n\nSigma Delta dense block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\nmask(bool array,optional) – boolean synapse mask that only enables relevant synapses. None means no\nmasking is applied. Defaults to None.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractFlatten](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractFlatten)`AbstractFlatten`AbstractFlatten\n\nSigma Delta flatten block class. This should never be instantiated on its\nown.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractInput](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractInput)`AbstractInput`AbstractInput\n\nSigma Delta input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nweight(float,optional) – weight for affine transform of input. None means no weight scaling.\nDefaults to None.\n\nbias(float,optional) – bias for affine transform of input. None means no bias shift.\nDefaults to None.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of\naverage event rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu\n\nSigma Delta output block class. The block is 8 bit quantization ready.\n\nneuron_params(dict) – a dictionary of sigma delta neuron parameters.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nHdf5 export method for the block.\n\nhandle(file handle) – hdf5 handle to export block description.\n\nShape of the block.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractPool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractPool)`AbstractPool`AbstractPool\n\nSigma Delta input block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of pooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of pooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of pooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of pooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\nBases:[AbstractSDRelu](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.sigma_delta.AbstractSDRelu)`AbstractSDRelu`AbstractSDRelu,[AbstractUnpool](https://lava-nc.org/lava-lib-dl/slayer/block/block.html#lava.lib.dl.slayer.block.base.AbstractUnpool)`AbstractUnpool`AbstractUnpool\n\nSigma Delta Unpool block class. The block is 8 bit quantization ready.\n\nneuron_params(dict,optional) – a dictionary of Sigma Delta neuron parameter. Defaults to None.\n\nkernel_size(intortupleoftwo ints) – size of unpooling kernel.\n\nstride(intortupleoftwo ints,optional) – stride of unpooling operation. Defaults to None.\n\npadding(intortupleoftwo ints,optional) – padding of unpooling operation. Defaults to 0.\n\ndilation(intortupleoftwo ints,optional) – dilation of unpooling kernel. Defaults to 1.\n\nweight_scale(int,optional) – weight initialization scaling. Defaults to 1.\n\nweight_norm(bool,optional) – flag to enable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function pointer or lambda that is applied to synaptic weights before\nsynaptic operation. None means no transformation. Defaults to None.\n\ndelay(bool,optional) – flag to enable axonal delay. Defaults to False.\n\ndelay_shift(bool,optional) – flag to simulate spike propagation delay from one layer to next.\nDefaults to True.\n\ncount_log(bool,optional) – flag to return event count log. If True, an additional value of average\nevent rate is returned. Defaults to False.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractCuba\n``````\n\n``````\nAbstractAffine\n``````\n\n``````\nAbstractAverage\n``````\n\n``````\nAbstractConv\n``````\n\n``````\nAbstractConvT\n``````\n\n``````\nAbstractDense\n``````\n\n``````\nAbstractFlatten\n``````\n\n``````\nAbstractInput\n``````\n\n``````\nAbstractKWTA\n``````\n\n``````\nAbstractPool\n``````\n\n``````\nAbstractRecurrent\n``````\n\n``````\nAbstractTimeDecimation\n``````\n\n``````\nAbstractUnpool\n``````\n\n``````\nAbstractRF\n``````\n\n``````\nAbstractRFIz\n``````\n\n``````\nAbstractALIF\n``````\n\n``````\nAbstractADRF\n``````\n\n``````\nAbstractADRFIZ\n``````\n\n``````\nAbstractSDRelu\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/block/modules.html",
    "title": "Blocks — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/classifier.html",
    "title": "Classifier — Lava  documentation",
    "content": "Classifier modules.\n\nMoving window based classifier. It produces a timeseries of\nclassification/prediction based on moving window estimate.\n\n\\text{rate: } {\\bf r}(t)\n    &= \\frac{1}{W}\\int_{t-W}^T {\\bf s}(t)\\,\\text dt \\\n\n\\text{confidence: } {\\bf c}(t) &= \\begin{cases}\n    \\frac{{\\bf r}(t)}{{\\bf r}(t)^\\top {\\bf1}}\n        &\\text{ if mode=probability} \\\\\n    \\frac{\\exp({\\bf r}(t))}{\\exp({\\bf r}(t))^\\top \\bf1}\n        &\\text{ if mode=softmax} \\\\\n    \\log\\left(\n        \\frac{\\exp({\\bf r}(t))}{\\exp({\\bf r}(t))^\\top \\bf1}\n    \\right) &\\text{ if mode=softmax}\n\\end{cases} \\\n\n\\text{prediction: } p(t) &= \\arg\\max({\\bf r}(t))\n\ntime_window(int) – size of moving window.\n\nmode(str) – confidence mode. One of ‘probability’|’softmax’|’logsoftmax’.\nDefaults to ‘probability’.\n\neps(float) – infinitesimal value. Defaults to 1e-6.\n\nExamples\n\nMoving window confidence.\n\nspike(torch tensor) – spike input.\n\nmode(str) – confidence mode. If it is None, the object’s mode is used.\nDefaults to None.\n\noutput confidence.\n\ntorch tensor\n\nExamples\n\nMoving window prediction.\n\nspike(torch tensor) – spike input.\n\noutput prediction.\n\ntorch tensor\n\nExamples\n\nMoving window spike rate.\n\nspike(torch tensor) – spike input.\n\nspike rate.\n\ntorch tensor\n\nExamples\n\nGlobal rate based classifier. It considers the event rate of the spike\ntrain over the entire duration as the confidence score.\n\n\\text{rate: } {\\bf r} &= \\frac{1}{T}\\int_T{\\bf s}(t)\\,\\text dt\\\n\n\\text{confidence: } {\\bf c} &= \\begin{cases}\n    \\frac{\\bf r}{\\bf r^\\top 1}\n        &\\text{ if mode=probability} \\\\\n    \\frac{\\exp({\\bf r})}{\\exp({\\bf r})^\\top \\bf1}\n        &\\text{ if mode=softmax} \\\\\n    \\log\\left(\n        \\frac{\\exp({\\bf r})}{\\exp({\\bf r})^\\top \\bf1}\n    \\right)\n        &\\text{ if mode=softmax}\n\\end{cases} \\\\\n\n\\text{prediction: } p &= \\arg\\max(\\bf r)\n\nExamples\n\nGiven spike train, returns the confidence of the output class based\non spike rate.\n\nspike(torch tensor) – spike tensor. First dimension is assumed to be batch, and last\ndimension is assumed to be time. Spatial dimensions are collapsed\nby default.\n\nmode(str) – confidence mode. One of ‘probability’|’softmax’|’logsoftmax’.\nDefaults to ‘probability’.\n\neps(float) – infinitesimal value. Defaults to 1e-6.\n\nconfidence.\n\ntorch tensor\n\nExamples\n\nGiven spike train, predicts the output class based on spike rate.\n\nspike(torch tensor) – spike tensor. First dimension is assumed to be batch, and last\ndimension is assumed to be time. Spatial dimensions are collapsed\nby default.\n\nindices of max spike activity.\n\ntorch tensor\n\nExamples\n\nGiven spike train, returns the output spike rate.\n\nspike(torch tensor) – spike tensor. First dimension is assumed to be batch, and last\ndimension is assumed to be time. Spatial dimensions are collapsed\nby default.\n\nspike rate.\n\ntorch tensor\n\nExamples\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>classifier=MovingWindow(20)>>>prediction=classifier(spike)\n``````\n\n``````\n>>>confidence=classifier.confidence(spike)\n``````\n\n``````\n>>>prediction=classifier.predict(spike)\n``````\n\n``````\n>>>rate=classifier.rate(spike)\n``````\n\n``````\n>>>classifier=Rate>>>prediction=classifier(spike)\n``````\n\n``````\n>>>confidence=classifier.confidence(spike)>>>confidence=Rate.confidence(spike)\n``````\n\n``````\n>>>prediction=classifier.predict(spike)>>>prediction=Rate.predict(spike)\n``````\n\n``````\n>>>rate=classifier.rate(spike)>>>rate=Rate.rate(spike)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/dendrite/dendrite.html",
    "title": "Dendrite Module — Lava  documentation",
    "content": "Sigma Decoder implementation.\n\nBases:`Module`Module\n\nSigma decoder implementation.\n\npersistent_state(bool) – flag to enable persistent state. Defaults to False.\n\nshape of the sigma unit. It is initialized on first run. The value is\nNone before initialization.\n\ntorch shape\n\nprevious state of sigma unit.\n\ntorch tensor\n\nDictionary of device parameters.\n\nBases:`Module`Module\n\nSigma decoder implementation.\n\npersistent_state(bool) – flag to enable persistent state. Defaults to False.\n\nshape of the sigma unit. It is initialized on first run. The value is\nNone before initialization.\n\ntorch shape\n\nprevious state of sigma unit.\n\ntorch tensor\n\nDictionary of device parameters.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/dendrite/modules.html",
    "title": "Dendrite — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/index.html",
    "title": "SLAYER — Lava  documentation",
    "content": "Contents:\n\n[Index](https://lava-nc.org/genindex.html)Index\n\n[Module Index](https://lava-nc.org/py-modindex.html)Module Index\n\n[Search Page](https://lava-nc.org/search.html)Search Page\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nSparsityEnforcer\n``````\n\n``````\nSpikeMax\n``````\n\n``````\nSpikeMoid\n``````\n\n``````\nSpikeRate\n``````\n\n``````\nSpikeTime\n``````\n\n``````\nMovingWindow\n``````\n\n``````\nRate\n``````\n\n``````\nEvent\n``````\n\n``````\nencode_1d_spikes()\n``````\n\n``````\nencode_2d_spikes()\n``````\n\n``````\nencode_np_spikes()\n``````\n\n``````\nread_1d_spikes()\n``````\n\n``````\nread_2d_spikes()\n``````\n\n``````\nread_np_spikes()\n``````\n\n``````\ntensor_to_event()\n``````\n\n``````\nSequentialNetwork\n``````\n\n``````\nget_classes()\n``````\n\n``````\nget_neuron_params()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/io.html",
    "title": "Input/Output — Lava  documentation",
    "content": "Spike/event Input Output and visualization module.\n\nThis class provides a way to store, read, write and visualize spike\nevent.\n\nx (numpy int array): x index of spike event.\n\ny (numpy int array): y index of spike event\n(not used if the spatial dimension is 1).\n\nc (numpy int array): channel index of spike event.\n\nt (numpy double array): timestamp of spike event.\nTime is assumed to be in ms.\n\np (numpy int or double array): payload of spike event.\nNone for binary spike.\n\ngraded (bool): flag to indicate graded or binary spike.\n\nx_event(int array) – x location of event\n\ny_event(int arrayorNone) – y location of event\n\nc_event(int array) – c location of event\n\nt_event(int arrayorfloat array) – time of event\n\npayload(int arrayorfloat arrayorNone) – payload of event. None for binary event. Defaults to None.\n\nExamples\n\nGet animation object for spike event.\n\nfig(int) – plot figure ID. Defaults to None.\n\nframe_rate(int) – frame rate of visualization. Defaults to 24.\n\npre_compute_frames(bool) – flag to enable precomputation of frames for faster visualization.\nDefaults to True.\n\nrepeat(bool) – flag to enable repeat of animation. Defaults to False.\n\nmatplotlib anim object.\n\nanim\n\nExamples\n\nReturns a numpy tensor that contains the spike events sampled in\nbins of`sampling_time`sampling_time. The tensor is of dimension\n(channels, height, width, time) or``CHWT``.\n\nempty_tensor(numpyortorch tensor) – an empty tensor to hold spike data .\n\nsampling_time(float) – the width of time bin. Defaults to 1.\n\nrandom_shift(bool) – flag to randomly shift the sample in time. Defaults to False.\n\nbinning_mode(str) – the way spikes are binned. Options are ‘OR’|’SUM’. If the event is\ngraded binning mode is overwritten to ‘SUM’. Defaults to ‘OR’.\n\nspike tensor.\n\nnumpy or torch tensor\n\nExamples\n\nVisualizes spike event.\n\nfig(int) – plot figure ID. Defaults to None.\n\nframe_rate(int) – frame rate of visualization. Defaults to 24.\n\npre_compute_frames(bool) – flag to enable precomputation of frames for faster visualization.\nDefaults to True.\n\nrepeat(bool) – flag to enable repeat of animation. Defaults to False.\n\nExamples\n\nReturns a numpy tensor that contains the spike events sampled in\nbins of`sampling_time`sampling_time. The array is of dimension\n(channels, height, time) or``CHT`` for 1D data. The array is of\ndimension (channels, height, width, time) or``CHWT`` for 2D data.\n\nsampling_time(int) – event data sampling time. Defaults to 1.\n\ndim(intorNone) – desired dimension. It is inferred if None. Defaults to None.\n\nspike tensor.\n\nnp array\n\nExamples\n\nWrites one dimensional binary spike file from a td_event event.\n\nEach spike event is represented by a 40 bit number.\n\nFirst 16 bits (bits 39-24) represent the neuronID.\n\nBit 23 represents the sign of spike event: 0=>OFF event, 1=>ON event.\n\nthe last 23 bits (bits 22-0) represent the spike event timestamp in\nmicroseconds.\n\nfilename(str) – name of spike file.\n\ntd_event(event) – spike event object\n\nExamples\n\nWrites two dimensional binary spike file from a td_event event.\nIt is the same format used in neuromorphic datasets NMNIST & NCALTECH101.\n\nEach spike event is represented by a 40 bit number.\n\nFirst 8 bits (bits 39-32) represent the xID of the neuron.\n\nNext 8 bits (bits 31-24) represent the yID of the neuron.\n\nBit 23 represents the sign of spike event: 0=>OFF event, 1=>ON event.\n\nThe last 23 bits (bits 22-0) represent the spike event timestamp in\nmicroseconds.\n\nfilename(str) – name of spike file.\n\ntd_event(event) – spike event object\n\nExamples\n\nWrites td_event event into numpy file.\n\nfilename(str) – name of spike file.\n\ntd_event(event) – spike event.\n\nfmt(str) – format of numpy event ordering. Options are ‘xypt’. Defaults to ‘xypt’.\n\ntime_unit(float) – scale factor that converts the data to seconds. Defaults to 1e-3.\n\nExamples\n\nReads one dimensional binary spike file and returns a td_event event.\n\nEach spike event is represented by a 40 bit number.\n\nFirst 16 bits (bits 39-24) represent the neuronID.\n\nBit 23 represents the sign of spike event: 0=>OFF event, 1=>ON event.\n\nmicroseconds.\n\nfilename(str) – name of spike file.\n\nspike event.\n\n[Event](https://lava-nc.org/lava-lib-dl/slayer/io.html#lava.lib.dl.slayer.io.Event)Event\n\nExamples\n\nReads two dimensional binary spike file and returns a td_event event.\nIt is the same format used in neuromorphic datasets NMNIST & NCALTECH101.\n\nEach spike event is represented by a 40 bit number.\n\nFirst 8 bits (bits 39-32) represent the xID of the neuron.\n\nNext 8 bits (bits 31-24) represent the yID of the neuron.\n\nBit 23 represents the sign of spike event: 0=>OFF event, 1=>ON event.\n\nThe last 23 bits (bits 22-0) represent the spike event timestamp in\nmicroseconds.\n\nfilename(str) – name of spike file.\n\nspike event.\n\n[Event](https://lava-nc.org/lava-lib-dl/slayer/io.html#lava.lib.dl.slayer.io.Event)Event\n\nExamples\n\nReads numpy spike event and returns a td_event event.\nThe numpy array is assumed to be of nEvent x event dimension.\n\nfilename(str) – name of spike file.\n\nfmt(str) – format of numpy event ordering. Options are ‘xypt’. Defaults to ‘xypt’.\n\ntime_unit(float) – scale factor that converts the data to seconds. Defaults to 1e-3.\n\nspike object.\n\n[Event](https://lava-nc.org/lava-lib-dl/slayer/io.html#lava.lib.dl.slayer.io.Event)Event\n\nExamples\n\nReturns td_event event from a numpy or torch tensor (of dimension 3 or 4).\nThe array or tensor must be of dimension (channels, height, time) or`CHT`CHTfor 1D data.\nThe array or tensor must be of dimension (channels, height, width, time) or`CHWT`CHWTfor 2D data.\n\nspike_tensor(numpyortorch tensor) – spike tensor.\n\nsampling_time(float) – the width of time bin. Defaults to 1.\n\nspike event\n\n[Event](https://lava-nc.org/lava-lib-dl/slayer/io.html#lava.lib.dl.slayer.io.Event)Event\n\nExamples\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>td_event=Event(x_event,y_event,c_event,t_event)\n``````\n\n``````\n>>>anim=self.anim()\n``````\n\n``````\n>>>spike=td_event.fill_tensor(torch.zeros((2,240,180,5000)))\n``````\n\n``````\n>>>self.show()\n``````\n\n``````\n>>>spike=td_event.to_tensor()\n``````\n\n``````\n>>>encode_1d_spikes(file_path,td_event)\n``````\n\n``````\n>>>encode_2d_spikes(file_path,td_event)\n``````\n\n``````\n>>>encode_np_spikes(file_path,td_event)>>>encode_np_spikes(file_path,td_event,fmt='xypt')\n``````\n\n``````\n>>>td_event=read_1d_spikes(file_path)\n``````\n\n``````\n>>>td_event=read_2d_spikes(file_path)\n``````\n\n``````\n>>>td_event=read_np_spikes(file_path)>>>td_event=read_np_spikes(file_path,fmt='xypt')>>>td_event=read_np_spikes(file_path,time_unit=1e-6)\n``````\n\n``````\n>>>td_event=tensor_to_Event(spike)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/loss.html",
    "title": "Loss — Lava  documentation",
    "content": "This module provides some pre-built loss methods to be used with\nspike-train. Standard PyTorch loss are also compatible.\n\nEvent sparsity enforcement module. Penalizes event rate higher than\na specific value.\n\nmax_rate(float,optional) – Rate above which the events are penalized, by default 0.01.\n\nlam(float,optional) – Ratio of event rate loss scaling, by default 1.0.\n\nAppends loss tickets given the state of input tensors.\n\nx(torch.tensor) – Input tensor.\n\n`None`None\n\nClear all gathered sparsity loss.\n\n`None`None\n\nAccumulate sparsity loss.\n\nSpike max (NLL) loss.\n\nL &= \\begin{cases}\n    -\\int_T\n        {\\bf 1}[\\text{label}]^\\top\n        \\log(\\boldsymbol p(t))\\,\\text dt\n        &\\text{ if moving window}\\\\\n    -{\\bf 1}[\\text{label}]^\\top\n    \\log(\\boldsymbol p) &\\text{ otherwise}\n\\end{cases}\n\nNote: input is always collapsed in spatial dimension.\n\nmoving_window(int) – size of moving window. If not None, assumes label to be specified\nat every time step. Defaults to None.\n\nmode(str) – confidence mode. One of ‘probability’|’softmax’.\nDefaults to ‘probability’.\n\nreduction(str) – loss reduction method. One of ‘sum’|’mean’. Defaults to ‘sum’.\n\nForward computation of loss.\n\nSpikeMoid (BCE) loss.\n\n\\text{if sliding window:} \\quad\np(t) = \\sigma\\left(\\frac{r(t) - \\theta}{\\alpha}\\right) \\\\\n\\text{otherwise:} \\quad\np = \\sigma\\left(\\frac{r - \\theta}{\\alpha}\\right)\n\nr signifies a spike rate calculated over the time dimension\n\n\\mathcal{L} = \\begin{cases}\n    -\\int_T \\hat{y}(t) \\cdot \\log{p(t)}\n    + (1 - \\hat{y}(t)) \\cdot \\log{(1 - p(t))}\\,\\text{d}t\n    &\\text{if sliding window} \\\\\n    -\\left(\\hat{y} \\cdot \\log{p}\n    + (1 - \\hat{y}) \\cdot \\log{(1 - p)}\\right)\n    &\\text{otherwise}\n\\end{cases}\n\nNote: input is always collapsed in the spatial dimension.\nr signifies a spike rate calculated over the time dimension\n\nmoving_window(int) – size of moving window. If not None, assumes label to be specified\nat every time step. Defaults to None.\n\nreduction(str) – loss reduction method. One of ‘sum’|’mean’. Defaults to ‘sum’.\n\nalpha(int) – Sigmoid temperature parameter. Defaults to 1.\n\ntheta(int) – Bias term for logits. Defaults to 1.\n\nForward computation of loss.\n\nSpike rate loss.\n\n\\hat {\\boldsymbol r} &=\n    r_\\text{true}\\,{\\bf 1}[\\text{label}] +\n    r_\\text{false}\\,(1 - {\\bf 1}[\\text{label}])\\\n\nL &= \\begin{cases}\n\\frac{1}{2}\\int_T(\n    {\\boldsymbol r}(t) - \\hat{\\boldsymbol r}(t)\n)^\\top {\\bf 1}\\,\\text dt &\\text{ if moving window}\\\\\n\\frac{1}{2}(\n    \\boldsymbol r - \\hat{\\boldsymbol r}\n)^\\top 1 &\\text{ otherwise}\n\\end{cases}\n\nNote: input is always collapsed in spatial dimension.\n\ntrue_rate(float) – true spiking rate.\n\nfalse_rate(float) – false spiking rate.\n\nmoving_window(int) – size of moving window. If not None, assumes label to be specified\nat every time step. Defaults to None.\n\nreduction(str) – loss reduction method. One of ‘sum’|’mean’. Defaults to ‘sum’.\n\nForward computation of loss.\n\nSpike-time based loss. It is similar to van Rossum distance between\noutput and desired spike train.\n\nL = \\int_0^T \\left( \\varepsilon * (s - \\hat{s}) \\right)(t)^2\\,\n    \\text{d}t\n\ntime_constant(int) – time constant of low pass filter. Defaults to 5.\n\nlength(int) – length of low pass filter. Defaults to 100.\n\nfilter_order(int) – order of low pass filter. Defaults to 1.\n\nreduction(str) – mean square reduction. Options are ‘mean’|’sum’. Defaults to ‘sum’.\n\nForward computation of loss.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/neuron/modules.html",
    "title": "Neuron — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html",
    "title": "Neuron Modules — Lava  documentation",
    "content": "Abstract neuron base class.\n\nBases:`Module`Module\n\nThis is an abstract class that governs the minimal basic functionality\nof a neuron object.\n\nthreshold(floatortorch tensor) – neuron threshold.\n\ntau_grad(float,optional) – controls the relaxation of spike function gradient. It determines\nthe scope of voltage/state around the neuron threshold that\neffectively contributes to the error. Default is 1\n\nscale_grad(float,optional) – controls the scale of spike function gradient. It controls the\ngradient flow across layers. It should be increased if there is\nvanishing gradient and increased if there is exploding gradient.\nDefault is 1\n\np_scale(int,optional) – scaling factor of neuron parameter. Default is 1\n\nw_scale(int,optional) – scaling factor for dendrite input (weighted spikes). It’s good to\ncompute synaptic operations and its gradients on smaller range.\nw_scale scales down the synaptic weights for quantization.\nThe actual synaptic weights must be descaled. Default is 1\n\ns_scale(int,optional) – scaling factor for neuron states. The fixed percision neuron states\nare scaled down by s_scale so that they are in a reasonable range\nfor gradient flow. Default is 1\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Default is None\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no dropout. Default is None\n\npersistent_state(bool,optional) – flag to enable/disable persitent state between iterations.\nDefault is False\n\nshared_param(bool,optional) – flag to enable/disable shared parameter for neuron group. If False,\nidividual parameters are assigned on a per-channel basis.\nDefault is True\n\nrequires_grad(bool,optional) – flag to enable/disable learnable neuron decays. Default is True\n\ncomplex(bool,optional) – flag to indicate real or complex neuron. Defaul\n\nthreshold\n\ntau_grad\n\nscale_grad\n\np_scale\n\nw_scale\n\ns_scale\n\nnorm\n\ndropout\n\npersistent_state\n\nshared_param\n\ncomplex\n\nIt is False by default. There shall be no constructor access to this\nflag. If desired, it should be explicitly set.\n\nQuantization method for 8 bit equivalent input when descaled.\nThis should be linked with synapse instance.\n\nweight(torch.tensor) – synaptic weight.\n\ndescale(Bool) – flag to scale/descale the weight (Default value = False)\n\nquantized weight.\n\ntorch.tensor\n\nExamples\n\nIt can be used like a normal function. But the intended use is as\nfollows\n\nGet voltage-threshold-mantessa parameter.\n\nGet weight exponent.\n\nCUBA neuron model.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of Loihi CUBA neuron.\n\nu[t] &= (1 - \\alpha_u)\\,u[t-1] + x[t] \\\n\nv[t] &= (1 - \\alpha_v)\\,v[t-1] + u[t] + \\text{bias} \\\n\ns[t] &= v[t] \\geq \\vartheta \\\n\nv[t] &= v[t]\\,(1-s[t])\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\ncurrent_decay(floatortuple) – the fraction of current decay per time step. If`shared_param`shared_paramis False, then it can be specified as a tuple (min_decay, max_decay).\n\nvoltage_decay(floatortuple) – the fraction of voltage decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple (min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1<<6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations. Defaults to\nFalse.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment current decay parameter to be used for configuring\nLoihi hardware.\n\nThe compartment voltage decay parameter to be used for configuring\nLoihi hardware.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to an input. The input shape must match with the neuron shape.\nFor the first time, the neuron shape is determined from the input\nautomatically.\n\ninput(torch tensor) – Input tensor.\n\ntorch tensor– current response of the neuron.torch tensor– voltage response of the neuron.\n\ntorch tensor– current response of the neuron.\n\ntorch tensor– voltage response of the neuron.\n\nComputes the full response of the neuron instance to an input.\nThe input shape must match with the neuron shape. For the first time,\nthe neuron shape is determined from the input automatically.\n\ninput(torch tensor) – Input tensor.\n\nspike response of the neuron.\n\ntorch tensor\n\nRefractory delay.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the voltage timeseries. It assumes the\nreset dynamics is already applied.\n\nvoltage(torch tensor) – neuron voltage dynamics\n\nspike output\n\ntorch tensor\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nResonate and Fire neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of RF neuron.\n\n\\mathfrak{Re}(z[t]) &= (1-\\alpha)(\\cos\\phi\\ \\mathfrak{Re}(z[t-1])\n    - \\sin\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Re}(x[t]) + \\text{real bias} \\\n\n\\mathfrak{Im}(z[t]) &= (1-\\alpha)(\\sin\\phi\\ \\mathfrak{Re}(z[t-1])\n    + \\cos\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Im}(x[t]) + \\text{imag bias}  \\\n\ns[t] &= |z[t]| \\geq \\vartheta \\text{ and } \\arg(z[t])=0\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\nperiod(floatortuple) – period of the neuron. If`shared_param`shared_paramis False, then it can be\nspecified as a tuple (min_period, max_period).\n\ndecay(floatortuple) – decay factor of the neuron. If`shared_param`shared_paramis False, then it can\nbe specified as a tuple (min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1  <<  6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations. Defaults to\nFalse.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nlog_init(bool,optional) – if True, initialized the natural frequency in log spaced range.\nDefault is True.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment cos decay parameter to be used for configuration.\n\nThe compartment sin decay parameter to be used for configuration.\n\nThe decay parameter of the neuron.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to a complex input tuple. The input shape must match with the\nneuron shape. For the first time, the neuron shape is determined from\nthe input automatically. It is essentially a resonator dynamics.\n\ninput(tupleoftorch tensors) – Complex input tuple of tensor, i.e. (real_input, imag_input).\n\ntorch tensor– real response of the neuron.torch tensor– imaginary response of the neuron.\n\ntorch tensor– real response of the neuron.\n\ntorch tensor– imaginary response of the neuron.\n\nComputes the full response of the neuron instance to a complex\ninput tuple. The input shape must match with the neuron shape. For the\nfirst time, the neuron shape is determined from the input\nautomatically.\n\ninput– Complex input tuple of tensor, i.e. (real_input, imag_input).\n\nspike response of the neuron.\n\ntorch tensor\n\nThe frequency of neuron oscillation.\n\nThe lambda parameter of the neuron.\n\nThe period of the neuron oscillation.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the real and imaginary states.\n\nreal(torch tensor) – real state\n\nimag(torch tensor) – imaginary state\n\nspike output\n\ntorch tensor\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nResonate and Fire Izhikevich neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of RF Izhikevich neuron.\n\n\\mathfrak{Re}(z[t]) &= (1-\\alpha)(\\cos\\phi\\ \\mathfrak{Re}(z[t-1])\n    - \\sin\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Re}(x[t]) + \\text{real bias} \\\n\n\\mathfrak{Im}(z[t]) &= (1-\\alpha)(\\sin\\phi\\ \\mathfrak{Re}(z[t-1])\n    + \\cos\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Im}(x[t]) + \\text{imag bias}\\\n\ns[t] &= \\mathfrak{Im}(z[t]) \\geq \\vartheta \\\n\n\\mathfrak{Re}(z[t]) &= \\mathfrak{Re}(z[t])\\,(1-s[t])\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\nperiod(floatortuple) – period of the neuron. If`shared_param`shared_paramis False, then it can be\nspecified as a tuple (min_period, max_period).\n\ndecay(floatortuple) – decay factor of the neuron. If`shared_param`shared_paramis False, then it can\nbe specified as a tuple (min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1 << 6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations. Defaults to\nFalse.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nlog_init(bool,optional) – if True, initialized the natural frequency in log spaced range.\nDefault is True.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment cos decay parameter to be used for configuration.\n\nThe compartment sin decay parameter to be used for configuration.\n\nThe decay parameter of the neuron.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to a complex input tuple. The input shape must match with the\nneuron shape. For the first time, the neuron shape is determined from\nthe input automatically. It is essentially a resonator dynamics with\nIzhikevich reset.\n\ninput(tupleoftorch tensors) – Complex input tuple of tensor, i.e. (real_input, imag_input).\n\ntorch tensor– real response of the neuron.torch tensor– imaginary response of the neuron.\n\ntorch tensor– real response of the neuron.\n\ntorch tensor– imaginary response of the neuron.\n\nComputes the full response of the neuron instance to a complex\ninput tuple. The input shape must match with the neuron shape. For the\nfirst time, the neuron shape is determined from the input\nautomatically.\n\ninput– Complex input tuple of tensor, i.e. (real_input, imag_input).\n\nspike response of the neuron.\n\ntorch tensor\n\nThe frequency of neuron oscillation.\n\nThe lambda parameter of the neuron.\n\nThe period of the neuron oscillation.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the real and imaginary states.\n\nreal(torch tensor) – real state\n\nimag(torch tensor) – imaginary state\n\nspike output\n\ntorch tensor\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nAdaptive Leaky Integrate and Fire neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of Adaptive LIF neuron.\n\nu[t] &= (1-\\alpha_u)\\,u[t-1] + x[t] + \\text{bias} \\\n\nv[t] &= (1-\\alpha_v)\\,v[t-1] + u[t] \\\n\n\\vartheta[t] &= (1-\\alpha_{\\vartheta})\\,(\\vartheta[t-1]\n    - \\vartheta_0) + \\vartheta_0 \\\n\nr[t] &= (1-\\alpha_r)\\,r[t-1] \\\n\ns[t] &= (v[t] - r[t]) \\geq \\vartheta[t] \\\n\nr[t] &= r[t] + 2\\,\\vartheta[t] \\\n\n\\vartheta[t] &= \\vartheta[t] + \\vartheta_{\\text{step}}\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – base neuron threshold.\n\nthreshold_step(float) – the increase in threshold after spike.\n\ncurrent_decay(floatortuple) – the fraction of current decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple (min_decay, max_decay).\n\nvoltage_decay(floatortuple) – the fraction of voltage decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple (min_decay, max_decay).\n\nthreshold_decay(floatortuple) – the fraction of threshold decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple (min_decay, max_decay).\n\nrefractory_decay(floatortuple) – the fraction of refractory decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple (min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1 << 6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations.\nDefaults to False.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment current decay parameter to be used for configuring\nLoihi hardware.\n\nThe compartment refractory decay parameter to be used for\nconfiguring Loihi hardware.\n\nThe compartment threshold decay parameter to be used for configuring\nLoihi hardware.\n\nThe compartment voltage decay parameter to be used for configuring\nLoihi hardware.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to an input. The input shape must match with the neuron shape.\nFor the first time, the neuron shape is determined from the input\nautomatically.\n\ninput(torch tensor) – Input tensor.\n\ntorch tensor– current response of the neuron.torch tensor– voltage response of the neuron.torch tensor– adaptive threshold of the neuron.torch tensor– refractory response of the neuorn.\n\ntorch tensor– current response of the neuron.\n\ntorch tensor– voltage response of the neuron.\n\ntorch tensor– adaptive threshold of the neuron.\n\ntorch tensor– refractory response of the neuorn.\n\nComputes the full response of the neuron instance to an input.\nThe input shape must match with the neuron shape. For the first time,\nthe neuron shape is determined from the input automatically.\n\ninput(torch tensor) – Input tensor.\n\nspike response of the neuron.\n\ntorch tensor\n\nRefractory delay.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the voltage timeseries.\n\nvoltage(torch tensor) – neuron voltage dynamics of the neuron.\n\nthreshold(torch tensor) – threshold dynamics of the neuron.\n\nthreshold– refractory dynamics of the neuron.\n\nspike output\n\ntorch tensor\n\nGet voltage-threshold step parameter.\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nAdaptive RF Izhikevich neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of RF neuron.\n\n\\mathfrak{Re}(z[t]) &= (1-\\alpha)(\\cos\\phi\\ \\mathfrak{Re}(z[t-1])\n    - \\sin\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Re}(x[t]) + \\text{real bias}\\\n\n\\mathfrak{Im}(z[t]) &= (1-\\alpha)(\\sin\\phi\\ \\mathfrak{Re}(z[t-1])\n    + \\cos\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Im}(x[t]) + \\text{imag bias} \\\n\n\\vartheta[t] &= (1-\\alpha_{\\vartheta})\\,\n    (\\vartheta[t-1] - \\vartheta_0) + \\vartheta_0 \\\n\nr[t] &= (1-\\alpha_r)\\,r[t-1] \\\n\ns[t] &= |z[t]| \\geq (\\vartheta[t] + r[t]) \\text{ and } \\arg(z[t])=0\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\nthreshold_step(float) – the increase in threshold after spike.\n\nperiod(floatortuple) – period of the neuron. If`shared_param`shared_paramis False, then it can be\nspecified as a tuple (min_period, max_period).\n\ndecay(floatortuple) – decay factor of the neuron. If`shared_param`shared_paramis False, then it can\nbe specified as a tuple (min_decay, max_decay).\n\nthreshold_decay(floatortuple) – the fraction of threshold decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple : min_decay, max_decay).\n\nrefractory_decay(floatortuple) – the fraction of refractory decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple : min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1  <<  6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations.\nDefaults to False.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nlog_init(bool,optional) – if True, initialized the natural frequency in log spaced range.\nDefault is True.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment cos decay parameter to be used for configuration.\n\nThe compartment refractory decay parameter to be used for\nconfiguring Loihi hardware.\n\nThe compartment sin decay parameter to be used for configuration.\n\nThe compartment threshold decay parameter to be used for configuring\nLoihi hardware.\n\nThe decay parameter of the neuron.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to a complex input tuple. The input shape must match with the\nneuron shape. For the first time, the neuron shape is determined from\nthe input automatically. It is essentially a resonator dynamics with\nadaptive threshold and refractory response.\n\ninput(tupleoftorch tensors) – Complex input tuple of tensor, i.e. (real_input, imag_input).\n\ntorch tensor– real response of the neuron.torch tensor– imaginary response of the neuron.torch tensor– adaptive threshold of the neuron.torch tensor– refractory response of the neuorn.\n\ntorch tensor– real response of the neuron.\n\ntorch tensor– imaginary response of the neuron.\n\ntorch tensor– adaptive threshold of the neuron.\n\ntorch tensor– refractory response of the neuorn.\n\nomputes the full response of the neuron instance to a complex\ninput tuple. The input shape must match with the neuron shape. For the\nfirst time, the neuron shape is determined from the input\nautomatically.\n\ninput– Complex input tuple of tensor, i.e. (real_input, imag_input).\n\nspike response of the neuron.\n\ntorch tensor\n\nThe frequency of neuron oscillation.\n\nThe lambda parameter of the neuron.\n\nThe period of the neuron oscillation.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the real and imaginary states.\n\nreal(torch tensor) – real dynamics of the neuron.\n\nimag(torch tensor) – imaginary dynamics of the neuron.\n\nthreshold(torch tensor) – threshold dynamics of the neuron.\n\nrefractory(torch tensor) – refractory dynamics of the neuron.\n\nspike output\n\ntorch tensor\n\nGet voltage-threshold step parameter.\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nAdaptive RF Izhikevich neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of Adaptive RF Izhikevich neuron.\n\n\\mathfrak{Re}(z[t]) &= (1-\\alpha)(\\cos\\phi\\ \\mathfrak{Re}(z[t-1])\n    - \\sin\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Re}(x[t]) + \\text{real bias}\\\n\n\\mathfrak{Im}(z[t]) &= (1-\\alpha)(\\sin\\phi\\ \\mathfrak{Re}(z[t-1])\n    + \\cos\\phi\\ \\mathfrak{Im}(z[t-1]))\n    + \\mathfrak{Im}(x[t]) + \\text{imag bias}\\\n\n\\vartheta[t] &= (1-\\alpha_{\\vartheta})\\,\n    (\\vartheta[t-1] - \\vartheta_0) + \\vartheta_0 \\\n\nr[t] &= (1-\\alpha_r)\\,r[t-1] \\\n\ns[t] &= \\mathfrak{Im}(z[t]) \\geq (\\vartheta[t] + r[t])\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\nthreshold_step(float) – the increase in threshold after spike.\n\nperiod(floatortuple) – period of the neuron. If`shared_param`shared_paramis False, then it can be\nspecified as a tuple (min_period, max_period).\n\ndecay(floatortuple) – decay factor of the neuron. If`shared_param`shared_paramis False, then it can\nbe specified as a tuple (min_decay, max_decay).\n\nthreshold_decay(floatortuple) – the fraction of threshold decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple : min_decay, max_decay).\n\nrefractory_decay(floatortuple) – the fraction of refractory decay per time step. If`shared_param`shared_paramis\nFalse, then it can be specified as a tuple : min_decay, max_decay).\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.`scale=1`scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1 << 6.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations.\nDefaults to False.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\ngraded_spike(bool,optional) – flag to enable/disable graded spike output. Defaults to False.\n\nlog_init(bool,optional) – if True, initialized the natural frequency in log spaced range.\nDefault is True.\n\nA function to clamp the sin decay and cosine decay parameters to be\nwithin valid range. The user will generally not need to call this\nfunction.\n\nThe compartment cos decay parameter to be used for configuration.\n\nThe compartment refractory decay parameter to be used for\nconfiguring Loihi hardware.\n\nThe compartment sin decay parameter to be used for configuration.\n\nThe compartment threshold decay parameter to be used for configuring\nLoihi hardware.\n\nThe decay parameter of the neuron.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the dynamics (without spiking behavior) of the neuron\ninstance to a complex input tuple. The input shape must match with the\nneuron shape. For the first time, the neuron shape is determined from\nthe input automatically. It is essentially a resonator dynamics with\nIzhikevich firing with adaptive threshold and refractory response.\n\ninput(tupleoftorch tensors) – Complex input tuple of tensor, i.e. (real_input, imag_input).\n\ntorch tensor– real response of the neuron.torch tensor– imaginary response of the neuron.torch tensor– adaptive threshold of the neuron.torch tensor– refractory response of the neuorn.\n\ntorch tensor– real response of the neuron.\n\ntorch tensor– imaginary response of the neuron.\n\ntorch tensor– adaptive threshold of the neuron.\n\ntorch tensor– refractory response of the neuorn.\n\nComputes the full response of the neuron instance to a complex\ninput tuple. The input shape must match with the neuron shape. For the\nfirst time, the neuron shape is determined from the input\nautomatically.\n\ninput(tupleoftorch tensors) – Complex input tuple of tensor, i.e. (real_input, imag_input).\n\nspike response of the neuron.\n\ntorch tensor\n\nThe frequency of neuron oscillation.\n\nThe lambda parameter of the neuron.\n\nThe period of the neuron oscillation.\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nExtracts spike points from the real and imaginary states.\n\nreal(torch tensor) – real dynamics of the neuron.\n\nimag(torch tensor) – imaginary dynamics of the neuron.\n\nthreshold(torch tensor) – threshold dynamics of the neuron.\n\nrefractory(torch tensor) – refractory dynamics of the neuron.\n\nspike output\n\ntorch tensor\n\nGet voltage-threshold step parameter.\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1 << 6.\n\np_scale(int) – parameter scale value. Default value = 1 << 12\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nSigma Delta neuron.\n\nBases:[Neuron](https://lava-nc.org/lava-lib-dl/slayer/neuron/neuron.html#lava.lib.dl.slayer.neuron.base.Neuron)`Neuron`Neuron\n\nThis is the implementation of Sigma-Delta wrapper neuron.\n\nThe internal state representations are scaled down compared to\nthe actual hardware implementation. This allows for a natural range of\nsynaptic weight values as well as the gradient parameters.\n\nThe neuron parameters like threshold, decays are represented as real\nvalues. They internally get converted to fixed precision representation of\nthe hardware. It also provides properties to access the neuron\nparameters in fixed precision states. The parameters are internally clamped\nto the valid range.\n\nthreshold(float) – neuron threshold.\n\nactivation(fx-ptrorlambda) – The neuron activation class instance that needs to be wrapped\nby sigma-delta unit. For e.g.`torch.nn.functional.relu`torch.nn.functional.reluwould\ngive sigma-delta-relu unit.\n\ntau_grad(float,optional) – time constant of spike function derivative. Defaults to 1.\n\nscale_grad(float,optional) – scale of spike function derivative. Defaults to 1.\n\nscale(int,optional) – scale of the internal state.scale=1scale=1will result in values in the\nrange expected from the of Loihi hardware. Defaults to 1 << 6.\n\ncum_error(bool,optional) – flag to enable/disable residual state of delta unit. Defaults to False.\n\nnorm(fx-ptrorlambda,optional) – normalization function on the dendrite output. None means no\nnormalization. Defaults to None.\n\ndropout(fx-ptrorlambda,optional) – neuron dropout method. None means no normalization. Defaults to None.\n\nshared_param(bool,optional) – flag to enable/disable shared parameter neuron group. If it is\nFalse, individual parameters are assigned on a per-channel basis.\nDefaults to True.\n\npersistent_state(bool,optional) – flag to enable/disable persistent state between iterations.\nDefaults to False.\n\nrequires_grad(bool,optional) – flag to enable/disable learning on neuron parameter. Defaults to False.\n\nThe device memory (cpu/cuda) where the object lives.\n\nDictionary of device parameters.\n\nComputes the full response of the neuron instance to an input.\nThe input shape must match with the neuron shape. For the first time,\nthe neuron shape is determined from the input automatically.\n\ninput(torch tensor) – Input tensor.\n\ngraded spike response of the neuron.\n\ntorch tensor\n\nScale difference between slayer representation and hardware\nrepresentation of the variable states.\n\nSets the bias for sigma-delta unit\n\nbias(torch tensor) – bias corresponding to each neuron.\n\nNeuron threshold\n\nTranslates device parameters to neuron parameters.\n\ndevice_params(dictionary) – dictionary of device parameter specification.\n\nscale(int) – neuron scale value. Default value = 1  <<  6.\n\ndictionary of neuron parameters that can be used to initialize neuron\nclass.\n\ndictionary\n\nNeuron Dropout.\n\nBases:`Dropout3d`Dropout3d\n\nNeuron dropout method. It behaves similar totorch.nn.Dropouttorch.nn.Dropout.\nHowever, dropout over time dimension is preserved, i.e. if a neuron is\ndropped, it remains dropped for the entire time duration.\n\np(float) – dropout probability.\n\ninplace(bool) – inplace operation flag. Default is False.\n\nExamples\n\nNeuron normalization methods.\n\nBases:`Module`Module\n\nImplements mean only batch norm with optional user defined quantization\nusing pre-hook-function. The mean of batchnorm translates to negative bias\nof the neuron.\n\nnum_features(int) – number of features. It is automatically initialized on first run if the\nvalue is None. Default is None.\n\nmomentum(float) – momentum of mean calculation. Defaults to 0.1.\n\npre_hook_fx(function pointerorlambda) – pre-hook-function that is applied to the normalization output.\nUser can provide a quantization method as needed.\nDefaults to None.\n\nrunning mean estimate.\n\ntorch tensor\n\nenable mean estimte update.\n\nbool\n\nEquivalent bias shift.\n\nReset states.\n\nBases:`Module`Module\n\nImplements batch norm with variance scale in powers of 2. This allows\neventual normalizaton to be implemented with bit-shift in a hardware\nfriendly manner. Optional user defined quantization can be enabled using a\npre-hook-function. The mean of batchnorm translates to negative bias of the\nneuron.\n\nnum_features(int) – number of features. It is automatically initialized on first run if the\nvalue is None. Default is None.\n\nmomentum(float) – momentum of mean calculation. Defaults to 0.1.\n\nweight_exp_bits(int) – number of allowable bits for weight exponentation. Defaults to 3.\n\neps(float) – infitesimal value. Defaults to 1e-5.\n\npre_hook_fx(function pointerorlambda) – pre-hook-function that is applied to the normalization output.\nUser can provide a quantization method as needed.\nDefaults to None.\n\nrunning mean estimate.\n\ntorch tensor\n\nrunning variance estimate.\n\ntorch tensor\n\nenable mean estimte update.\n\nbool\n\nEquivalent bias shift.\n\nReset states.\n\nEquivalent weight exponent value.\n\nBases:`Dropout3d`Dropout3d\n\nNeuron dropout method. It behaves similar totorch.nn.Dropouttorch.nn.Dropout.\nHowever, dropout over time dimension is preserved, i.e. if a neuron is\ndropped, it remains dropped for the entire time duration.\n\np(float) – dropout probability.\n\ninplace(bool) – inplace operation flag. Default is False.\n\nExamples\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>synapse.pre_hook_fx=neuron.quantize_8bit\n``````\n\n``````\n>>>drop=Dropout(0.2,inplace=True)>>>output=drop(input)\n``````\n\n``````\n>>>drop=Dropout(0.2,inplace=True)>>>output=drop(input)\n``````\n\n``````\nNeuron\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/notebooks/neuron_dynamics/dynamics.html",
    "title": "Dynamics, Neurons, and Spikes — Lava  documentation",
    "content": "Dynamicsare the fundamental building blocks of neurons in`lava.dl.lib.slayer`lava.dl.lib.slayer. In this tutorial, we will go throught some fundamental neuron dynamics that are built in SLAYER and illustrate how they can be combined to build a variety of neuron models. These dynamics are customCUDAaccelerated,fixed precisioncompatible, andPyTorch autogardcompatible withlearnable decay(s)andpersistent state(s).\n\nDynamics scaling:the internal dynamics computation are done in fixed precision range. However, the parameters and state can be interpreted in scaled representation. This has two advantages.\n\nFirst, the dynamics are scaled such that the backpropagation gradients are usually in proper range for good gradient flow thus eliminating the need for unnatural scaling of surrogate gradients.\n\nSecond, the states and decays are in intuitive range rather than in abstract scaled fixed point state.\n\nFollowing notation for variable is used in this notebook.\n\nNotation\n\nVariable\n\nx[t]\n\ninput\n\ny[t]\n\nstate variable\n\n\\vartheta[t]\n\nthreshold\n\nr[t]\n\nrefractory state\n\ns[t]\n\nspike flag\n\n\\alpha\n\nleak parameter\n\n\\phi\n\nphase shift\n\nNOTE:this is a deep dive tutorial. It introduces * neuron dynamics in SLAYER. * how some available neurons are implemented. * how one can use these dynamics to build their custom neuron model. * real and complex spike mechanism in SLAYER.\n\nLeaky integrator is the basic first order neuron dynamics represented by the following discrete system:\n\nstate dynamics:y[t] = (1-\\alpha)\\,y[t-1] + x[t]\n\nspike dynamics:s[t] = y[t] \\geq \\vartheta\n\nreset dynamics:y[t] = 0\n\nLeaky integrator dynamics can be cascaded with other dynamics to form a second order neuron like CUBA neuron and other higer order neurons.\n\nFully backpropagable spike mechanism is avalilable as`slayer.spike.Spike`slayer.spike.Spike. It supports binary as well as graded spikes.\n\nA CUBA-LIF neuron is simply the leaky integrator dynamics applied to current followed by voltage. For easy usage, CUBA neuron is avaliable as``slayer.neuron.cuba``.\n\nAdaptive threshold dynamics provides first order threshold adaptation and refractory state adaptation dynamics.\n\nthreshold dynamics:\\vartheta[t] = (1-\\alpha_{\\vartheta})\\,(\\vartheta[t-1] - \\vartheta_0) + \\vartheta_0\n\nrefractory dynamics:r[t] = (1-\\alpha_r)\\,r[t-1]\n\nspike dynamics:s[t] = (x[t] - r[t]) \\geq \\vartheta[t]\n\npost spike dynamics:r[t] = r[t] + 2\\,\\vartheta[t]and\\vartheta[t] = \\vartheta[t] + \\vartheta_{\\text{step}}\n\nWhen coupled with a second order leaky integrator, it results in second order adaptive leaky integartor neuron. For easy usage, ALIF neuron is avaliable as``slayer.neuron.alif``.\n\nResonator is first order complex leaky dynamics. The leak is, in general, complex and gives rise to oscillatory dynamics. The resonator dynamics is described by\n\n\\frac{\\text dz}{\\text dt} = (-\\lambda + i\\omega)\\,z + \\zeta\n\nwhere\\zeta \\in \\mathbb{R}^nis the complex input to the system,\\lambda, \\omega \\in R^+.\n\nDiscretization\n\nThe impulse response of resonator is\n\nh(t) = e^{-\\lambda t}\\,e^{i\\omega t}\\,\\mathcal H(t)\n\nThe coresponding discrete system has the impulse respnse\n\nh[n] = e^{-\\lambda n \\Delta t}\\,e^{i\\omega n \\Delta t}\\,\\mathcal H[n] =  (e^{-\\lambda \\Delta t}\\,e^{i\\omega \\Delta t})^n\\,\\mathcal H[n] = e^{-\\lambda \\Delta t}\\,e^{i\\omega \\Delta t} h[n-1],\\ h[0]=1\n\nThe equivalent discrete system is therefore\n\nz[n + 1] = e^{-\\lambda \\Delta t}\\,e^{i\\omega \\Delta t}\\,z[n] + \\zeta[n]\n\nThe complex decay can be decoupled as\n\nmagnitude leak:\\alpha = 1 - e^{-\\lambda \\Delta t}\n\nphase shift:\\phi = e^{i\\omega \\Delta t}\n\ndecay matrix:(1-\\alpha)\\begin{bmatrix} \\cos\\phi &-\\sin\\phi \\\\ \\sin\\phi &\\cos\\phi\\end{bmatrix}\n\nA resonate and fire neuron spikes when it’s internal state crosses the real axis in the positive real half plane greater than the neuron’s theshold,\\vartheta. Formally, it can be stated as follows.\n\nf_s(z) = \\mathcal{H}(\\mathfrak{Re}(z) - \\vartheta)\\,\\delta(\\mathfrak{Im}(z))\n\nor equivalently\n\nf_s(z) = \\mathcal{H}(|z| - \\vartheta)\\,\\delta(\\arg(z))\n\nFor easy usage, RF neuron is avaliable as``slayer.neuron.rf``.\n\nFully backpropagable complex spike mechanism supporting phase spiking mechanism of RF neuron is avalilable as`slayer.spike.complex.Spike`slayer.spike.complex.Spike. It supports binary as well as graded spikes.\n\nspike dynamics:|z[t]| \\geq \\varthetaand\\arg(z[t]) = 0\n\nRF-Izhikevich[1] neuron dynamics is same as the basic RF neuron. However the firing and reset mechanism is different. The neuron fires when the imaginary state is above threshold and the real state is reset to zero post spike.\n\nspike dynamics:\\mathfrak{Im}(z[t]) \\geq \\vartheta\n\npost spike dynamics:\\mathfrak{Re}(z[t]) = 0\n\nFor easy usage, RF-Izhikevich neuron is avaliable as``slayer.neuron.rf_iz``.\n\n[1][Eugene M. Izhikevich Resonate and Fire Neurons.](https://www.izhikevich.org/publications/resfire.pdf)Eugene M. Izhikevich Resonate and Fire Neurons.\n\nTwo resonator dynamics can be cascaded to produce Gammatone like second order resonator dynamics. In theory, it could also be combined with leaky integrator for even more exotic neuron model.\n\nAdaptive resonator adds adaptive threshold and refractory dynamics on top of resonator. Two flavors of adaptive resonator dynamics are available in SLAYER:`slayer.neuron.dynamics.phase_th`slayer.neuron.dynamics.phase_thand`slayer.neuron.dynamics.adaptive_resonator`slayer.neuron.dynamics.adaptive_resonatorcorresponding tophase spikingandIzhikevich spikingmechanisms respectively. Both dynamics follow the same post spike dynamics.\n\npost spike dynamics:r[t] = r[t] + 2\\,\\vartheta[t]and\\vartheta[t] = \\vartheta[t] + \\vartheta_{\\text{step}}\n\nAdRF neuron spikes when its state crosses zero phase with real value higher than refractory dynamics and threshold dynamics combined.\n\nspike dynamics:|z[t]| \\geq (\\vartheta[t] + r[t])and\\arg(z[t]) = 0\n\nFor easy usage, AdRF neuron is avaliable as``slayer.neuron.adrf``.\n\nAdRF-Iz neuron fires when the imaginary state exceeds the threshold and refractory dynamics. There is no hard reset in this model.\n\nspike dynamics:\\mathfrak{Im}(z[t]) \\geq (\\vartheta[t] + r[t])\n\nFor easy usage, AdRF-Iz neuron is avaliable as``slayer.neuron.adrf_iz``.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\nimport lava.lib.dl.slayer as slayer\n``````\n\n``````\n[2]:\n``````\n\n``````\ndevice = torch.device('cpu')\n# device = torch.device('cuda')\n``````\n\n``````\n[3]:\n``````\n\n``````\ntime = np.arange(1000)\nt = torch.FloatTensor(time).to(device)\ninput = torch.zeros_like(t)\nfor tt, ww in [[10, 1], [97, 1.8], [100, 1.6], [270, -3], [500, 0.5]]:\n    input[tt] = ww\n``````\n\n``````\n[4]:\n``````\n\n``````\nscale = 1<<12 # scale factor for integer simulation\ndecay = torch.FloatTensor([0.1 * scale]).to(device)\ninitial_state = torch.FloatTensor([0]).to(device)\nthreshold = 1.5\n``````\n\n``````\n[5]:\n``````\n\n``````\ny = slayer.neuron.dynamics.leaky_integrator.dynamics(input, decay=decay, state=initial_state, w_scale=scale, threshold=threshold)\n``````\n\n``````\n[6]:\n``````\n\n``````\nsp = slayer.spike.Spike.apply(\n        y,\n        threshold,\n        1, # tau_rho: gradient relaxation constant\n        1, # scale_rho: gradient scale constant\n        False, # graded_spike: graded or binary spike\n        0, # voltage_last: voltage at t=-1\n        1, # scale: graded spike scale\n    )\n``````\n\n``````\n[7]:\n``````\n\n``````\nsecond_order_th = threshold * 5\ncurrent = slayer.neuron.dynamics.leaky_integrator.dynamics(input, decay=decay, state=initial_state, w_scale=scale)\nvoltage = slayer.neuron.dynamics.leaky_integrator.dynamics(current, decay=decay, state=initial_state, w_scale=scale, threshold=second_order_th)\n``````\n\n``````\n[8]:\n``````\n\n``````\nfig,ax = plt.subplots(3, 1, figsize=(15, 7))\nax[0].plot(time, input.cpu(), label='weighted spikes')\nax[0].legend(loc='upper right')\n\nax[1].plot(time, y.cpu(), label='Leaky integrator dynamics')\nax[1].plot(time, threshold * np.ones_like(time), alpha=0.5, label='threshold')\nax[1].plot(time[sp>0], y[sp>0], '*', label='spike')\nax[1].legend(loc='upper right')\n\nax[2].plot(time, current.cpu(), label='CUBA current')\nax[2].plot(time, voltage.cpu(), label='CUBA voltage')\nax[2].plot(time, second_order_th * np.ones_like(time), alpha=0.5, label='threshold')\nax[2].plot(time[voltage>second_order_th], voltage[voltage>second_order_th].cpu(), label='spike')\nax[2].legend(loc='upper right')\n\nax[-1].set_xlabel('time')\n``````\n\n``````\n[8]:\n``````\n\n``````\nText(0.5, 0, 'time')\n``````\n\n``````\n[9]:\n``````\n\n``````\ncurrent = slayer.neuron.dynamics.leaky_integrator.dynamics(input, decay=decay, state=initial_state, w_scale=scale)\nvoltage = slayer.neuron.dynamics.leaky_integrator.dynamics(current, decay=decay, state=initial_state, w_scale=scale)\nth, ref = slayer.neuron.dynamics.adaptive_threshold.dynamics(\n        voltage,                      # dynamics state\n        ref_state=initial_state,      # previous refractory state\n        ref_decay=0.5*decay,          # refractory decay\n        th_state=initial_state + second_order_th, # previous threshold state\n        th_decay=decay,               # threshold decay\n        th_scale=0.5*second_order_th, # threshold step\n        th0=second_order_th,          # threshold stable state\n        w_scale=scale                 # fixed precision scaling\n    )\n``````\n\n``````\n[10]:\n``````\n\n``````\nfig,ax = plt.subplots(2, 1, figsize=(15, 4.5))\nax[0].plot(time, input.cpu(), label='weighted spikes')\nax[0].legend(loc='upper right')\n\nax[1].plot(time, current.cpu(), label='ALIF current')\nax[1].plot(time, voltage.cpu(), label='ALIF voltage')\nax[1].plot(time, ref.cpu(), label='refractory dynamics')\nax[1].plot(time, th.cpu(), alpha=0.5, label='threshold')\nax[1].plot(time[(voltage-ref)>th], voltage[(voltage-ref)>th], '*', label='spike')\nax[1].legend(loc='upper right')\n\nax[-1].set_xlabel('time')\n``````\n\n``````\n[10]:\n``````\n\n``````\nText(0.5, 0, 'time')\n``````\n\n``````\n[11]:\n``````\n\n``````\nre_input = input\nim_input = 2*torch.randn_like(re_input) * (re_input > 0)\nalpha = torch.FloatTensor([0.03 * scale]).to(device)\nphi = 2 * np.pi /25\nsin_decay = (scale-alpha) * np.sin(phi)\ncos_decay = (scale-alpha) * np.cos(phi)\nre, im = slayer.neuron.dynamics.resonator.dynamics(\n        re_input, im_input,\n        sin_decay, cos_decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        w_scale=scale,\n    )\n``````\n\n``````\n[12]:\n``````\n\n``````\nsp = slayer.spike.complex.Spike.apply(\n        re, im,\n        threshold,\n        1, # tau_rho: gradient relaxation constant\n        1, # scale_rho: gradient scale constant\n        False, # graded_spike: graded or binary spike\n        0, # voltage_last: voltage at t=-1\n        1, # scale: graded spike scale\n    )\n``````\n\n``````\n[13]:\n``````\n\n``````\niz_re, iz_im = slayer.neuron.dynamics.resonator.dynamics(\n        re_input, im_input,\n        sin_decay, cos_decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        w_scale=scale,\n        threshold=threshold,\n    )\n``````\n\n``````\n[14]:\n``````\n\n``````\nsecond_order_th = threshold * 15\nre_0, im_0 = slayer.neuron.dynamics.resonator.dynamics(\n        re_input, im_input,\n        sin_decay, cos_decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        w_scale=scale,\n    )\nre_1, im_1 = slayer.neuron.dynamics.resonator.dynamics(\n        re_0, im_0,\n        sin_decay, cos_decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        w_scale=scale,\n    )\nsp_1 = slayer.spike.complex.Spike.apply(re_1, im_1, second_order_th, 1, 1, False, 0, 1)\n``````\n\n``````\n[15]:\n``````\n\n``````\nfig,ax = plt.subplots(4, 1, figsize=(15, 9))\nax[0].plot(time, re_input.cpu(), label='real weighted spikes')\nax[0].plot(time, im_input.cpu(), label='imag weighted spikes')\nax[0].legend(loc='upper right')\n\nax[1].plot(time, re.cpu(), label='RF real state')\nax[1].plot(time, im.cpu(), label='RF imag state')\nax[1].plot(time, threshold * np.ones_like(time), alpha=0.5, label='threshold')\nax[1].plot(time[sp>0], re[sp>0], '*', label='spike')\nax[1].legend(loc='upper right')\n\nax[2].plot(time, iz_re.cpu(), label='RF Izhikevich real state')\nax[2].plot(time, iz_im.cpu(), label='RF Izhikevich imag state')\nax[2].plot(time, threshold * np.ones_like(time), alpha=0.5, label='threshold')\nax[2].plot(time[iz_im > threshold], iz_im[iz_im > threshold], '*', label='spike')\nax[2].legend(loc='upper right')\n\nax[3].plot(time, re_1.cpu(), label='RF-Or2 real state')\nax[3].plot(time, im_1.cpu(), label='RF-Or2 imag state')\nax[3].plot(time, second_order_th * np.ones_like(time), alpha=0.5, label='threshold')\nax[3].plot(time[sp_1>0], re_1[sp_1>0], '*', label='spike')\nax[3].legend(loc='upper right')\n\nax[-1].set_xlabel('time')\n``````\n\n``````\n[15]:\n``````\n\n``````\nText(0.5, 0, 'time')\n``````\n\n``````\n[16]:\n``````\n\n``````\ndef plot_phase_region(ax, threshold, sin_decay):\n    xlims = ax.get_xlim()\n    ylims = ax.get_ylim()\n    xx = np.array([threshold, threshold+500])\n    yy = xx * sin_decay\n    ax.fill_between(xx, yy, color='green', alpha=0.1)\n    ax.set_xlim(xlims)\n    ax.set_ylim(ylims)\n\ndef plot_iz_region(ax, threshold):\n    ylims = ax.get_ylim()\n    ax.axhspan(threshold, threshold+50, color='green', alpha=0.1)\n    ax.set_ylim(ylims)\n``````\n\n``````\n[17]:\n``````\n\n``````\nfig, ax = plt.subplots(1, 3, figsize=(10, 3))\nax[0].plot(re.cpu(), im.cpu())\nax[0].plot(re[sp>0], im[sp>0], '*', label='spike')\nplot_phase_region(ax[0], threshold, sin_decay.item()/scale)\nax[0].set_xlabel('$\\mathfrak{Re}(z)$')\nax[0].set_ylabel('$\\mathfrak{Im}(z)$')\nax[0].legend(loc='lower left')\nax[0].set_title('RF')\nax[1].plot(iz_re.cpu(), iz_im.cpu())\nax[1].plot(iz_re[iz_im > threshold], iz_im[iz_im > threshold], '*', label='spike')\nplot_iz_region(ax[1], threshold)\nax[1].set_xlabel('$\\mathfrak{Re}(z)$')\nax[1].legend(loc='lower left')\nax[1].set_title('RF Izhikevich')\n\nax[2].plot(re_1.cpu(), im_1.cpu())\nax[2].plot(re_1[sp_1>0], im_1[sp_1>0], '*', label='spike')\nplot_phase_region(ax[2], second_order_th, sin_decay.item()/scale)\nax[2].set_xlabel('$\\mathfrak{Re}(z)$')\nax[2].legend(loc='lower left')\nax[2].set_title('RF-Or2')\n``````\n\n``````\n[17]:\n``````\n\n``````\nText(0.5, 1.0, 'RF-Or2')\n``````\n\n``````\n[18]:\n``````\n\n``````\nadrf_re, adrf_im = slayer.neuron.dynamics.resonator.dynamics(\n        re_input, im_input,\n        sin_decay, cos_decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        w_scale=scale,\n    )\nadrf_th, adrf_ref = slayer.neuron.dynamics.adaptive_phase_th.dynamics(\n        adrf_re, adrf_im,\n        im_state=initial_state, # only imaginary state is needed to determine first phase crossing\n        ref_state=initial_state, ref_decay=0.5*decay,       # refractory state and decay\n        th_state=initial_state + threshold, th_decay=decay, # threshold state and decay\n        th_scale=0.5 * threshold, # threshold step\n        th0=threshold,          # threshold stable state\n        w_scale=scale,\n    )\nadrf_sp = slayer.spike.complex.Spike.apply(\n        adrf_re, adrf_im, adrf_th + adrf_ref,\n        1, # tau_rho: gradient relaxation constant\n        1, # scale_rho: gradient scale constant\n        False, # graded_spike: graded or binary spike\n        0, # voltage_last: voltage at t=-1\n        1, # scale: graded spike scale\n    )\n``````\n\n``````\n[19]:\n``````\n\n``````\nadrf_iz_re, adrf_iz_im, adrf_iz_th, adrf_iz_ref = slayer.neuron.dynamics.adaptive_resonator.dynamics(\n        re_input, im_input,\n        sin_decay, cos_decay, ref_decay=0.5*decay, th_decay=decay,\n        real_state=initial_state,\n        imag_state=initial_state,\n        ref_state=initial_state,\n        th_state=initial_state + threshold,\n        th_scale=0.5 * threshold, # threshold step\n        th0=threshold,          # threshold stable state\n        w_scale=scale,\n    )\n\nadrf_iz_sp = adrf_iz_im > (adrf_iz_th + adrf_iz_ref)\n``````\n\n``````\n[20]:\n``````\n\n``````\nfig,ax = plt.subplots(3, 1, figsize=(15, 6.6))\nax[0].plot(time, re_input.cpu(), label='real weighted spikes')\nax[0].plot(time, im_input.cpu(), label='imag weighted spikes')\nax[0].legend(loc='upper right')\n\nax[1].plot(time, adrf_re.cpu(), label='AdRF real state')\nax[1].plot(time, adrf_im.cpu(), label='AdRF imag state')\nax[1].plot(time, adrf_ref.cpu(), label='refractory dynamics')\nax[1].plot(time, adrf_th.cpu(), alpha=0.5, label='threshold')\nax[1].plot(time[adrf_sp>0], adrf_re[adrf_sp>0], '*', label='spike')\nax[1].legend(loc='upper right')\n\nax[2].plot(time, adrf_iz_re.cpu(), label='AdRF-Iz real state')\nax[2].plot(time, adrf_iz_im.cpu(), label='AdRF-Iz imag state')\nax[2].plot(time, adrf_iz_ref.cpu(), label='refractory dynamics')\nax[2].plot(time, adrf_iz_th.cpu(), alpha=0.5, label='threshold')\nax[2].plot(time[adrf_iz_sp], adrf_iz_im[adrf_iz_sp], '*', label='spike')\nax[2].legend(loc='upper right')\n\nax[-1].set_xlabel('time')\n``````\n\n``````\n[20]:\n``````\n\n``````\nText(0.5, 0, 'time')\n``````\n\n``````\n[21]:\n``````\n\n``````\nimport matplotlib.patches as patches\n\nfig, ax = plt.subplots(1, 2, figsize=(6.5, 3), sharey=True)\nax[0].plot(adrf_re.cpu(), adrf_im.cpu())\nax[0].plot(adrf_re[sp>0], adrf_im[sp>0], '*', label='spike')\nplot_phase_region(ax[0], threshold, sin_decay.item()/scale)\nax[0].set_xlabel('$\\mathfrak{Re}(z)$')\nax[0].set_ylabel('$\\mathfrak{Im}(z)$')\nax[0].legend(loc='lower left')\nax[0].set_title('AdRF')\n\nax[1].plot(adrf_iz_re.cpu(), adrf_iz_im.cpu())\nax[1].plot(adrf_iz_re[adrf_iz_sp], adrf_iz_im[adrf_iz_sp], '*', label='spike')\nplot_iz_region(ax[1], threshold)\nax[1].set_xlabel('$\\mathfrak{Re}(z)$')\nax[1].legend(loc='lower left')\nax[1].set_title('AdRF Izhikevich')\n``````\n\n``````\n[21]:\n``````\n\n``````\nText(0.5, 1.0, 'AdRF Izhikevich')\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/notebooks/nmnist/train.html",
    "title": "N-MNIST Classification — Lava  documentation",
    "content": "N-MNISTis the neuromorphic version of MNIST digit recognition. The MNIST digits are converted into event based data using a DVS sensor moving in a repatable tri-saccadic motion each about 100 ms long.\n\nThe task is to classify each event sequence to it’s corresponding digit.\n\nNMNIST dataset is freely available[here](https://www.garrickorchard.com/datasets/n-mnist)here(© CC-4.0).\n\nOrchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.“Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades”, Frontiers in Neuroscience, vol.9, no.437, Oct. 2015\n\nThe dataset class follows standard torch dataset definition. They are defined in`nmnist.py`nmnist.py. We will just import the dataset and augmentation routine here.\n\nA slayer network definition follows standard PyTorch way using`torch.nn.Module`torch.nn.Module.\n\nThe network can be described with a combination of individual`synapse`synapse,`dendrite`dendrite,`neuron`neuronand`axon`axoncomponents. For rapid and easy development, slayer providesblock interface-`slayer.block`slayer.block- which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n\nIn the example below,`slayer.block.cuba`slayer.block.cubais illustrated.\n\nRunning the network in GPU is as simple as selecting`torch.device('cuda')`torch.device('cuda').\n\nA`slayer.io.Event`slayer.io.Eventcan be visualized by invoking it’s`Event.show()`Event.show()routine.`Event.anim()`Event.anim()instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it.\n\nSlayer provides prebuilt loss modules:`slayer.loss.{SpikeTime,SpikeRate,SpikeMax}`slayer.loss.{SpikeTime,SpikeRate,SpikeMax}. *`SpikeTime`SpikeTime: precise spike time based loss when target spike train is known. *`SpikeRate`SpikeRate: spike rate based loss when desired rate of the output neuron is known. *`SpikeMax`SpikeMax: negative log likelihood losses for classification without any rate tuning.\n\nSince the target spike train is not known for this problem, we use`SpikeRate`SpikeRateloss and target high spiking rate for true class and low spiking rate for false class.\n\ntarget rate:\\hat{\\boldsymbol r} = r_\\text{true}\\,{\\bf 1}[\\text{label}] + r_\\text{false}\\,(1-{\\bf 1}[\\text{label}])where{\\bf 1}[\\text{label}]is one-hot encoding of label. The loss is:\n\nL = \\frac{1}{2} \\left(\\frac{1}{T}\\int_T {\\boldsymbol s}(t)\\,\\text dt -  \\hat{\\boldsymbol r}\\right)^\\top {\\bf 1}\n\nSlayer provides`slayer.utils.LearningStats`slayer.utils.LearningStatsas a simple learning statistics logger for training, validation and testing.\n\nIn addtion,`slayer.utils.Assistant`slayer.utils.Assistantmodule wraps common training validation and testing routine which help simplify the training routine.\n\nTraining loop mainly consists of looping over epochs and calling`assistant.train`assistant.trainand`assistant.test`assistant.testutilities over training and testing dataset. The`assistant`assistantutility takes care of statndard backpropagation procedure internally.\n\n`stats`statscan be used in print statement to get formatted stats printout.\n\n`stats.testing.best_accuracy`stats.testing.best_accuracycan be used to find out if the current iteration has the best testing accuracy. Here, we use it to save the best model.\n\n`stats.update()`stats.update()updates the stats collected for the epoch.\n\n`stats.save`stats.savesaves the stats in files.\n\nPlotting the learning curves is as easy as calling`stats.plot()`stats.plot().\n\nLoad the best model during training and export it as hdf5 network. It is supported by`lava.lib.dl.netx`lava.lib.dl.netxto automatically load the network as a lava process.\n\nHere, we will use`slayer.io.tensor_to_event`slayer.io.tensor_to_eventmethod to convert the torch output spike tensor into`slayer.io.Event`slayer.io.Eventobject and visualize a few input and output event pairs.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport os, sys\nimport glob\nimport zipfile\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# import slayer from lava-dl\nimport lava.lib.dl.slayer as slayer\n\nimport IPython.display as display\nfrom matplotlib import animation\n``````\n\n``````\n[2]:\n``````\n\n``````\nfrom nmnist import augment, NMNISTDataset\n``````\n\n``````\n[3]:\n``````\n\n``````\nclass Network(torch.nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        neuron_params = {\n                'threshold'     : 1.25,\n                'current_decay' : 0.25,\n                'voltage_decay' : 0.03,\n                'tau_grad'      : 0.03,\n                'scale_grad'    : 3,\n                'requires_grad' : True,\n            }\n        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n\n        self.blocks = torch.nn.ModuleList([\n                slayer.block.cuba.Dense(neuron_params_drop, 34*34*2, 512, weight_norm=True, delay=True),\n                slayer.block.cuba.Dense(neuron_params_drop, 512, 512, weight_norm=True, delay=True),\n                slayer.block.cuba.Dense(neuron_params, 512, 10, weight_norm=True),\n            ])\n\n    def forward(self, spike):\n        for block in self.blocks:\n            spike = block(spike)\n        return spike\n\n    def grad_flow(self, path):\n        # helps monitor the gradient flow\n        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n\n        plt.figure()\n        plt.semilogy(grad)\n        plt.savefig(path + 'gradFlow.png')\n        plt.close()\n\n        return grad\n\n    def export_hdf5(self, filename):\n        # network export to hdf5 format\n        h = h5py.File(filename, 'w')\n        layer = h.create_group('layer')\n        for i, b in enumerate(self.blocks):\n            b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\n[4]:\n``````\n\n``````\ntrained_folder = 'Trained'\nos.makedirs(trained_folder, exist_ok=True)\n\n# device = torch.device('cpu')\ndevice = torch.device('cuda')\n\nnet = Network().to(device)\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\ntraining_set = NMNISTDataset(train=True, transform=augment)\ntesting_set  = NMNISTDataset(train=False)\n\ntrain_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)\n``````\n\n``````\nNMNIST dataset is freely available here: https://www.garrickorchard.com/datasets/n-mnist\n\n(c) Creative Commons:\n    Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.\n    \"Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades\",\n    Frontiers in Neuroscience, vol.9, no.437, Oct. 2015\n``````\n\n``````\n[5]:\n``````\n\n``````\nfor i in range(5):\n    spike_tensor, label = testing_set[np.random.randint(len(testing_set))]\n    spike_tensor = spike_tensor.reshape(2, 34, 34, -1)\n    event = slayer.io.tensor_to_event(spike_tensor.cpu().data.numpy())\n    anim = event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n    anim.save(f'gifs/input{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n``````\n\n``````\n[6]:\n``````\n\n``````\ngif_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 250px;\"/> </td>'\nheader = '<table><tr>'\nimages = ' '.join([gif_td(f'gifs/input{i}.gif') for i in range(5)])\nfooter = '</tr></table>'\ndisplay.HTML(header + images + footer)\n``````\n\n``````\n[6]:\n``````\n\n``````\n[7]:\n``````\n\n``````\nerror = slayer.loss.SpikeRate(true_rate=0.2, false_rate=0.03, reduction='sum').to(device)\n``````\n\n``````\n[8]:\n``````\n\n``````\nstats = slayer.utils.LearningStats()\nassistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)\n``````\n\n``````\n[9]:\n``````\n\n``````\nepochs = 100\n\nfor epoch in range(epochs):\n    for i, (input, label) in enumerate(train_loader): # training loop\n        output = assistant.train(input, label)\n    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n\n    for i, (input, label) in enumerate(test_loader): # training loop\n        output = assistant.test(input, label)\n    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n\n    if epoch%20 == 19: # cleanup display\n        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n        stats_str = str(stats).replace(\"| \", \"\\n\")\n        print(f'[Epoch {epoch:2d}/{epochs}]\\n{stats_str}')\n\n    if stats.testing.best_accuracy:\n        torch.save(net.state_dict(), trained_folder + '/network.pt')\n    stats.update()\n    stats.save(trained_folder + '/')\n    net.grad_flow(trained_folder + '/')\n``````\n\n``````\n[Epoch 19/100]\nTrain loss =     0.11669 (min =     0.11887)    accuracy = 0.96173 (max = 0.96188)\nTest  loss =     0.07722 (min =     0.07424)    accuracy = 0.97720 (max = 0.97770)\n\n[Epoch 39/100]\nTrain loss =     0.09383 (min =     0.09434)    accuracy = 0.97182 (max = 0.97180)\nTest  loss =     0.06004 (min =     0.06169)    accuracy = 0.98240 (max = 0.98250)\n\n[Epoch 59/100]\nTrain loss =     0.08660 (min =     0.08739)    accuracy = 0.97570 (max = 0.97692)\nTest  loss =     0.05640 (min =     0.05682)    accuracy = 0.98490 (max = 0.98420)\n\n[Epoch 79/100]\nTrain loss =     0.08141 (min =     0.08102)    accuracy = 0.97768 (max = 0.97808)\nTest  loss =     0.05284 (min =     0.05230)    accuracy = 0.98500 (max = 0.98660)\n\n[Epoch 99/100]\nTrain loss =     0.07434 (min =     0.07396)    accuracy = 0.97933 (max = 0.97993)\nTest  loss =     0.05019 (min =     0.04633)    accuracy = 0.98540 (max = 0.98720)\n``````\n\n``````\n[10]:\n``````\n\n``````\nstats.plot(figsize=(15, 5))\n``````\n\n``````\n[11]:\n``````\n\n``````\nnet.load_state_dict(torch.load(trained_folder + '/network.pt'))\nnet.export_hdf5(trained_folder + '/network.net')\n``````\n\n``````\n[12]:\n``````\n\n``````\noutput = net(input.to(device))\nfor i in range(5):\n    inp_event = slayer.io.tensor_to_event(input[i].cpu().data.numpy().reshape(2, 34, 34, -1))\n    out_event = slayer.io.tensor_to_event(output[i].cpu().data.numpy().reshape(1, 10, -1))\n    inp_anim = inp_event.anim(plt.figure(figsize=(5, 5)), frame_rate=240)\n    out_anim = out_event.anim(plt.figure(figsize=(10, 5)), frame_rate=240)\n    inp_anim.save(f'gifs/inp{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n    out_anim.save(f'gifs/out{i}.gif', animation.PillowWriter(fps=24), dpi=300)\n``````\n\n``````\n[13]:\n``````\n\n``````\nhtml = '<table>'\nhtml += '<tr><td align=\"center\"><b>Input</b></td><td><b>Output</b></td></tr>'\nfor i in range(5):\n    html += '<tr>'\n    html += gif_td(f'gifs/inp{i}.gif')\n    html += gif_td(f'gifs/out{i}.gif')\n    html += '</tr>'\nhtml += '</tr></table>'\ndisplay.HTML(html)\n``````\n\n``````\n[13]:\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/notebooks/oxford/train.html",
    "title": "Spike to Spike Regression: Oxford — Lava  documentation",
    "content": "This tutorial demonstratesspike to spike regressiontraining using``lava.lib.dl.slayer``.\n\nThe task is to learn to transform a random Poisson spike train to produce output spike pattern that resemblesThe Radcliffe Camerabuilding of Oxford University, England. The input and output both consist of 200 neurons each and the spikes span approximately 1900ms. The input and output pair are converted from[SuperSpike](https://github.com/fzenke/pub2018superspike)SuperSpike(© GPL-3).\n\nInput\n\nTarget\n\nCreate a simple PyTorch dataset class. The dataset class follows standard torch dataset definition.\n\nIt shows usage of``slayer.io``module. The module provides a way to\n\neasily represent events including graded spikes\n\nread/write events in different known binary and numpy formats\n\ntransform event to tensor for processing it using slayer network and convert a spike tensor back to event\n\ndisplay/animate the tensor for visualization\n\nA slayer network definition follows standard PyTorch way using`torch.nn.Module`torch.nn.Module.\n\nThe network can be described with a combination of individual`synapse`synapse,`dendrite`dendrite,`neuron`neuronand`axon`axoncomponents. For rapid and easy development, slayer providesblock interface-`slayer.block`slayer.block- which bundles all these individual components into a single unit. These blocks can be cascaded to build a network easily. The block interface provides additional utilities for normalization (weight and neuron), dropout, gradient monitoring and network export.\n\nIn the example below,`slayer.block.cuba`slayer.block.cubais illustrated.\n\nRunning the network in GPU is as simple as selecting`torch.device('cuda')`torch.device('cuda').\n\nA`slayer.io.Event`slayer.io.Eventcan be visualized by invoking it’s`Event.show()`Event.show()routine.`Event.anim()`Event.anim()instead returns the event visualization animation which can be embedded in notebook or exported as video/gif. Here, we will export gif animation and visualize it.\n\nSlayer provides prebuilt loss modules:`slayer.loss.{SpikeTime,SpikeRate,SpikeMax}`slayer.loss.{SpikeTime,SpikeRate,SpikeMax}. *`SpikeTime`SpikeTime: precise spike time based loss when target spike train is known. *`SpikeRate`SpikeRate: spike rate based loss when desired rate of the output neuron is known. *`SpikeMax`SpikeMax: negative log likelihood losses for classification without any rate tuning.\n\nSince the target spike train\\hat{\\boldsymbol s}(t)is known for this problem, we use`SpikeTime`SpikeTimeloss here. It usesvan Rossumlike spike train distance metric. The actual and target spike trains are filtered using a FIR filter and the norm of the timeseries is the loss metric.\n\nL = \\frac{1}{2T} \\int_T \\left(h_\\text{FIR} * ({\\boldsymbol s} - \\hat{\\boldsymbol s})\\right)(t)^\\top{\\bf 1}\\,\\text dt\n\n`time_constant`time_constant: time constant of the FIR filter.\n\n`filter_order`filter_order: the order of FIR filter. Exponential decay is first order filter.\n\nSlayer provides`slayer.utils.LearningStats`slayer.utils.LearningStatsas a simple learning statistics logger for training, validation and testing.\n\nIn addtion,`slayer.utils.Assistant`slayer.utils.Assistantmodule wraps common training validation and testing routine which help simplify the training routine.\n\nTraining loop mainly consists of looping over epochs and calling`assistant.train`assistant.trainutility to train.\n\n`stats`statscan be used in print statement to get formatted stats printout.\n\n`stats.training.best_loss`stats.training.best_losscan be used to find out if the current iteration has the best loss. Here, we use it to save the best model.\n\n`stats.update()`stats.update()updates the stats collected for the epoch.\n\n`stats.save`stats.savesaves the stats in files.\n\nPlotting the learning curves is as easy as calling`stats.plot()`stats.plot().\n\nLoad the best model during training and export it as hdf5 network. It is supported by`lava.lib.dl.netx`lava.lib.dl.netxto automatically load the network as a lava process.\n\nHere, we will use`slayer.io.tensor_to_event`slayer.io.tensor_to_eventmethod to convert the torch output spike tensor into`slayer.io.Event`slayer.io.Eventobject and visualize the input and output event.\n\nEvent data can be accessed as`slayer.io.Event.{x,y,c,t,p}`slayer.io.Event.{x,y,c,t,p}for x-address, y-address, channel-address, timestamp and graded-payload. This can be used for further processing and visualization of event data.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport os\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# import slayer from lava-dl\nimport lava.lib.dl.slayer as slayer\n\nimport IPython.display as display\nfrom matplotlib import animation\n``````\n\n``````\n[2]:\n``````\n\n``````\nclass OxfordDataset(Dataset):\n    def __init__(self):\n        super(OxfordDataset, self).__init__()\n        self.input  = slayer.io.read_1d_spikes('input.bs1' )\n        self.target = slayer.io.read_1d_spikes('output.bs1')\n        self.target.t = self.target.t.astype(int)\n\n    def __getitem__(self, _):\n        return (\n            self.input.fill_tensor(torch.zeros(1, 1, 200, 2000)).squeeze(),  # input\n            self.target.fill_tensor(torch.zeros(1, 1, 200, 2000)).squeeze(), # target\n        )\n\n    def __len__(self):\n        return 1 # just one sample for this problem\n``````\n\n``````\n[3]:\n``````\n\n``````\nclass Network(torch.nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        neuron_params = {\n                'threshold'     : 0.1,\n                'current_decay' : 1,\n                'voltage_decay' : 0.1,\n                'requires_grad' : True,\n            }\n\n        self.blocks = torch.nn.ModuleList([\n                slayer.block.cuba.Dense(neuron_params, 200, 256),\n                slayer.block.cuba.Dense(neuron_params, 256, 200),\n            ])\n\n    def forward(self, spike):\n        for block in self.blocks:\n            spike = block(spike)\n        return spike\n\n    def export_hdf5(self, filename):\n        # network export to hdf5 format\n        h = h5py.File(filename, 'w')\n        layer = h.create_group('layer')\n        for i, b in enumerate(self.blocks):\n            b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\n[4]:\n``````\n\n``````\ntrained_folder = 'Trained'\nos.makedirs(trained_folder, exist_ok=True)\n\n# device = torch.device('cpu')\ndevice = torch.device('cuda')\n\nnet = Network().to(device)\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)\n\ntraining_set = OxfordDataset()\ntrain_loader = DataLoader(dataset=training_set, batch_size=1)\n``````\n\n``````\n[5]:\n``````\n\n``````\ninput_anim  = training_set.input.anim(plt.figure(figsize=(10, 10)))\ntarget_anim = training_set.target.anim(plt.figure(figsize=(10, 10)))\n\n## This produces interactive animation\n# display.HTML(input_anim.to_jshtml())\n# display.HTML(target_anim.to_jshtml())\n\n## Saving and loading gif for better animation in github\ninput_anim.save('input.gif', animation.PillowWriter(fps=24), dpi=300)\ntarget_anim.save('target.gif', animation.PillowWriter(fps=24), dpi=300)\n``````\n\n``````\n[6]:\n``````\n\n``````\ngif_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 400px;\"/> </td>'\nhtml = '<table><tr>'\nhtml += '<td> Input </td><td> Target </td></tr><tr>'\nhtml += gif_td(f'input.gif')\nhtml += gif_td(f'target.gif')\nhtml += '</tr></table>'\ndisplay.HTML(html)\n``````\n\n``````\n[6]:\n``````\n\n``````\n[7]:\n``````\n\n``````\nerror = slayer.loss.SpikeTime(time_constant=2, filter_order=2).to(device)\n\n# the followng portion just illustrates the SpikeTime loss calculation.\n# IT IS NOT NEEDED IN PRACTICE\ninput, target = training_set[0]\noutput = net(input.unsqueeze(dim=0).to(device))[0]\n# just considering first neuron for illustration\noutput_trace = error.filter(output[0].to(device)).flatten().cpu().data.numpy()\ntarget_trace = error.filter(target[0].to(device)).flatten().cpu().data.numpy()\nfig, ax = plt.subplots(2, 1, figsize=(15, 3), sharex=True)\nax[0].plot(output_trace, label='output trace')\nax[0].plot(target_trace, label='target trace')\nax[1].plot(output_trace - target_trace, label='error trace')\nax[0].set_ylabel('trace')\nax[1].set_ylabel('trace')\nax[1].set_xlabel('time [ms]')\nfor a in ax: a.legend()\n``````\n\n``````\n[8]:\n``````\n\n``````\nstats = slayer.utils.LearningStats()\nassistant = slayer.utils.Assistant(net, error, optimizer, stats)\n``````\n\n``````\n[9]:\n``````\n\n``````\nepochs = 5000\n\nfor epoch in range(epochs):\n    for i, (input, target) in enumerate(train_loader): # training loop\n        output = assistant.train(input, target)\n        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n\n    if stats.training.best_loss:\n        torch.save(net.state_dict(), trained_folder + '/network.pt')\n    stats.update()\n    stats.save(trained_folder + '/')\n``````\n\n``````\n[Epoch 4999/5000] Train loss = 46478.09375 (min = 44918.71875))\n``````\n\n``````\n[10]:\n``````\n\n``````\nstats.plot(figsize=(15, 5))\n``````\n\n``````\n[11]:\n``````\n\n``````\nnet.load_state_dict(torch.load(trained_folder + '/network.pt'))\nnet.export_hdf5(trained_folder + '/network.net')\n``````\n\n``````\n[12]:\n``````\n\n``````\noutput = net(input.to(device))\nevent = slayer.io.tensor_to_event(output.cpu().data.numpy())\noutput_anim = event.anim(plt.figure(figsize=(10, 10)))\n# display.HTML(output_anim.to_jshtml())\noutput_anim.save('output.gif', animation.PillowWriter(fps=24), dpi=300)\n\nhtml = '<table><tr>'\nhtml += '<td>Output</td><td>Target</td></tr><tr>'\nhtml += gif_td(f'output.gif')\nhtml += gif_td(f'target.gif')\nhtml += '</tr></table>'\ndisplay.HTML(html)\n``````\n\n``````\n[12]:\n``````\n\n``````\n[13]:\n``````\n\n``````\nplt.figure(figsize=(7, 7))\nplt.plot(training_set.target.t, training_set.target.x, '.', markersize=12, label='target')\nplt.plot(event.t, event.x, '.', label='actual')\nplt.xlabel('time [ms]')\nplt.ylabel('neuron')\nplt.legend()\n``````\n\n``````\n[13]:\n``````\n\n``````\n<matplotlib.legend.Legend at 0x7f78ce257430>\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/notebooks/pilotnet/train.html",
    "title": "PilotNet Sigma-Delta Neural Network (SDNN) Training — Lava  documentation",
    "content": "PilotNet: Predict the car’s steering angle from the dashboard view.\n\nPilotNet dataset is available freely here. © MIT License.\n\nSigma-delta neural networksconsists of two main units:sigmadecoder in the dendrite anddeltaencoder in the axon. Delta encoder uses differential encoding on the output activation of a regular ANN activation, for e.g. ReLU. In addition it only sends activation to the next layer when the encoded message magnitude is larger than its threshold. The sigma unit accumulates the sparse event messages and accumulates it to restore the original value.\n\nA sigma-delta neuron is simply a regular activation wrapped around by a sigma unit at it’s input and a delta unit at its output.\n\nWhen the input to the network is a temporal sequence, the activations do not change much. Therefore, the message between the layers are reduced which in turn reduces the synaptic computation in the next layer. In addition, the graded event values can encode the change in magnitude in one time-step. Therefore there is no increase in latency at the cost of time-steps unlike the rate coded Spiking Neural Networks.\n\nCredit Eadweard Muybridge © Public Domain\n\nSparsity loss to penalize the network for high event-rate.\n\nSLAYER 2.0(``lava.dl.slayer``) provides a variety of learnableneuron models,synapsesaxonsanddendritesthat support quantized training. For easier use, it also provides``block``interface which packages the associated neurons, synapses, axons and dendrite features into a single module.\n\nSigma-delta blocksare available as`slayer.blocks.sigma_delta.{Dense,Conv,Pool,Input,Output,Flatten,...}`slayer.blocks.sigma_delta.{Dense,Conv,Pool,Input,Output,Flatten,...}which can be easily composed to create a variety of sequential network descriptions as shown below. The blocks can easily enablesynaptic weight normalization,neuron normalizationas well as provide usefulgradient monitoringutility andhdf5 network exportutility.\n\nThese blocks can be used to create a network using standard PyTorch procedure.\n\nTraining loop mainly consists of looping over epochs and calling`assistant.train`assistant.trainand`assistant.test`assistant.testutilities over training and testing dataset. The`assistant`assistantutility takes care of statndard backpropagation procedure internally.\n\n`stats`statscan be used in print statement to get formatted stats printout.\n\n`stats.testing.best_loss`stats.testing.best_losscan be used to find out if the current iteration has the best testing loss. Here, we use it to save the best model.\n\n`stats.update()`stats.update()updates the stats collected for the epoch.\n\n`stats.save`stats.savesaves the stats in files.\n\nPlotting the learning curves is as easy as calling`stats.plot()`stats.plot().\n\nLoad the best model during training and export it as hdf5 network. It is supported by`lava.lib.dl.netx`lava.lib.dl.netxto automatically load the network as a lava process.\n\nHere, we compare the synaptic operation and neuron activity of the trained SDNN and an ANN of iso-architecture.\n\nIf you want to learn more about Sigma-Delta neurons, take a look at the[Lava tutorial](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial10_sigma_delta_neurons.ipynb)Lava tutorial.\n\nFind out more about Lava and have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport sys, os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\n\nimport lava.lib.dl.slayer as slayer\n\nfrom pilotnet_dataset import PilotNetDataset\nimport utils\n``````\n\n``````\n[2]:\n``````\n\n``````\ntorch.manual_seed(4205)\n``````\n\n``````\n[2]:\n``````\n\n``````\n<torch._C.Generator at 0x7fa1881d0a90>\n``````\n\n``````\n[3]:\n``````\n\n``````\ndef event_rate_loss(x, max_rate=0.01):\n    mean_event_rate = torch.mean(torch.abs(x))\n    return F.mse_loss(F.relu(mean_event_rate - max_rate), torch.zeros_like(mean_event_rate))\n``````\n\n``````\n[4]:\n``````\n\n``````\nclass Network(torch.nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        sdnn_params = { # sigma-delta neuron parameters\n                'threshold'     : 0.1,    # delta unit threshold\n                'tau_grad'      : 0.5,    # delta unit surrogate gradient relaxation parameter\n                'scale_grad'    : 1,      # delta unit surrogate gradient scale parameter\n                'requires_grad' : True,   # trainable threshold\n                'shared_param'  : True,   # layer wise threshold\n                'activation'    : F.relu, # activation function\n            }\n        sdnn_cnn_params = { # conv layer has additional mean only batch norm\n                **sdnn_params,                                 # copy all sdnn_params\n                'norm' : slayer.neuron.norm.MeanOnlyBatchNorm, # mean only quantized batch normalizaton\n            }\n        sdnn_dense_params = { # dense layers have additional dropout units enabled\n                **sdnn_cnn_params,                        # copy all sdnn_cnn_params\n                'dropout' : slayer.neuron.Dropout(p=0.2), # neuron dropout\n            }\n\n        self.blocks = torch.nn.ModuleList([# sequential network blocks\n                # delta encoding of the input\n                slayer.block.sigma_delta.Input(sdnn_params),\n                # convolution layers\n                slayer.block.sigma_delta.Conv(sdnn_cnn_params,  3, 24, 3, padding=0, stride=2, weight_scale=2, weight_norm=True),\n                slayer.block.sigma_delta.Conv(sdnn_cnn_params, 24, 36, 3, padding=0, stride=2, weight_scale=2, weight_norm=True),\n                slayer.block.sigma_delta.Conv(sdnn_cnn_params, 36, 64, 3, padding=(1, 0), stride=(2, 1), weight_scale=2, weight_norm=True),\n                slayer.block.sigma_delta.Conv(sdnn_cnn_params, 64, 64, 3, padding=0, stride=1, weight_scale=2, weight_norm=True),\n                # flatten layer\n                slayer.block.sigma_delta.Flatten(),\n                # dense layers\n                slayer.block.sigma_delta.Dense(sdnn_dense_params, 64*40, 100, weight_scale=2, weight_norm=True),\n                slayer.block.sigma_delta.Dense(sdnn_dense_params,   100,  50, weight_scale=2, weight_norm=True),\n                slayer.block.sigma_delta.Dense(sdnn_dense_params,    50,  10, weight_scale=2, weight_norm=True),\n                # linear readout with sigma decoding of output\n                slayer.block.sigma_delta.Output(sdnn_dense_params,   10,   1, weight_scale=2, weight_norm=True)\n            ])\n\n\n    def forward(self, x):\n        count = []\n        event_cost = 0\n\n        for block in self.blocks:\n            # forward computation is as simple as calling the blocks in a loop\n            x = block(x)\n            if hasattr(block, 'neuron'):\n                event_cost += event_rate_loss(x)\n                count.append(torch.sum(torch.abs((x[..., 1:]) > 0).to(x.dtype)).item())\n\n        return x, event_cost, torch.FloatTensor(count).reshape((1, -1)).to(x.device)\n\n    def grad_flow(self, path):\n        # helps monitor the gradient flow\n        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n\n        plt.figure()\n        plt.semilogy(grad)\n        plt.savefig(path + 'gradFlow.png')\n        plt.close()\n\n        return grad\n\n    def export_hdf5(self, filename):\n        # network export to hdf5 format\n        h = h5py.File(filename, 'w')\n        layer = h.create_group('layer')\n        for i, b in enumerate(self.blocks):\n            b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\n[5]:\n``````\n\n``````\nbatch  = 8  # batch size\nlr     = 0.001 # leaerning rate\nlam    = 0.01  # lagrangian for event rate loss\nepochs = 200  # training epochs\nsteps  = [60, 120, 160] # learning rate reduction milestones\n\ntrained_folder = 'Trained'\nlogs_folder = 'Logs'\n\nos.makedirs(trained_folder, exist_ok=True)\nos.makedirs(logs_folder   , exist_ok=True)\n\ndevice = torch.device('cuda')\n``````\n\n``````\n[6]:\n``````\n\n``````\nnet = Network().to(device)\n\noptimizer = torch.optim.RAdam(net.parameters(), lr=lr, weight_decay=1e-5)\n\n# Datasets\ntraining_set = PilotNetDataset(\n    train=True,\n    transform=transforms.Compose([\n        transforms.Resize([33, 100]),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]),\n)\ntesting_set = PilotNetDataset(\n    train=False,\n    transform=transforms.Compose([\n        transforms.Resize([33, 100]),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]),\n)\n\ntrain_loader = DataLoader(dataset=training_set, batch_size=batch, shuffle=True, num_workers=8)\ntest_loader  = DataLoader(dataset=testing_set , batch_size=batch, shuffle=True, num_workers=8)\n\nstats = slayer.utils.LearningStats()\nassistant = slayer.utils.Assistant(\n        net=net,\n        error=lambda output, target: F.mse_loss(output.flatten(), target.flatten()),\n        optimizer=optimizer,\n        stats=stats,\n        count_log=True,\n        lam=lam\n    )\n``````\n\n``````\n[7]:\n``````\n\n``````\nfor epoch in range(epochs):\n    if epoch in steps:\n        for param_group in optimizer.param_groups:\n            print('\\nLearning rate reduction from', param_group['lr'])\n            param_group['lr'] /= 10/3\n\n    for i, (input, ground_truth) in enumerate(train_loader): # training loop\n        assistant.train(input, ground_truth)\n        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n\n    for i, (input, ground_truth) in enumerate(test_loader): # testing loop\n        assistant.test(input, ground_truth)\n        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n\n    if epoch%50==49: print()\n    if stats.testing.best_loss:\n        torch.save(net.state_dict(), trained_folder + '/network.pt')\n    stats.update()\n    stats.save(trained_folder + '/')\n\n    # gradient flow monitoring\n    net.grad_flow(trained_folder + '/')\n\n    # checkpoint saves\n    if epoch%10 == 0:\n        torch.save({'net': net.state_dict(), 'optimizer': optimizer.state_dict()}, logs_folder + f'/checkpoint{epoch}.pt')\n``````\n\n``````\n[Epoch  49/200] Train loss =     0.08724 (min =     0.06125) | Test  loss =     0.07638 (min =     0.07048)\n[Epoch  59/200] Train loss =     0.05042 (min =     0.06125) | Test  loss =     0.06043 (min =     0.05631)\nLearning rate reduction from 0.001\n[Epoch  99/200] Train loss =     0.03670 (min =     0.03420) | Test  loss =     0.04726 (min =     0.04006)\n[Epoch 119/200] Train loss =     0.03588 (min =     0.03177) | Test  loss =     0.04133 (min =     0.03812)\nLearning rate reduction from 0.0003\n[Epoch 149/200] Train loss =     0.03995 (min =     0.02701) | Test  loss =     0.04514 (min =     0.03812)\n[Epoch 159/200] Train loss =     0.03351 (min =     0.02701) | Test  loss =     0.04028 (min =     0.03812)\nLearning rate reduction from 8.999999999999999e-05\n[Epoch 199/200] Train loss =     0.03122 (min =     0.02523) | Test  loss =     0.04434 (min =     0.03812)\n``````\n\n``````\n[8]:\n``````\n\n``````\nstats.plot(figsize=(15, 5))\n``````\n\n``````\n[9]:\n``````\n\n``````\nnet.load_state_dict(torch.load(trained_folder + '/network.pt'))\nnet.export_hdf5(trained_folder + '/network.net')\n``````\n\n``````\n[10]:\n``````\n\n``````\ncounts = []\nfor i, (input, ground_truth) in enumerate(test_loader):\n    _, count = assistant.test(input, ground_truth)\n    count = (count.flatten()/(input.shape[-1]-1)/input.shape[0]).tolist() # count skips first events\n    counts.append(count)\n    print('\\rEvent count : ' + ', '.join([f'{c:.4f}' for c in count]), f'| {stats.testing}', end='')\n\ncounts = np.mean(counts, axis=0)\n``````\n\n``````\nEvent count : 2170.5430, 224.2953, 372.4000, 507.9524, 28.2095, 4.7143, 1.0000, 0.1333, 0.4286 | loss =     0.04975 (min =     0.03812)\n``````\n\n``````\n[11]:\n``````\n\n``````\nutils.compare_ops(net, counts, mse=stats.testing.min_loss)\n``````\n\n``````\n|-----------------------------------------------------------------------------|\n|                         |          SDNN           |           ANN           |\n|-----------------------------------------------------------------------------|\n|         |     Shape     |  Events  |    Synops    | Activations|    MACs    |\n|-----------------------------------------------------------------------------|\n| layer-0 | (100, 33,  3) |  2475.93 |              |       9900 |            |\n| layer-1 | ( 49, 16, 24) |   239.46 |    133700.12 |      18816 |     534600 |\n| layer-2 | ( 24,  7, 36) |   422.39 |     19395.86 |       6048 |    1524096 |\n| layer-3 | ( 22,  4, 64) |   558.71 |    121649.28 |       5632 |    1741824 |\n| layer-4 | ( 20,  2, 64) |    29.90 |    321818.47 |       2560 |    3244032 |\n| layer-5 | (  1,  1,100) |     4.80 |      2989.97 |        100 |     256000 |\n| layer-6 | (  1,  1, 50) |     1.35 |       240.00 |         50 |       5000 |\n| layer-7 | (  1,  1, 10) |     0.16 |        13.46 |         10 |        500 |\n| layer-8 | (  1,  1,  1) |     0.44 |         0.16 |          1 |         10 |\n|-----------------------------------------------------------------------------|\n|  Total  |               |  3733.14 |    599807.33 |      43117 |    7306062 |\n|-----------------------------------------------------------------------------|\n\n\nMSE            : 0.038123 sq. radians\nTotal neurons  : 43117\nEvents sparsity: 11.55x\nSynops sparsity: 12.18x\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/slayer.html",
    "title": "Lava-DL SLAYER — Lava  documentation",
    "content": "`lava.lib.dl.slayer`lava.lib.dl.slayeris an enhanced version of[SLAYER](https://github.com/bamsumit/slayerPytorch)SLAYER. It now supports\na wide variety of learnable event-basedneuron models,synapse,axon, anddendriteproperties. Other enhancements include various\nutilities useful during training for event IO, visualization,and\nfiltering as well as logging of training statistics.\n\nHighlight Features\n\nResonator, Adaptive leaky neuron dynamics in addtion to conventional\nLeaky neuron dynamics\n\nSigma-Delta wrapper around arbitrary neuron dynamics\n\nGraded spikes\n\nLearnable neuron parameters at a granularity of individual neuron\n\nPersistent states between iterations for robotics application\n\nArbitrary recurrent architectures including k-winner-take-all (KWTA)\n\nComplex valued synapses\n\nSparse connectivity with connection masking\n\nRuntime shape identification (eliminates the need fora prioriarchitecture shape calculation)\n\nJust-In-Time compilation of CUDA acccelerated code.\n\nBlock interface for easy description of network.\n\nEasy network export to hdf5 interface format.\n\nEnd to End\n\n[Oxford spike train regression](https://lava-nc.org/lava-lib-dl/slayer/notebooks/oxford/train.html)Oxford spike train regression\n\n[NMNIST digit classification](https://lava-nc.org/lava-lib-dl/slayer/notebooks/nmnist/train.html)NMNIST digit classification\n\n[PilotNet steering angle prediction](https://lava-nc.org/lava-lib-dl/slayer/notebooks/pilotnet/train.html)PilotNet steering angle prediction\n\nDeep Dive\n\n[Dynamics and Neurons](https://lava-nc.org/lava-lib-dl/slayer/notebooks/neuron_dynamics/dynamics.html)Dynamics and Neurons\n\nThe overall feature organization is described below.\n\nSLAYER supports binary as well as graded spikes, which are amenable to\nbackpropagation. This opens the door for a new class of neuron behavior.\n\nNeuron models in SLAYER are built around custom CUDA accelerated\nfundamental linear dynamics. Each neuron model has individually\nlearnable parameters from its neural dynamicsas well as persistent state\nbehavior between iterations. Following neuron dynamics are supported.\n\nLeaky Integrator\n\nResonator\n\nAdaptive Integrator with Refractory Dynamics\n\nAdaptive Resonator with Refractory Dynamics\n\nThese fundamental dynamics can be combined to build a variety of neuron\nmodels. Following neuron models are currently supported:\n\nIn addition, SLAYER also supportsneuron dropoutand quantization\nready batch-normalization methods.\n\nSLAYER supports dense, conv, and pool synaptic connections. Masking is\npossible in both real as well as complex connections:`slayer.synapse.{complex}.{Dense,Conv,Pool}`slayer.synapse.{complex}.{Dense,Conv,Pool}.\n\nLearnable axonal delay (`slayer.axon.Delay`slayer.axon.Delay)\n\nLearnable delta encoder (`slayer.axon.Delta`slayer.axon.Delta)\n\nSigma decoder (`slayer.dendrite.Sigma`slayer.dendrite.Sigma)\n\nSLAYER provides easy encapsulation of neuron, synapse, axon, and\ndendrite classes for a variety of standard neuron-connection\ncombinations:`slayer.block.{cuba,alif,rf,rf_iz,sigma_delta}.{input,output,dense,conv,pool,kwta,recurrent}`slayer.block.{cuba,alif,rf,rf_iz,sigma_delta}.{input,output,dense,conv,pool,kwta,recurrent}These blocks can be easily used to define a network and export it in\npytorch as well as our platform independent hdf5 format.\n\nTensors are always assumed to be in the order`NCHWT`NCHWTor`NCT`NCTwhere`N`N:Batch,`C`C:Channel,`H`H: Height(y),`W`W: Width(x)\nand`T`T: Time.\n\n`NCHW`NCHWis the default PyTorch ordering.\n\nSynapse values are maintained in scaled down range.\n\nNeurons hold the shape of the layer. It shall be automatically\nidentified on runtime.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n# like any standard pyTorch networkclassNetwork(torch.nn.Module):def__init__(self):...self.blocks=torch.nn.ModuleList([# sequential network blocksslayer.block.sigma_delta.Input(sdnn_params),slayer.block.sigma_delta.Conv(sdnn_params,3,24,3),slayer.block.sigma_delta.Conv(sdnn_params,24,36,3),slayer.block.rf_iz.Conv(rf_params,36,64,3,delay=True),slayer.block.rf_iz.Conv(sdnn_cnn_params,64,64,3,delay=True),slayer.block.rf_iz.Flatten(),slayer.block.alif.Dense(alif_params,64*40,100,delay=True),slayer.block.cuba.Recurrent(cuba_params,100,50),slayer.block.cuba.KWTA(cuba_params,50,50,num_winners=5)])defforward(self,x):forblockinself.blocks:# forward computation is as simple as calling the blocks in a loopx=block(x)returnxdefexport_hdf5(self,filename):# network export to hdf5 formath=h5py.File(filename,'w')layer=h.create_group('layer')fori,binenumerate(self.blocks):b.export_hdf5(layer.create_group(f'{i}'))\n``````\n\n``````\nslayer.spike\n``````\n\n``````\nslayer.neuron\n``````\n\n``````\nslayer.neuron.cuba\n``````\n\n``````\nslayer.neuron.alif\n``````\n\n``````\nslayer.neuron.{rf,rf_iz}\n``````\n\n``````\nslayer.neuron.{adrf,adrf_iz}\n``````\n\n``````\nslayer.neuron.sigma_delta\n``````\n\n``````\nslayer.syanpse\n``````\n\n``````\nslayer.axon\n``````\n\n``````\nslayer.dendrite\n``````\n\n``````\nslayer.blocks\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/spike/modules.html",
    "title": "Spike — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/spike/spike.html",
    "title": "Spike Module — Lava  documentation",
    "content": "Spike mechanism implementation.\n\nBases:`Function`Function\n\nSpiking mechanism with autograd link.\n\nf_s(x) &= \\mathcal{H}(x - \\vartheta)\n\nvoltage(torch tensor) – neuron voltage.\n\nthreshold(floatortorch tensor) – neuron threshold\n\ntau_rho(float) – gradient relaxation\n\nscale_rho(float) – gradient scale\n\ngraded_spike(bool) – flag for graded spike\n\nvoltage_last(torch tensor) – voltage at t=-1\n\nscale(int) – variable scale value.\n\nspike tensor\n\ntorch tensor\n\nExamples\n\nComplex spike (Phase threshold) implementation.\n\nBases:`Function`Function\n\nComplex spike function with autograd link.\n\nf_s(z) &= \\mathcal{H}(|z| - \\vartheta)\\,\\delta(\\arg(z))\n\nreal(torch tensor) – real component of neuron response.\n\nimag(torch tensor) – imaginary component of neuron response.\n\nthreshold(torch tensororfloat) – neuron threshold.\n\ntau_rho(float) – gradient relaxation\n\nscale_rho(float) – gradient scale\n\ngraded_spike(bool) – flag for graded spike\n\nimag_last(torch tensor) – imaginary response at t=-1\n\nscale(int) – variable scale value.\n\nspike tensor\n\ntorch tensor\n\nExamples\n\nBases:`Function`Function\n\nSpiking mechanism with autograd link.\n\nf_s(x) &= \\mathcal{H}(x - \\vartheta)\n\nvoltage(torch tensor) – neuron voltage.\n\nthreshold(floatortorch tensor) – neuron threshold\n\ntau_rho(float) – gradient relaxation\n\nscale_rho(float) – gradient scale\n\ngraded_spike(bool) – flag for graded spike\n\nvoltage_last(torch tensor) – voltage at t=-1\n\nscale(int) – variable scale value.\n\nspike tensor\n\ntorch tensor\n\nExamples\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>spike=Spike.apply(v,th,tau_rho,scale_rho,False,0,1)\n``````\n\n``````\n>>>spike=Spike.apply(re,im,th,tau_rho,scale_rho,False,0,1)\n``````\n\n``````\n>>>spike=Spike.apply(v,th,tau_rho,scale_rho,False,0,1)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/synapse/modules.html",
    "title": "Synapse — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html",
    "title": "Synapse Module — Lava  documentation",
    "content": "Synapse module\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nConvolution synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`ConvTranspose3d`ConvTranspose3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nTransposed convolution synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the transposed convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the transposed convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the transposed convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the transposed convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nDense synapse layer.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int) – weight initialization scaling factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCT or NCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`Module`Module\n\nAbstract synapse layer class.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nDisables weight normalization on synapse.\n\nEnables weight normalization on synapse.\n\nNorm of weight gradients. Useful for monitoring gradient flow.\n\nReturns the pre-hook function for synapse operation. Typically\nintended to define the quantization method.\n\nShape of the synapse\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nPooling synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of pooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the pooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the pooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`ConvTranspose3d`ConvTranspose3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nUnpooling synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of unpooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the unpooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the unpooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nComplex synapse\n\nBases:`Module`Module\n\nAbstract complex layer class.\n\nDisables weight normalization on synapse.\n\nEnables weight normalization on synapse.\n\nForward complex synaptic operation.\n\nNorm of weight gradients. Useful for monitoring gradient flow.\n\nReturns the pre-hook function for synapse operation. Typically\nintended to define the quantization method.\n\nShape of the synapse\n\nBases:[ComplexLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.complex.ComplexLayer)`ComplexLayer`ComplexLayer\n\nConvolution complex-synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nreal synapse.\n\n[slayer.synapse.Conv](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Conv)slayer.synapse.Conv\n\nimaginary synapse.\n\n[slayer.synapse.Conv](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Conv)slayer.synapse.Conv\n\nTrue. Indicates synapse is complex.\n\nbool\n\nBases:[ComplexLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.complex.ComplexLayer)`ComplexLayer`ComplexLayer\n\nTransposed convolution synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the transposed convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the transposed convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the transposed convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the transposed convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nreal synapse.\n\n[slayer.synapse.ConvTranspose](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.ConvTranspose)slayer.synapse.ConvTranspose\n\nimaginary synapse.\n\n[slayer.synapse.ConvTranspose](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.ConvTranspose)slayer.synapse.ConvTranspose\n\nTrue. Indicates synapse is complex.\n\nbool\n\nBases:[ComplexLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.complex.ComplexLayer)`ComplexLayer`ComplexLayer\n\nDense compelx-synapse layer.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int) – weight initialization scaling factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nreal synapse.\n\n[slayer.synapse.Dense](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Dense)slayer.synapse.Dense\n\nimaginary synapse.\n\n[slayer.synapse.Dense](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Dense)slayer.synapse.Dense\n\nTrue. Indicates synapse is complex.\n\nbool\n\nBases:[ComplexLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.complex.ComplexLayer)`ComplexLayer`ComplexLayer\n\nPooling complex-synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of pooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the pooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the pooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nreal synapse.\n\n[slayer.synapse.Pool](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Pool)slayer.synapse.Pool\n\nimaginary synapse.\n\n[slayer.synapse.Pool](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Pool)slayer.synapse.Pool\n\nTrue. Indicates synapse is complex.\n\nbool\n\nBases:[ComplexLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.complex.ComplexLayer)`ComplexLayer`ComplexLayer\n\nUnpooling complex-synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of unpooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the unpooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the unpooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nreal synapse.\n\n[slayer.synapse.Unpool](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Unpool)slayer.synapse.Unpool\n\nimaginary synapse.\n\n[slayer.synapse.Unpool](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.Unpool)slayer.synapse.Unpool\n\nTrue. Indicates synapse is complex.\n\nbool\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nConvolution synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`ConvTranspose3d`ConvTranspose3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nTransposed convolution synapse layer.\n\nin_features(int) – number of input features.\n\nout_features(int) – number of output features.\n\nkernel_size(intortupleoftwo ints) – size of the transposed convolution kernel.\n\nstride(intortupleoftwo ints) – stride of the transposed convolution. Defaults to 1.\n\npadding(intortupleoftwo ints) – padding of the transposed convolution. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the transposed convolution. Defaults to 1.\n\ngroups(int) – number of blocked connections from input channel to output channel.\nDefaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nDense synapse layer.\n\nin_neurons(int) – number of input neurons.\n\nout_neurons(int) – number of output neurons.\n\nweight_scale(int) – weight initialization scaling factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCT or NCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`Conv3d`Conv3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nPooling synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of pooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the pooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the pooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\nBases:`ConvTranspose3d`ConvTranspose3d,[GenericLayer](https://lava-nc.org/lava-lib-dl/slayer/synapse/synapse.html#lava.lib.dl.slayer.synapse.layer.GenericLayer)`GenericLayer`GenericLayer\n\nUnpooling synape layer.\n\nkernel_size(int) – [description]\n\nstride(intortupleoftwo ints) – stride of unpooling. Defaults tokernel_sizekernel_size.\n\npadding(intortupleoftwo ints) – padding of the unpooling. Defaults to 0.\n\ndilation(intortupleoftwo ints) – dilation of the unpooling. Defaults to 1.\n\nweight_scale(int) – weight initialization scale factor. Defaults to 1.\n\nweight_norm(bool) – flag to enable/disable weight normalization. Defaults to False.\n\npre_hook_fx(optional) – a function reference or a lambda function. If the function is provided,\nit will be applied to it’s weight before the forward operation of the\nsynapse. Typically the function is a quantization mechanism of the\nsynapse. Defaults to None.\n\nNote\n\nFor kernel_size, stride, padding and dilation, the tuple of two ints are\nrepresented in (height, width) order. The integer value is broadcast to\nheight and width.\n\nflag indicating weather weight norm in enabled or not.\n\nbool\n\nFalse. Indicates synapse is not complex.\n\nbool\n\nApplies the synapse to the input.\n\ninput(torch tensor) – Input tensor. Typically spikes. Input is expected to be of shape\nNCHWT.\n\ndendrite accumulation / weighted spikes.\n\ntorch tensor\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nGenericLayer\n``````\n\n``````\nComplexLayer\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/utils/modules.html",
    "title": "Utilities — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html",
    "title": "Utility Modules — Lava  documentation",
    "content": "Assistant utility for automatically load network from network\ndescription.\n\nBases:`object`object\n\nAssistant that bundles training, validation and testing workflow.\n\nnet(torch.nn.Module) – network to train.\n\nerror(objectorlambda) – an error object or a lambda function that evaluates error.\nIt is expected to take`(output,target)`(output,target)|`(output,label)`(output,label)as it’s argument and return a scalar value.\n\noptimizer(torch optimizer) – the learning optimizer.\n\nstats(slayer.utils.stats) – learning stats logger. If None, stats will not be logged.\nDefaults to None.\n\nclassifier(slayer.classifierorlambda) – classifier object or lambda function that takes output and\nreturns the network prediction. None means regression mode.\nClassification steps are bypassed.\nDefaults to None.\n\ncount_log(bool) – flag to enable count log. Defaults to False.\n\nlam(float) – lagrangian to merge network layer based loss.\nNone means no such additional loss.\nIf not None, net is expected to return the accumulated loss as second\nargument. It is intended to be used with layer wise sparsity loss.\nDefaults to None.\n\nthe main device memory where network is placed. It is not at start and\ngets initialized on the first call.\n\ntorch.device or None\n\nReduces the learning rate of the optimizer by`factor`factor.\n\nfactor(float) – learning rate reduction factor. Defaults to 10/3.\n\nTesting assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nTraining assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nValidation assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nTime dimension filtering utilities.\n\nBases:`Module`Module\n\nFinite impulse response filter. The filters are not learnable. For\nlearnable filter, useFIRBankFIRBankwith one filter.\n\nfir_response(array) – Desired FIR response. If it is None, an exponentially decaying filter\nis initialized. Defaults to None.\n\ntime_constant(float) – time constant of exponentially decaying filter. Defaults to 1.\n\nlength(int) – length of the FIR filter to initialize. Defaults to 20.\n\nsampling_time(float) – sampling time of FIR filter. Defaults to 1.\n\nimpulse response of FIR filter.\n\ntorch tensor\n\nsampling time of FIR filter.\n\nfloat\n\nBases:`Conv3d`Conv3d\n\nFinite impulse response filter bank. The filters are learnable.\n\nnum_filter(int) – number of FIR filters in the bank.\n\nfilter_length(float) – time length of the filter.\n\nsampling_time(float) – sampling time of the filter. Defaults to 1.\n\nscale(float) – initialization scaling factor for filter. Defaults to 1.\n\nsampling time of the filter. Defaults to 1.\n\nfloat\n\nTime length of the filter.\n\nImpulse response of filter bank\n\nNumber of filters in the bank.\n\nConvolution in time.\n\ninput(torch tensor) – input signal. Last dimension is assumed to be time.\n\nfilter(torch tensor) – convolution filter. Assumed to be 1 dimensional. It will be flattened\notherwise.\n\nsampling_time(float) – sampling time. Defaults to 1.\n\nconvolution output. Output shape is same as input.\n\ntorch tensor\n\nExamples\n\nCorrelation in time.\n\ninput(torch tensor) – input signal. Last dimension is assumed to be time.\n\nfilter(torch tensor) – correlation filter. Assumed to be 1 dimensional. It will be flattened\notherwise.\n\nsampling_time(float) – sampling time. Defaults to 1.\n\ncorrelation output. Output shape is same as input.\n\ntorch tensor\n\nExamples\n\nInteger bit shift utilities.\n\nBases:`Function`Function\n\nAutograd compliant version of quantization towards zero.\n\nRight shift with quantization towards zero implementation.\n\nx(torch.int32ortorch.int64) – input tensor.\n\nbits(int) – number of bits to shift.\n\nright shift to zero result.\n\ntorch.int32 or torch.int64\n\nQuantization utility.\n\nBases:`IntEnum`IntEnum\n\nQuantization mode constants. Options are {`ROUND:0`ROUND:0,`FLOOR:1`FLOOR:1}.\n\nImplements quantization of parameters. Round or floor behavior can be\nselected using mode argument.\n\ninput(torch tensor) – input tensor\n\nstep(float) – quantization step. Default is 1.\n\nmode([MODE](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.quantize.MODE)MODE) – quantization mode. Default is MODE.ROUND.\n\nquantized tensor\n\ntorch tensor\n\nExamples\n\nQuantize prehook function to use in slayer synapse pre-hook for\nquantization.\n\nx(torch.tensor) – Input tensor.\n\nscale(int,optional) – Quantization decimal scale corresponding to 1.0 value,\nby default (1 << 6).\n\nnum_bits(int,optional) – Number of bits to use in quantization, by default 8.\n\ndescale(bool,optional) – Flag to descale the fixed point number to integer or keep it as\nfixed point number. By default False.\n\nQuantized tensor.\n\ntorch.tensor\n\nModule for managing, visualizing, and displaying learning statistics.\n\nBases:`object`object\n\nLearning stat manager\n\naccumulated loss sum.\n\nfloat\n\naccumulated correct samples.\n\nint\n\nnumber of samples accumulated.\n\nint\n\nbest loss recorded.\n\nfloat\n\nbest accuracy recorded.\n\nfloat\n\nlog of loss stats.\n\nlist of float\n\nlog of accuracy stats.\n\nlist of float\n\ndoes current epoch have best loss? It is updated only\nafterstats.update()stats.update().\n\nbool\n\ndoes current epoch have best accuracy? It is updated only\nafterstats.update()stats.update().\n\nbool\n\nString method.\n\nCurrent accuracy.\n\nCurrent loss.\n\nReset stat.\n\nUpdate stat.\n\nBases:`object`object\n\nManages training, validation and testing stats.\n\nLearningStatLearningStatobject to manage training statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.stats.LearningStat)LearningStat\n\nLearningStatLearningStatobject to manage testing statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.stats.LearningStat)LearningStat\n\nLearningStatLearningStatobject to manage validation statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.stats.LearningStat)LearningStat\n\nString summary of stats.\n\nForces stats printout on new line.\n\nPlots the training curves.\n\nfigures(tupleofints) – figures to plot loss and accuracy. Defaults to (1, 2).\n\nfigsize(tupleofintsorNone) – custom width and height of the figure. None means default size.\nDefaults to None.\n\npath(str) – If not None, saves the plot to the path specified.\nDefaults to None.\n\nDynamic print method for stats.\n\nepoch(int) – current epoch\n\niter(intorNone) – iteration number in epoch. Defaults to None.\n\ntime_elapsed(floatorNone) – elapsed time. Defaults to None.\n\nheader(listorNone) – List of strings to print before statistics. It can be used to\ncustomize additional prints. None means no header.\nDefaults to None.\n\ndataloader(torch.dataloaderorNone) – torch dataloader. If not None, shows progress in the epoch.\nDefaults to None.\n\nSaves learning stats to file\n\npath(str) – Folder path to save the stats. Defaults to ‘’.\n\nUpdate all the stats. Typically called at the end of epoch.\n\nTensor time manipulation utilities.\n\nReplicates input in time dimension. Additional dimension of time is\nadded at the end.\n\ninput(torch tensor) – torch input tensor.\n\nnum_steps(int) – number of steps to replicate.\n\ninput replicated num_steps times in time\n\ntorch tensor\n\nExamples\n\nImplements shift in time axis.\n\ninput(torch tensor) – input tensor.\n\nshift_val(torch tensororfloatorint) – shift tensor. If it is scalar, same shift is\napplied to all the spatial dimension. Otherwise, the input’s spatial\ndimension  must match shift’s dimension.\n\nsampling_time(float) – sampling time. Defaults to 1.\n\nshifted output\n\ntorch tensor\n\nExamples\n\nCollection of utilities.\n\nCreates a binary mask with ones around the major diagonal defined bynum_diagonalsnum_diagonals.\n\ndim(int) – dimension of the mask matrix\n\nnum_diagonals(int) – number of diagonals. The number gets rounded up to the nearest odd\nnumber. 1 means an identity matrix.\n\nmask tensor\n\ntorch tensor\n\nBases:`dict`dict\n\nDot notation access to dictionary attributes. For e.g.`my_dict[\"key\"]`my_dict[\"key\"]is same as`my_dict.key`my_dict.key\n\nCalculate the rate of event (non-zero value) in a torch tensor. If\nthe tensor has more than one time dimesion, first dimension is ignored\nas it represents initialization events.\n\nx(torch.tensor) – Input torch tensor.\n\nAverage event rate.\n\nfloat\n\nBases:`property`property\n\nwraps static member function of a class as a static property of that\nclass.\n\nBases:`object`object\n\nAssistant that bundles training, validation and testing workflow.\n\nnet(torch.nn.Module) – network to train.\n\nerror(objectorlambda) – an error object or a lambda function that evaluates error.\nIt is expected to take`(output,target)`(output,target)|`(output,label)`(output,label)as it’s argument and return a scalar value.\n\noptimizer(torch optimizer) – the learning optimizer.\n\nstats(slayer.utils.stats) – learning stats logger. If None, stats will not be logged.\nDefaults to None.\n\nclassifier(slayer.classifierorlambda) – classifier object or lambda function that takes output and\nreturns the network prediction. None means regression mode.\nClassification steps are bypassed.\nDefaults to None.\n\ncount_log(bool) – flag to enable count log. Defaults to False.\n\nlam(float) – lagrangian to merge network layer based loss.\nNone means no such additional loss.\nIf not None, net is expected to return the accumulated loss as second\nargument. It is intended to be used with layer wise sparsity loss.\nDefaults to None.\n\nthe main device memory where network is placed. It is not at start and\ngets initialized on the first call.\n\ntorch.device or None\n\nReduces the learning rate of the optimizer by`factor`factor.\n\nfactor(float) – learning rate reduction factor. Defaults to 10/3.\n\nTesting assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nTraining assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nValidation assistant.\n\ninput(torch tensor) – input tensor.\n\ntarget(torch tensor) – ground truth or label.\n\noutput– network’s output.count(optional) – spike count if`count_log`count_logis enabled\n\noutput– network’s output.\n\ncount(optional) – spike count if`count_log`count_logis enabled\n\nBases:`object`object\n\nLearning stat manager\n\naccumulated loss sum.\n\nfloat\n\naccumulated correct samples.\n\nint\n\nnumber of samples accumulated.\n\nint\n\nbest loss recorded.\n\nfloat\n\nbest accuracy recorded.\n\nfloat\n\nlog of loss stats.\n\nlist of float\n\nlog of accuracy stats.\n\nlist of float\n\ndoes current epoch have best loss? It is updated only\nafterstats.update()stats.update().\n\nbool\n\ndoes current epoch have best accuracy? It is updated only\nafterstats.update()stats.update().\n\nbool\n\nString method.\n\nCurrent accuracy.\n\nCurrent loss.\n\nReset stat.\n\nUpdate stat.\n\nBases:`object`object\n\nManages training, validation and testing stats.\n\nLearningStatLearningStatobject to manage training statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.LearningStat)LearningStat\n\nLearningStatLearningStatobject to manage testing statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.LearningStat)LearningStat\n\nLearningStatLearningStatobject to manage validation statistics.\n\n[LearningStat](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.LearningStat)LearningStat\n\nString summary of stats.\n\nForces stats printout on new line.\n\nPlots the training curves.\n\nfigures(tupleofints) – figures to plot loss and accuracy. Defaults to (1, 2).\n\nfigsize(tupleofintsorNone) – custom width and height of the figure. None means default size.\nDefaults to None.\n\npath(str) – If not None, saves the plot to the path specified.\nDefaults to None.\n\nDynamic print method for stats.\n\nepoch(int) – current epoch\n\niter(intorNone) – iteration number in epoch. Defaults to None.\n\ntime_elapsed(floatorNone) – elapsed time. Defaults to None.\n\nheader(listorNone) – List of strings to print before statistics. It can be used to\ncustomize additional prints. None means no header.\nDefaults to None.\n\ndataloader(torch.dataloaderorNone) – torch dataloader. If not None, shows progress in the epoch.\nDefaults to None.\n\nSaves learning stats to file\n\npath(str) – Folder path to save the stats. Defaults to ‘’.\n\nUpdate all the stats. Typically called at the end of epoch.\n\nalias of[MODE](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.quantize.MODE)`MODE`MODE\n\nCreates a binary mask with ones around the major diagonal defined bynum_diagonalsnum_diagonals.\n\ndim(int) – dimension of the mask matrix\n\nnum_diagonals(int) – number of diagonals. The number gets rounded up to the nearest odd\nnumber. 1 means an identity matrix.\n\nmask tensor\n\ntorch tensor\n\nBases:`dict`dict\n\nDot notation access to dictionary attributes. For e.g.`my_dict[\"key\"]`my_dict[\"key\"]is same as`my_dict.key`my_dict.key\n\nImplements quantization of parameters. Round or floor behavior can be\nselected using mode argument.\n\ninput(torch tensor) – input tensor\n\nstep(float) – quantization step. Default is 1.\n\nmode([MODE](https://lava-nc.org/lava-lib-dl/slayer/utils/utils.html#lava.lib.dl.slayer.utils.quantize.MODE)MODE) – quantization mode. Default is MODE.ROUND.\n\nquantized tensor\n\ntorch tensor\n\nExamples\n\nBases:`property`property\n\nwraps static member function of a class as a static property of that\nclass.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>output=conv(input,filter)\n``````\n\n``````\n>>>output=corr(input,filter)\n``````\n\n``````\n>>># Quantize in step of 0.5>>>x_quantized=quantize(x,step=0.5)\n``````\n\n``````\n>>>input=torch.rand(2,3,4)>>>out=replicate(input,10)\n``````\n\n``````\n>>>output=shift(input,7)>>>output=shift(torch.rand(1,10,100),torch.arange(10))\n``````\n\n``````\n>>># Quantize in step of 0.5>>>x_quantized=quantize(x,step=0.5)\n``````\n\n``````\nMODE\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.html",
    "title": "lava — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.connect.html",
    "title": "lava.lib.dnf.connect — Lava  documentation",
    "content": "Creates and returns a Connections Process <conn> and connects the source\nOutPort <src_op> to the InPort of <conn> and the OutPort of <conn> to the\nInPort of <dst_ip>.\n\nThe connectivity is generated from a list of operation objects <ops>.\nEach operation generates a dense connectivity matrix based\non its parameters. These matrices are multiplied into a single\nconnectivity matrix, which is then used to generate a Connections Process\nbetween source and destination.\n\nsrc_op([OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – OutPort of the source Process that will be connected\n\ndst_ip([InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort) – InPort of the destination Process that will be connected\n\nops(list([AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)AbstractOperation),optional) – list of operations that describes how the connection between\n<src_op> and <dst_ip> will be created\n\nconnection_class(type(Sparse) ortype([Dense](https://lava-nc.org/lava-lib-dl/bootstrap/block/block.html#lava.lib.dl.bootstrap.block.cuba.Dense)Dense),optional) – Class of the process used between src_op and dst_ip. If connection_class\nis None the connection process will be defined automatically\n(currently a Sparse Process is used in that case).\n\nconnections– Process containing the connections between <src_op> and <dst_ip>\n\nSparse or Dense Process\n\nBases:`Exception`Exception\n\nException that is raised when the connection function is misconfigured\nwith a wrong combination of operations.\n\ncustom exception message that overwrites the default\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.html",
    "title": "Lava - Dynamic Neural Fields — Lava  documentation",
    "content": "Lava-DNF is a library within the Lava software framework. It provides\nLava Processes and other software infrastructure to build architectures\ncomposed of Dynamic Neural Fields (DNFs). In particular, it provides\nfunctions to generate connectivity patterns common to DNF architectures.\n\nDNFs are neural attractor networks that generate\nstabilized activity patterns in recurrently connected populations of neurons.\nThese activity patterns form the basis of neural representations, decision\nmaking, working memory, and learning.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.inputs.gauss_pattern.html",
    "title": "lava.lib.dnf.inputs.gauss_pattern — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nPyLoihiProcessModel for GaussPatternProcess.\n\nImplements the behavior of sending a gauss pattern asynchronously when\na change is triggered.\n\nalias of[GaussPattern](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.inputs.gauss_pattern.html#lava.lib.dnf.inputs.gauss_pattern.process.GaussPattern)`GaussPattern`GaussPattern\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nGauss pattern generator Process.\n\nThis process generates Gauss patterns and send them through\nthe OutPort a_out.\nIt recomputes new patterns and sends them asynchronously only when one of\nthe parameters amplitude, mean or stddev changes.\nOtherwise, sends an array full of numpy.nan.\n\nnumber of neurons per dimension, e.g. shape=(30, 40)\n\namplitude of the Gauss pattern\n\nmean of the Gauss pattern\n\nstandard deviation of the Gauss pattern\n\nGet value of the amplitude Var\n\namplitude\n\nnumpy.ndarray\n\nGet value of the mean Var\n\nmean\n\nnumpy.ndarray\n\nGet value of the shape Var\n\nshape\n\nnumpy.ndarray\n\nGet value of the stddev Var\n\nstddev\n\nnumpy.ndarray\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nndarray\n``````\n\n``````\nGaussPattern\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.inputs.html",
    "title": "lava.lib.dnf.inputs — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.inputs.rate_code_spike_gen.html",
    "title": "lava.lib.dnf.inputs.rate_code_spike_gen — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nPyLoihiProcessModel for SpikeGeneratorProcess.\n\nImplements the behavior of a rate-coded spike input generator.\n\nalias of[RateCodeSpikeGen](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.inputs.rate_code_spike_gen.html#lava.lib.dnf.inputs.rate_code_spike_gen.process.RateCodeSpikeGen)`RateCodeSpikeGen`RateCodeSpikeGen\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nSpike generator Process for rate-coded input.\n\nThis process generates spike trains based on patterns it receives through\nits InPort a_in.\nIt interprets these patterns as spiking rates (rate coding).\n\nReceives a new pattern through a_in only once and while and trigger state\nupdate upon receipt of new pattern.\nIn other time steps, receives null patterns (array full of numpy.nan).\nSends spike values through its OutPort s_out every time step.\n\nnumber of neurons per dimension, e.g. shape=(30, 40)\n\nminimum spike rate\n(neurons with rates below this value will never spike)\n\nseed used for computing first spike times everytime pattern changes\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyInPort\n``````\n\n``````\nndarray\n``````\n\n``````\nRateCodeSpikeGen\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html",
    "title": "lava.lib.dnf.kernels — Lava  documentation",
    "content": "Bases:`ABC`ABC\n\nMixin for kernels that are generated with the gauss function.\n\namp_exc(float) – amplitude of the excitatory Gaussian of the kernel\n\nwidth_exc(list(float)) – widths of the excitatory Gaussian of the kernel\n\nlimit(float) – determines the size/shape of the kernel such that the weight matrix\nwill have the size 2*limit*width_exc; defaults to 1\n\nshape(tuple(int),optional) – will return the weight with this explicit shape; if used, the limit\nargument will have no effect\n\nBases:`object`object\n\nRepresents a kernel that can be used in the Convolution operation.\n\nweights(numpy.ndarray) – weight matrix of the kernel\n\npadding_value(float,optional) – value that is used to pad the kernel when the Convolution operation\nuses BorderType.PADDED\n\nReturns the padding value\n\nReturns the weights\n\nBases:[GaussianMixin](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html#lava.lib.dnf.kernels.kernels.GaussianMixin)`GaussianMixin`GaussianMixin,[Kernel](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html#lava.lib.dnf.kernels.kernels.Kernel)`Kernel`Kernel\n\n“Mexican hat” kernel (local excitation and mid-range inhibition) for a\nDNF that enables it to create multiple peaks.\n\namp_inh(float) – amplitude of the inhibitory Gaussian of the kernel\n\nwidth_inh(list(float)) – widths of the inhibitory Gaussian of the kernel\n\nBases:[GaussianMixin](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html#lava.lib.dnf.kernels.kernels.GaussianMixin)`GaussianMixin`GaussianMixin,[Kernel](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html#lava.lib.dnf.kernels.kernels.Kernel)`Kernel`Kernel\n\nA kernel that enables creating a selective dynamic neural field\n(local excitation, global inhibition).\n\namp_exc(float) – amplitude of the excitatory Gaussian of the kernel\n\nwidth_exc(list(float)) – widths of the excitatory Gaussian of the kernel\n\nglobal_inh(float) – global inhibition of the kernel; must be negative\n\nlimit(float) – determines the size/shape of the kernel such that the weight matrix\nwill have the size 2*limit*width_exc; defaults to 1\n\nshape(tuple(int),optional) – will return the weight with this explicit shape; if used, the limit\nargument will have no effect\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nGaussianMixin\n``````\n\n``````\nKernel\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html",
    "title": "lava.lib.dnf.operations — Lava  documentation",
    "content": "Bases:`Enum`Enum\n\nAn enumeration.\n\nValidate type of <border_type>\n\n`None`None\n\nBases:`Enum`Enum\n\nEnum for reduce methods of ReduceDims operation\n\nValidate type of <reduce_op>\n\n`None`None\n\nBases:`Exception`Exception\n\nException that is raised when an operation is misconfigured.\n\ncustom exception message that overwrites the default\n\nBases:`ABC`ABC\n\nAbstract Operation, subclasses of which can be used to parameterize the\nconnect() function.\n\nshape_handler([AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)AbstractShapeHandler) – handles, configures, and validates the input and output shape of the\noperation\n\nComputes the connectivity weight matrix of the operation.\nThis public method only validates the configuration of the\noperation. The actual weights are computed in the\nabstract method _compute_weights().\n\nconnectivity weight matrix\n\nnumpy.ndarray\n\nConfigures an operation by setting its input and output shape.\n\ninput_shape(tuple(int)) – input shape of the operation\n\n`None`None\n\nReturn the output shape of the operation\n\nReturn the output shape of the operation\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nCreates connectivity that resembles a convolution with a kernel.\nPerhaps contrary to other implementations of the convolution, this\noperation always leaves the shape of the input intact. That is, a\nConvolution operation applied, for instance, to the output of a\npopulation of neurons of shape (42, 42) will also yield an output of\nshape (42, 42).\n\nkernel([Kernel](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.kernels.html#lava.lib.dnf.kernels.kernels.Kernel)Kernel) – kernel of weights that the input will be convolved with; must be of the\nsame dimensionality as the input\n\nborder_types([BorderType](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.enums.BorderType)BorderTypeorlist([BorderType](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.enums.BorderType)BorderType)) – determines how the Convolution operation treats borders; valid values\nare (1) PADDED, in which case the borders will be padded with a value\nthat can be specified in the Kernel or (2) CIRCULAR, in which case\nthe values from the other side of the input will be used as ‘padding’\n(this is sometimes also called “wrapped”)\n\nReturns the list of border types\n\nReturns the kernel\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nCreates connectivity that projects the output of a source population\nonto the diagonal of the target population, where the target\npopulation has twice the number of dimensions as the source\npopulation. The dimensions of the source population can only have odd\nsizes. For instance, if the source population is a grid of\nneurons of shape (99, 79), the operation will project the output of\nthat two-dimensional population into a 4D target population of shape\n(50, 40, 50, 40) along its diagonal. Each entry in the output shape is\ncomputed by out_size = (in_size + 1) / 2.\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nOperation that expands the dimensionality of the input by projecting\nthe dimensions of the input to the newly added dimensions.\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nCreates connectivity that flips all specified dimensions.\n\ndims(tuple(int) orint) – indices of the dimensions that are to be flipped\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nCreates connectivity that projects the output of a source population\nalong its diagonal. For instance, if the source population is a grid of\nneurons of shape (40, 40), the operation will project (sum) the output of\nthat two-dimensional population along its diagonal, yielding a\none-dimensional output of shape (79,). The output size is computed by\n79 = 40 * 2 - 1.\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nOperation that reduces the dimensionality of the input by projecting\na specified subset of dimensions onto the remaining dimensions.\n\nreduce_dims(intortuple(int)) – indices of dimension that will be reduced/removed\n\nreduce_method([ReduceMethod](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.enums.ReduceMethod)ReduceMethod) – method by which the dimensions will be reduced (SUM or MEAN)\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nOperation that reorders the dimensions in the input to a specified new\norder.\n\norder(tuple(int)) – new order of the dimensions (see ReorderDimsShapeHandler)\n\nBases:[AbstractOperation](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.operations.AbstractOperation)`AbstractOperation`AbstractOperation\n\nOperation that generates one-to-one connectivity with given weights for\nevery synapse.\n\nweight(float) – weight used for every connection\n\nBases:`ABC`ABC\n\nAbstract class for handling input and output shape of the\nAbstractOperation class.\n\nAssert that input and output shape is configured.\n\n`None`None\n\nConfigures the input and output shape of an operation given an input\nshape.\n\ninput_shape(tuple(int)) – input shape of an operation\n\n`None`None\n\nReturn the input shape of the handler\n\nReturn the output shape of the handler\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for the ExpandAlongDiagonal operation, which projects\ndiagonally onto a multi-dimensional population.\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for operations that expand the dimensionality. New\ndimensions (axes) will be appended to the already existing ones of the\ninput. Their sizes must be specified using the <new_dims_shape> argument.\n\nnew_dims_shape(intortuple(int)) – shape of the added dimensions; they will be appended to the shape\nof the input, for instance an input shape (2,) and\nnew_dims_shape=(6, 8) will produce an output shape (2, 6, 8)\n\nReturn the <new_dims_shape> attribute\n\nBases:[KeepShapeShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.KeepShapeShapeHandler)`KeepShapeShapeHandler`KeepShapeShapeHandler\n\nShape handler for the Flip operation that flips specified dimensions.\n\ndims(tuple(int) orint) – indices of the dimensions that are to be flipped\n\nReturn the <dims> that are to be flipped\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for operations that do not change the shape of the\ninput.\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for the ReduceAlongDiagonal operation, which projects\ndiagonally from a multi-dimensional population.\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for operations that reduce the dimensionality of the\ninput.\n\nreduce_dims(intortuple(int)) – indices of the dimensions to remove\n\nReturn the output shape of the handler\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for operations that reorder the input shape.\n\norder(tuple(int)) – order of the dimensions of the output; for instance if the input shape\nis (1, 2, 3) and order=(0, 2, 1), the output shape will be (1, 3, 2);\nmust have the same number of elements as the input and output shape\n\nReturn the order of the handler\n\nBases:[AbstractShapeHandler](https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.operations.html#lava.lib.dnf.operations.shape_handlers.AbstractShapeHandler)`AbstractShapeHandler`AbstractShapeHandler\n\nShape handler for operations that reshape the input, changing\nthe shape but keeping the number of elements constant.\n\noutput_shape(tuple(int)) – output shape of an operation\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractOperation\n``````\n\n``````\nAbstractShapeHandler\n``````\n\n``````\nKeepShapeShapeHandler\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.dnf.utils.html",
    "title": "lava.lib.dnf.utils — Lava  documentation",
    "content": "Computes the dimensionality of a shape, assuming that (1,) represents\na zero-dimensional shape.\n\nshape(tuple(int)) – shape of a population of neurons\n\nnumber of dimensions\n\nint\n\nComputes the number of neurons from a shape.\n\n`int`int\n\nshape of a neural population (or input)\n\n:\nnum_neurons : int\n\nnumber of neurons\n\nConverts float, tuple, or list variables to numpy.ndarray.\n\nx(float,tuple,list,numpy.ndarray) – variable to convert\n\nreturn– input converted to numpy.ndarray\n\nnumpy.ndarray\n\nEvaluates the Gaussian function over a specified domain in multiple\ndimensions.\n\nIf a domain is specified, the function will evaluate the Gaussian at\nlinearly interpolated values between the lower and upper bounds of the\ndomain. The number of samples is determined by the shape parameter. For\nexample, for given parameters shape=(5,) and domain=[[-5, -1]], it will\nevaluate the function at positions -5,-4,-3,-2,-1.\n\nIf no domain is specified, the function will evaluate the Gaussian at the\nindices of the sampling points. For instance, for a given shape of\nshape=(5,), it will evaluate the function at positions 0,1,2,3,4.\n\nshape(tuple(int)) – number of sampling points along each dimension\n\ndomain(numpy.ndarray,optional) – lower and upper bound of input values for each dimension at which\nthe Gaussian function is evaluated\n\namplitude(float,optional) – amplitude of the Gaussian, defaults to 1\n\nmean(numpy.ndarray,optional) – mean of the Gaussian, defaults to 0\n\nstddev(numpy.ndarray,optional) – standard deviation of the Gaussian, defaults to 1\n\ngaussian– multi-dimensional array with samples of the Gaussian\n\nnumpy.ndarray\n\nChecks whether n is an odd number.\n\nn(int) – number to check\n\nTrue if <n> is an odd number\n\n`bool`bool\n\nComputes instantaneous spike rates for all neurons over time\n\nThis method uses the window_size parameter to derive a kernel with which\nit convolves the spike_data.\nYields an array of the same shape, with each value representing the spike\nrate of a neuron over the specified time window.\n\nspike_data(numpy.ndarray) – spike data of dtype=bool (spike: 1, no-spike: 0) and\nshape (num_time_steps, num_neurons)\n\nwindow_size(int,optional) – size of the time window in number of time steps\n\nspike_rates– array of same shape as spike_data which represents the instantaneous\nspike rate of every neuron at every time step\n\nnumpy.ndarray\n\nCreates a raster plot, showing the spikes of all neurons over time.\n\nThe plot will use color to express the spike rate within a time window\ndetermined by rate_window (specified in number of time steps).\n\nspike_data(numpy.ndarray) – spike data of dtype=bool (spike: 1, no-spike: 0) and\nshape (num_time_steps, num_neurons)\n\nwindow_size(int,optional) – size of the time window in number of time steps\n\n`None`None\n\nValidate and potentially convert shape parameter.\n\nThe shape of different elements of the DNF library can be passed in as\ntype tuple(int) or list(int) for multiple dimensions, or type int for a\nsingle dimension. In all cases, it is converted to tuple(int).\n\n`Tuple`Tuple[`int`int,`...`...]\n\nshape parameter to be validated\n\n:\nshape : tuple(int)\n\nvalidated and converted shape parameter\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-dnf/lava.lib.html",
    "title": "lava.lib — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.html",
    "title": "lava — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.html",
    "title": "lava.lib — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.html",
    "title": "Lava - Optimization — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.bayesian.html",
    "title": "lava.lib.optimization.problems.bayesian — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nA Python-based implementation of the DualInputFunction process that\nrepresents a dual continuous input, single output, non-linear objective\nfunction.\n\nalias of[DualInputFunction](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.bayesian.html#lava.lib.optimization.problems.bayesian.processes.DualInputFunction)`DualInputFunction`DualInputFunction\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\ntick the model forward by one time-step\n\n`None`None\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nA Python-based implementation of the SingleInput process that represents a\nsingle input/output non-linear objective function.\n\nalias of[SingleInputFunction](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.bayesian.html#lava.lib.optimization.problems.bayesian.processes.SingleInputFunction)`SingleInputFunction`SingleInputFunction\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\ntick the model forward by one time-step\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nA base objective function process that shall be used as the basis of\nall black-box processes.\n\nBases:[BaseObjectiveFunction](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.bayesian.html#lava.lib.optimization.problems.bayesian.processes.BaseObjectiveFunction)`BaseObjectiveFunction`BaseObjectiveFunction\n\nAn abstract process representing a dual input, single output\ntest function.\n\nBases:[BaseObjectiveFunction](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.bayesian.html#lava.lib.optimization.problems.bayesian.processes.BaseObjectiveFunction)`BaseObjectiveFunction`BaseObjectiveFunction\n\nAn abstract process representing a single input/output test function.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nDualInputFunction\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyInPort\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nSingleInputFunction\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nBaseObjectiveFunction\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html",
    "title": "lava.lib.optimization.problems — Lava  documentation",
    "content": "Bases:`object`object\n\nEvaluate the polynomial at the given point.\n\n`ndarray`ndarray\n\nMaximum order among the coefficients’ ranks.\n\nBases:`object`object\n\nEqualityConstraints object defined by tensor coefficients.\n\nInequalityConstraints object defined by tensor coefficients.\n\nBases:`object`object\n\nA set of constraints, including both discrete and arithmetic.\n\nDiscrete constraints can be of any arity and are defined by a tuple\ncontaining variable subsets and a relation tensor. Arithmetic constraints\ninclude equality and inequality constraints and are defined by a series\nof tensors defining the coefficients of scalar function of the variable\nvectors.\n\nConstraints defined via an arithmetic relation.\n\nConstraints over discrete variables only, defined via a relation.\n\nBases:`object`object\n\nSet of constraints involving only discrete variables.\n\nconstraints(Listofconstraints each as an n-tuple where the) –\n\nthe(first n-1 elements are the variables related by the n-th element of) –\n\nvariables(tuple. The n-th element is a tensor indicating what valuesofthe) –\n\nallowed.(are simultaneously) –\n\nvariables)(Although initially intended for tensorsofrank=2(binary) –\n\n:param :\n:param other ranks simply mean relations between any number of variables and:\n:param thus are allowed. In this way:\n:param tensor’s rank corresponds to the Arity of the:\n:param constraint they define.:\n\nUser specified constraints.\n\nExtract relations and variable subsets from constraints.\n\nconstraints(Listofconstraints each as an n-tuple where the first) –\n\nthe(n-1 elements are the variables related by the n-th element of) –\n\ndetails.(tuple(a tensor). See class docstring for more) –\n\nList of tensors specifying discrete constraint over var subsets.\n\nSet relations and variable subsets from constraints.\n\nconstraints(Listofconstraints each as an n-tuple where the first) –\n\nthe(n-1 elements are the variables related by the n-th element of) –\n\ndetails.(tuple(a tensor). See class docstring for more) –\n\nVerify relation size match domain sizes of affected variables.\n\nsubsets(Listofvariable subsets affected by the corresponding) –\n\nrelation.–\n\nrelations(Listoftensors specifying discrete constraint over var) –\n\nsubsets.–\n\nList of variable subsets affected by the corresponding relation.\n\nBases:[CoefficientTensorsMixin](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.coefficients.CoefficientTensorsMixin)`CoefficientTensorsMixin`CoefficientTensorsMixin\n\nList of equality constraints defined by tensor coefficients.\n\nWe consider generalized constraints of arbitrary degree:\n\nh(x) = 0\n\nwhere the terms ofh(x)have the form:\n\ng(x)= \\sum_{ijk...} \\epsilon_{ijk...} \\cdot x_i \\cdot x_j\n\\cdot x_k \\cdot ...\n\ncoefficients(tensor) – coefficients defining the constraints.\n\nBases:[CoefficientTensorsMixin](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.coefficients.CoefficientTensorsMixin)`CoefficientTensorsMixin`CoefficientTensorsMixin\n\nList of inequality constraints defined by tensor coefficients.\n\nWe consider generalized constraints of arbitrary degree:\n\ng(x) \\leq 0\n\nwhere the terms ofg(x)have the form:\n\n\\sum_{ijk...} \\epsilon_{ijk...} \\cdot x_i \\cdot x_j \\cdot x_k\n\\cdot ...\n\ncoefficients(tensor) – coefficients defining the constraints.\n\nBases:[CoefficientTensorsMixin](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.coefficients.CoefficientTensorsMixin)`CoefficientTensorsMixin`CoefficientTensorsMixin\n\nCost function of an optimization problem.\n\ncoefficients(`Union`Union[`List`List,`_SupportsArray`_SupportsArray[`dtype`dtype[`Any`Any]],`_NestedSequence`_NestedSequence[`_SupportsArray`_SupportsArray[`dtype`dtype[`Any`Any]]],`bool`bool,`int`int,`float`float,`complex`complex,`str`str,`bytes`bytes,`_NestedSequence`_NestedSequence[`Union`Union[`bool`bool,`int`int,`float`float,`complex`complex,`str`str,`bytes`bytes]]]) – cost tensor coefficients.\n\naugmented_terms(`Optional`Optional[`Tuple`Tuple[`Union`Union[`List`List,`_SupportsArray`_SupportsArray[`dtype`dtype[`Any`Any]],`_NestedSequence`_NestedSequence[`_SupportsArray`_SupportsArray[`dtype`dtype[`Any`Any]]],`bool`bool,`int`int,`float`float,`complex`complex,`str`str,`bytes`bytes,`_NestedSequence`_NestedSequence[`Union`Union[`bool`bool,`int`int,`float`float,`complex`complex,`str`str,`bytes`bytes]]],`...`...]]) – Tuple of terms not originally defined in the cost\n\nfunction, e.g. a regularization term or  those incorporating constraints\ninto the cost. Tuple elements have the same type as coefficients.\n\nEvaluate the cost at the given solution.\n\n`ndarray`ndarray\n\nAugmented terms present in the cost function.\n\nWhether augmented terms are present in the cost function.\n\nBases:[OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)`OptimizationProblem`OptimizationProblem\n\nA constraint satisfaction problem.\n\nThe CSP is in usually represented by the tuple (variables, domains,\nconstraints). However, because every variable must have a domain, the\nuser only provides the domains and constraints.\n\ndomains(either a listoftuples with values that each variable can take or) –\n\nvariable.(a listofintegers specifying the domain size for each) –\n\nconstraints(Discrete constraints defining mutually allowed values) –\n\nelements(between variables. Has to be a listofn-tuples where the first n-1) –\n\nelement(are the variables related by the n-th elementofthe tuple. The n-th) –\n\nsimultaneously(is a tensor indicating what valuesofthe variables are) –\n\nallowed.–\n\nSpecification of mutually allowed values between variables.\n\nConstant cost function, CSPs require feasibility not minimization.\n\nDiscrete variables over which the problem is specified.\n\nBases:[IQP](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.IQP)`IQP`IQP\n\nClass to instantiate an Integer Linear Programming (ILP) problem in the\nstandard form as:\n\nmin c^Tx \\\\\nAx >= b \\\\\nx >= 0 \\\\\nx in Z\n\nBases:[OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)`OptimizationProblem`OptimizationProblem\n\nClass to instantiate an Integer Quadratic Programming (IQP) problem in the\nstandard form as:\n\nmin x^THx+c^Tx \\\\\nAx >= b \\\\\nx >= 0 \\\\\nx in Z\n\nSpecification of mutually allowed values between variables.\n\nCost function.\n\nEvaluate constraints at provided solution as $Ax-b$.\n\n`ndarray`ndarray\n\nEvaluate cost of provided solution.\n\n`int`int\n\nDiscrete variables over which the problem is specified.\n\nBases:`ABC`ABC\n\nInterface for any concrete optimization problem.\n\nAny optimization problem can be defined by a set of variables, cost and\nconstraints.  Although for some problems some of these elements may be\nabsent, the user still has to specify them, e.g., defining constraints as\nNone.\n\nConstrains to be satisfied by mutual assignments to variables.\n\nFunction to be optimized and defined over the problem variables.\n\nVariables over which the optimization problem is defined.\n\nBases:[OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)`OptimizationProblem`OptimizationProblem\n\nAn interface for the QP solver. Equality Constraints should be of the\nform Ax=k. Support for inequality constraints is pending.\nThe cost of the QP is of the form 1/2*x^t*Q*x + p^Tx. The problem has to\nbe preconditioned with a preconditioner. The library currently supports\nruiz preconditioning which is a useful preconditioner for block diagonal\nQPs. Other preconditioners can be added in the future. Calling the\nprecondition_problem method preconditions the problem and modifies the\nmatrices and vectors that constitute the problem.\n\nQuadratic term of the cost function\n\nLinear term of the cost function, defaults vector of zeros of the\nsize of the number of variables in the QP\n\nInequality constrainting hyperplanes, by default None\n\nIneqaulity constraints offsets, by default None\n\nEquality constrainting hyperplanes, by default None\n\nEqaulity constraints offsets, by default None\n\nValueError exception raised if equality or inequality constraints\nare not properly defined. Ex: Defining A_eq while not defining k_eq\nand vice-versa.\n\nConstrains to be satisfied by mutual assignments to variables.\n\nFunction to be optimized and defined over the problem variables.\n\nEvalue constraint violations A_eq@x - k_eq. Returns a vector of all\nconstraint violations\n\nEvaluates the quadratic cost 1/2x^TQx + p^Tx. Returns a scalar cost.\n\nUsed to precondition problems before they can be used in the solver.\nOnly ruiz preconditioning supported at the moment. Other preconditioners\ncan be added by following the example of the Ruiz preconditioner.\n\nVariables over which the optimization problem is defined.\n\nBases:[OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)`OptimizationProblem`OptimizationProblem\n\nAs an unconstrained problem, QUBO constraints are None.\n\nQuadratic cost to be minimized.\n\n`int`int\n\nValidate that cost coefficient is a square, symmetric, int matrix.\n\nq(Quadratic coefficientofthe cost function.) –\n\nBinary variables of the QUBO problem.\n\nBases:`object`object\n\nSet of variables to which any values in specified ranges can be assigned.\n\nbounds(Listof2-tuples defining the range from which each corresponding) –\n\nvalues.(variable(by index)can take) –\n\nLimit values defining the ranges of allowed values for variables.\n\nNumber of variables in this set.\n\nList of continuous variables as instances of the Variable class.\n\nBases:`object`object\n\nA set of variables which can only be assigned discrete values.\n\ndomains(Listoftuples with values that each variable can takeorList) –\n\nvariable.(of domain sizes for each corresponding(by index)) –\n\nNumber of elements on the domain of each discrete variable.\n\nList of tuples containing the values that each variable can take.\n\nNumber of variables in this set.\n\nList of discrete variables each an instance of the Variable class.\n\nBases:`object`object\n\nAn entity to which a value can be assigned.\n\nname(Optional name for the variable.) –\n\nVariable’s current value.\n\nBases:`object`object\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nCoefficientTensorsMixin\n``````\n\n``````\nOptimizationProblem\n``````\n\n``````\nIQP\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.bayesian.html",
    "title": "lava.lib.optimization.solvers.bayesian — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nA Python-based implementation of the Bayesian Optimizer processes. For\nmore information, please refer to bayesian/processes.py.\n\nfinalize the optimization processing upon runtime conclusion\n\n`None`None\n\nalias of[BayesianOptimizer](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.bayesian.html#lava.lib.optimization.solvers.bayesian.processes.BayesianOptimizer)`BayesianOptimizer`BayesianOptimizer\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\ninitialize the search space from the standard schema\n\nThis method is designed to convert the numpy ndarray-based search\nspace description int scikit-optimize format compatible with all\nlower-level processes. Your search space should consist of three\ntypes of parameters:\n\n(“continuous”, <min_value>, <max_value>, np.nan, <name>)\n\n(“integer”, <min_value>, <max_value>, np.nan, <name>)\n\n(“categorical”, np.nan, np.nan, <choices>, <name>)\n\nsearch_space– A collection of continuous and discrete dimensions that represent\nthe entirety of the problem search space\n\nlist[Union[Real, Integer]]\n\nparse vec into params/objectives before informing optimizer\n\nvec(np.ndarray) – A single array of data from the black-box process containing\nall parameters and objectives for a total length of num_params\n+ num_objectives\n\n`None`None\n\ntick the model forward by one time-step\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nAn abstract process defining the internal state and input/output\nvariables required by all Bayesian optimizers\n\nBases:`object`object\n\nThe BayesianSolver is a class-based interface to abstract the details\nof initializing the BayesianOptimizer process and connecting it with\nthe user’s specific black-box function\n\nConduct hyperparameter optimization for the argued problem.\n\nname(str) – a unique identifier for the given experiment\n\nnum_iter(int) – the number of Bayesian iterations to conduct\n\nproblem([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – the black-box function whose parameters are represented by the\nBayesian optimizer’s search space\n\nsearch_space(np.ndarray) – At every index, your search space should consist of three types\nof parameters:\n1) (“continuous”, <min_value>, <max_value>, np.nan, <name>)\n2) (“integer”, <min_value>, <max_value>, np.nan, <name>)\n3) (“categorical”, np.nan, np.nan, <choices>, <name>)\n\n`None`None\n\nInitialize the BayesianSolver interface.\n\nacq_func_config(dict) – A dictionary to specify the function to minimize over the posterior\ndistribution.\n# {\n#     “type”: str\n#         specify the function to minimize over the posterior\n#         distribution:\n#             “LCB” = lower confidence bound\n#             “EI” = negative expected improvement\n#             “PI” = negative probability of improvement\n#             “gp_hedge” = probabilistically determine which of the\n#                 aforementioned functions to use at every iteration\n#             “EIps” = negative expected improved with consideration\n#                 of the total function runtime\n#             “PIps” = negative probability of improvement\n#                 while taking into account the total function\n#                 runtime\n# }\n\nacq_opt_config(dict) – A dictionary to specify the method to minimize the acquisition\nfunction.\n# {\n#     “type” : str\n#         specify the method to minimize the acquisition function:\n#             “sampling” = random selection from the acquisition\n#                 function\n#             “lbfgs” = inverse Hessian matrix estimation\n#             “auto” = automatically configure based on the search\n#                 space\n# }\n\nip_gen_config(dict) – A dictionary to specify the method to explore the search space\nbefore the Gaussian regressor starts to converge.\n# {\n#     “type”: str\n#         specify the method to explore the search space before the\n#         Gaussian regressor starts to converge:\n#             “random” = uniform distribution of random numbers\n#             “sobol” = Sobol sequence\n#             “halton” = Halton sequence\n#             “hammersly” = Hammersly sequence\n#             “lhs” = latin hypercube sequence\n#             “grid” = uniform grid sequence\n# }\n\nnum_ips(int) – The number of points to explore with the initial point generator\nbefore using the regressor.\n\nseed(int) – An integer seed that sets the random state increases consistency\nin subsequent runs.\n\nest_config(dict) – A dictionary to specify the type of surrogate regressor to learn\nthe search space.\n#{\n#     “type”: str\n#         specify the type of surrogate regressor to learn the\n#         search space:\n#             “GP” - gaussian process regressor\n#}\n\nnum_objectives(int) – Specify the number of objectives to optimize over; currently\nlimited to single objective.\n\n`None`None\n\nvalidate the arguments associated with the solve method\n\nname(str) – A unique identifier for the given experiment.\n\nnum_iter(int) – The number of Bayesian iterations to conduct.\n\nproblem([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – The black-box function whose parameters are represented by the\nBayesian optimizer’s search space.\n\nsearch_space(np.ndarray) – At every index, your search space should consist of three types\nof parameters:\n1) (“continuous”, <min_value>, <max_value>, np.nan, <name>)\n2) (“integer”, <min_value>, <max_value>, np.nan, <name>)\n3) (“categorical”, np.nan, np.nan, <choices>, <name>)\n\nnum_ips(int) – The number of points to explore with the initial point generator\nbefore using the regressor.\n\nnum_objectives(int) – Specify the number of objectives to optimize over; currently\nlimited to single objective.\n\n`None`None\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nBayesianOptimizer\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nPyInPort\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.cost_integrator.html",
    "title": "lava.lib.optimization.solvers.generic.cost_integrator — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nCPU model for the CostIntegrator process.\n\nThe process adds up local cost components from downstream units comming as\nspike payload. It has a cost_min variable which keeps track of the best\ncost seen so far, if the new cost is better, the minimum cost is updated\nand send as an output spike to an upstream process.\nNote that cost_min is divided into the first and last three bytes.\ncost_min = cost_min_first_byte << 24 + cost_min_last_bytes\n\nalias of[CostIntegrator](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.cost_integrator.html#lava.lib.optimization.solvers.generic.cost_integrator.process.CostIntegrator)`CostIntegrator`CostIntegrator\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nExecute spiking phase, integrate input, update dynamics and send\nmessages out.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nNode that integrates cost components and produces output when a better\ncost is found.\n\nshape(tuple(int)) – The expected number and topology of the input cost components.\n\nname(str,optional) – Name of the Process. Default is ‘Process_ID’, where ID is an\ninteger value that is determined automatically.\nlog_config: Configuration options for logging.\n\nInPorts–\n\n-------–\n\ncost_in– input to be additively integrated.\n\nOutPorts–\n\n--------–\n\ncost_out_last_bytes([OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – Notifies the next process about the detection of a better cost.\nMessages the last 3 byte of the new best cost.\nTotal cost = cost_out_first_byte << 24 + cost_out_last_bytes.\n\ncost_out_first_byte([OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – Notifies the next process about the detection of a better cost.\nMessages the first byte of the new best cost.\n\nVars–\n\n----–\n\ncost– Holds current cost as addition of input spikes’ payloads\n\ncost_min_last_bytes– Current minimum cost, i.e., the lowest reported cost so far.\nSaves the last 3 bytes.\ncost_min = cost_min_first_byte << 24 + cost_min_last_bytes\n\ncost_min_first_byte– Current minimum cost, i.e., the lowest reported cost so far.\nSaves the first byte.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nndarray\n``````\n\n``````\nPyInPort\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nCostIntegrator\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html",
    "title": "lava.lib.optimization.solvers.generic — Lava  documentation",
    "content": "Bases:`object`object\n\nBuilder to dynamically create the process and model for the solver of an\noptimization problem.\n\nCreate and set the model class for the solver process in the\nbuilding pipeline.\n\ntarget_cost(int) – A cost value provided by the user as a target for the solution to be\nfound by the solver, when a solution with such cost is found and\nread, execution ends.\n\nrequirements(ty.List[[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)AbstractComputeResource]) – Specifies which resources the ProcessModel requires.\n\nprotocol([AbstractSyncProtocol](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.protocol.AbstractSyncProtocol)AbstractSyncProtocol) – The SyncProtocol that the ProcessModel implements.\n\nCreate and set a solver process for the specified optimization\nproblem.\n\nproblem([OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)OptimizationProblem) – Optimization problem defined by cost and constraints which will be\nused to ensemble the necessary variables and ports with their shape\nand initial values deriving from the problem specification.\n\nhyperparameters(dict) – A dictionary specifying values for temperature and init_value.\nBoth are array-like of. init_value defines initial values for the\nvariables defining the problem. The temperature provides the\nlevel of noise.\n\nReturns the solver process model if already created.\n\nReturns the solver process if already created.\n\nAssert the solver process model has already been created.\n\nAssert the solver process has already been created.\n\nBases:`object`object\n\nProcesses implementing an optimization problem’s constraints and their\nenforcing.\n\nBases:`object`object\n\nProcesses implementing an optimization problem’s cost function.\n\nPort sending gradient descent components to the dynamical systems.\n\nPort receiving input from dynamical systems representing\nvariables.\n\nBases:`object`object\n\nBases:`object`object\n\nProcesses implementing the variables of an optimization problem.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess implementing cost coefficients as synapses.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess implementing continuous constraints via neurons and synapses.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess which implementation holds the evolution of continuous\nvariables on the solver of an optimization problem.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess that continuously monitors cost convergence.\n\nAdditive contributions to the total cost.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nNotifies the next process about the detection of a better cost.\nMessages the last 3 byte of the new best cost.\nTotal cost = cost_out_first_byte << 24 + cost_out_last_bytes.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nNotifies the next process about the detection of a better cost.\nMessages the first byte of the new best cost.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nCurrent minimum cost, i.e., the lowest reported cost so far.\nSaves the last 3 bytes.\ncost_min = cost_min_first_byte << 24 + cost_min_last_bytes\n\nCurrent minimum cost, i.e., the lowest reported cost so far.\nSaves the first byte.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess implementing discrete constraints via synapses.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess which implementation holds the evolution of discrete variables\non the solver of an optimization problem.\n\nThe addition of all inputs (per dynamical system) at this timestep\nwill be received by this port.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nThe payload to be exchanged between the underlying dynamical systems\nwhen these fire.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nThe cost components per dynamical system underlying these\nvariables, i.e., c_i = sum_j{Q_{ij} cdot x_i}  will be sent through\nthis port. The cost integrator will then complete the cost computation\nby adding all contributions, i.e., x^T cdot Q cdot x = sum_i{c_i}.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nHolds the current value assigned to the variables by\nthe solver network.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess implementing continuous constraints via neurons and synapses.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nEvent-driven stochastic discrete dynamical system with two outputs.\n\nThe main output is intended as input to other dynamical systems on\nthe network, whilst the second output is to transfer local information to be\nintegrated by an auxiliary dynamical system or circuit.\n\nThe addition of all inputs (per dynamical system) at this\ntimestep will be received by this port.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nThe payload to be sent to other dynamical systems when firing.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nthe cost component corresponding to this dynamical system, i.e.,\nc_i = sum_j{Q_{ij} cdot x_i}  will be sent through this port. The cost\nintegrator will then complete the cost computation  by adding all\ncontributions, i.e., x^T cdot Q cdot x = sum_i{c_i}.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess that continuously monitors satisfiability convergence.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nEvent-driven stochastic discrete dynamical system with two outputs.\n\nThe main output is intended as input to other dynamical systems on\nthe network, whilst the second output is to transfer local information to be\nintegrated by an auxiliary dynamical system or circuit.\n\nThe addition of all inputs (per dynamical system) at this\ntimestep will be received by this port.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nThe payload to be sent to other dynamical systems when firing.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nthe cost component corresponding to this dynamical system, i.e.,\nc_i = sum_j{Q_{ij} cdot x_i}  will be sent through this port. The cost\nintegrator will then complete the cost computation  by adding all\ncontributions, i.e., x^T cdot Q cdot x = sum_i{c_i}.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nEvent-driven stochastic discrete dynamical system with two outputs.\n\nThe main output is intended as input to other dynamical systems on\nthe network, whilst the second output is to transfer local information to be\nintegrated by an auxiliary dynamical system or circuit.\n\nThe addition of all inputs (per dynamical system) at this\ntimestep will be received by this port.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nThe payload to be sent to other dynamical systems when firing.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nthe cost component corresponding to this dynamical system, i.e.,\nc_i = sum_j{Q_{ij} cdot x_i}  will be sent through this port. The cost\nintegrator will then complete the cost computation  by adding all\ncontributions, i.e., x^T cdot Q cdot x = sum_i{c_i}.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nEvent-driven stochastic discrete dynamical system with two outputs.\n\nThe main output is intended as input to other dynamical systems on\nthe network, whilst the second output is to transfer local information to be\nintegrated by an auxiliary dynamical system or circuit.\n\nThe addition of all inputs (per dynamical system) at this\ntimestep will be received by this port.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nTodo: deprecate\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nThe payload to be sent to other dynamical systems when firing.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nthe cost component corresponding to this dynamical system, i.e.,\nc_i = sum_j{Q_{ij} cdot x_i}  will be sent through this port. The cost\nintegrator will then complete the cost computation  by adding all\ncontributions, i.e., x^T cdot Q cdot x = sum_i{c_i}.\n\n[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nGeneric solver for constrained optimization problems defined by\nvariables, cost and constraints.\n\nThe problem should behave according to the OptimizationProblem’s\ninterface so that the Lava solver can be built correctly.\n\nA Lava OptimizationSolverProcess and a Lava OptimizationSolverModel will\nbe created from the problem specification. The dynamics of such process\nimplements the algorithms that search a solution to the problem and\nreports it to the user.\n\nCreate solver from problem spec and run until it has\neither minimized the cost to the target_cost or ran for a number of\ntime steps provided by the timeout parameter.\n\nconfig([SolverConfig](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverConfig)SolverConfig,optional) – Solver configuration used. Refers to SolverConfig documentation.\n\nreport– An object containing all the data generated by the execution.\n\n[SolverReport](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverReport)SolverReport\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nDataclass to store and validate OptimizationSolver configurations.\n\ntimeout(int) – Maximum number of iterations (timesteps) to be run. If set to -1\nthen the solver will run continuously in non-blocking mode until a\nsolution is found.\n\ntarget_cost(int,optional) – A cost value provided by the user as a target for the solution to be\nfound by the solver, when a solution with such cost is found and\nread, execution ends.\n\nbackend(BACKENDS,optional) – Specifies the backend where the main solver network will be\ndeployed.\n\nhyperparameters(`Union`Union[`Dict`Dict,`List`List[`Dict`Dict],`None`None]) – ty.Union[ty.Dict, ty.Dict[str, ty.Union[int, npt.ArrayLike]]],\noptional.\nA dictionary specifying values for steps_to_fire, noise_amplitude,\nstep_size and init_value. All but the last are integers, the initial\nvalue is an array-like of initial values for the variables defining\nthe problem.\n\nprobe_cost(bool) – A boolean flag to request cost tracking through time.\n\nprobe_time(bool) – A boolean flag to request time profiling, available only on “Loihi2”\nbackend.\n\nprobe_energy(bool) – A boolean flag to request time profiling, available only on “Loihi2”\nbackend.\n\nlog_level(int) – Select log verbosity (40: default, 20: verbose).\n\nfolded_compilation(bool) – A boolean flag to enable folded compilation, available only on\n“Loihi2” backend. Default value is False.\n\nalias of[CPU](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.CPU)`CPU`CPU\n\nBases:`object`object\n\nDataclass to store OptimizationSolver results.\n\nbest_cost(int) – Best cost found during the execution.\n\nbest_state(np.ndarray) – Candidate solution associated to the best cost.\n\nbest_timestep(int) – Execution timestep during which the best solution was found.\n\nsolver_config([SolverConfig](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverConfig)SolverConfig) – Solver configuraiton used. Refers to SolverConfig documentation.\n\nprofiler([Profiler](https://lava-nc.org/lava/lava.utils.html#lava.utils.profiler.Profiler)Profiler) – Profiler instance containing time, energy and activity measurements.\n\nSolve the given optimization problem using the passed configuration, and\nreturns the best candidate solution.\n\nproblem([OptimizationProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.OptimizationProblem)OptimizationProblem) – Optimization problem to be solved.\n\nconfig([SolverConfig](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverConfig)SolverConfig,optional) – Solver configuraiton used. Refers to SolverConfig documentation.\n\n`ndarray`ndarray\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nModel for the ContinuousConstraints process.\n\nalias of[ContinuousConstraintsProcess](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.ContinuousConstraintsProcess)`ContinuousConstraintsProcess`ContinuousConstraintsProcess\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nModel for the ContinuousVariables process.\n\nalias of[ContinuousVariablesProcess](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.ContinuousVariablesProcess)`ContinuousVariablesProcess`ContinuousVariablesProcess\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nModel for the CostConvergence process.\n\nThe model composes a CostIntegrator unit with incomming connections,\nin this way, downstream processes can be directly connected to the\nCostConvergence process.\n\nalias of[CostConvergenceChecker](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.CostConvergenceChecker)`CostConvergenceChecker`CostConvergenceChecker\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nModel for the DiscreteVariables process.\n\nThe model composes a population of Boltzmann units and\nconnects them via Dense processes as to represent integer or binary\nvariables.\n\nGiven the neuron_model, return the appropriate class for the\nneurons representing discrete variables.\n\nalias of[DiscreteVariablesProcess](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.DiscreteVariablesProcess)`DiscreteVariablesProcess`DiscreteVariablesProcess\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nProcessModel for an NEBM process.\n\nalias of[NEBMAbstract](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.NEBMAbstract)`NEBMAbstract`NEBMAbstract\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:`object`object\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nProcessModel for an NEBM process with Simulated Annealing.\n\nalias of[SimulatedAnnealingAbstract](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.SimulatedAnnealingAbstract)`SimulatedAnnealingAbstract`SimulatedAnnealingAbstract\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nProcessModel for an NEBM process with Simulated Annealing.\n\nalias of[SimulatedAnnealingLocalAbstract](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.SimulatedAnnealingLocalAbstract)`SimulatedAnnealingLocalAbstract`SimulatedAnnealingLocalAbstract\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSubProcessModel](https://lava-nc.org/lava/lava.magma.core.model.sub.html#lava.magma.core.model.sub.model.AbstractSubProcessModel)`AbstractSubProcessModel`AbstractSubProcessModel\n\nModel for the StochasticIntegrateAndFire process.\nThe process is just a wrapper over the QuboScif process.\n# Todo deprecate in favour of QuboScif.\n\nalias of[StochasticIntegrateAndFire](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.hierarchical_processes.StochasticIntegrateAndFire)`StochasticIntegrateAndFire`StochasticIntegrateAndFire\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nContinuousConstraintsProcess\n``````\n\n``````\nDiscreteConstraintsProcess\n``````\n\n``````\nMixedConstraintsProcess\n``````\n\n``````\nSparse\n``````\n\n``````\nAugmentedTermsProcess\n``````\n\n``````\nContinuousVariablesProcess\n``````\n\n``````\nDiscreteVariablesProcess\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nCPU\n``````\n\n``````\nbool\n``````\n\n``````\nUnion\n``````\n\n``````\nDict\n``````\n\n``````\nList\n``````\n\n``````\nint\n``````\n\n``````\nndarray\n``````\n\n``````\nOptimizationProblem\n``````\n\n``````\nProfiler\n``````\n\n``````\nSolverConfig\n``````\n\n``````\nAbstractSubProcessModel\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nCostConvergenceChecker\n``````\n\n``````\nNEBMAbstract\n``````\n\n``````\nSimulatedAnnealingAbstract\n``````\n\n``````\nSimulatedAnnealingLocalAbstract\n``````\n\n``````\nStochasticIntegrateAndFire\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.monitoring_processes.html",
    "title": "lava.lib.optimization.solvers.generic.monitoring_processes — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.monitoring_processes.solution_readout.html",
    "title": "lava.lib.optimization.solvers.generic.monitoring_processes.solution_readout — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nCPU model for the SolutionReadout process.\nThe process receives two types of messages, an updated cost and the\nstate of\nthe solver network representing the current candidate solution to an\nOptimizationProblem. Additionally, a target cost can be defined by the\nuser, once this cost is reached by the solver network, this process\nwill request the runtime service to pause execution.\n\n`ndarray`ndarray\n\n`ndarray`ndarray\n\nalias of[SolutionReadout](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.monitoring_processes.solution_readout.html#lava.lib.optimization.solvers.generic.monitoring_processes.solution_readout.process.SolutionReadout)`SolutionReadout`SolutionReadout\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess to readout solution from SNN and make it available on host.\n\nshape(The shapeofthe setofnodes, orprocess,which state will be read.) –\n\ntarget_cost(cost value at which,once attained by the network,) –\n\nexecution.(this process will stop) –\n\nname(Nameofthe Process. Default is 'Process_ID',where ID is an) –\n\nautomatically.(integer value that is determined) –\n\nlog_config(Configuration options for logging.) –\n\ntime_steps_per_algorithmic_step(the numberofiteration steps that a) –\n\nthe(single algorithmic step requires. This value is required to decode) –\n\nprocess.(variable values from the spk_histofa) –\n\nA message received on this ports signifies the process\nshould call read on its RefPort.\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nA reference port to a variable in another process which state\nwill be remotely accessed upon read request. Here, it reads the\ncurrent variables assignment by a solver to an optimization problem.\n\n[RefPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.RefPort)RefPort\n\nCost value at which, once attained by the network.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyInPort\n``````\n\n``````\nSolutionReadout\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nndarray\n``````\n\n``````\nint\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.read_gate.html",
    "title": "lava.lib.optimization.solvers.generic.read_gate — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nCPU model for the ReadGate process.\n\nThe model verifies if better payload (cost) has been notified by the\ndownstream processes, if so, it reads those processes state and sends out to\nthe upstream process the new payload (cost) and the network state.\n\nalias of[ReadGate](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.read_gate.html#lava.lib.optimization.solvers.generic.read_gate.process.ReadGate)`ReadGate`ReadGate\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nDecide whether to run post management phase.\n\nExecute post management phase.\n\nExecute spiking phase, integrate input, update dynamics and\nsend messages out.\n\nProduce CPU model for the ReadGate process.\n\nThe model verifies if better payload (cost) has been notified by the\ndownstream processes, if so, it reads those processes state and sends\nout to\nthe upstream process the new payload (cost) and the network state.\n\nDecide whether to run post management phase.\n\nExecute post management phase.\n\nExecute spiking phase, integrate input, update dynamics and\nsend messages out.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess that triggers solution readout when problem is solved.\n\nshape(The shapeofthe setofunits in the downstream process whose state) – will be read by ReadGate.\n\ntarget_cost(cost value at which,once attained by the network,) – this process will stop execution.\n\nname(Nameofthe Process. Default is 'Process_ID',where ID is an) – integer value that is determined automatically.\n\nlog_config(Configuration options for logging.) –\n\nInPorts–\n\n-------–\n\ncost_in_last_bytes([OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – Receives a better cost found by the CostIntegrator at the\nprevious timestep.\nMessages the last 3 byte of the new best cost.\nTotal cost = cost_in_first_byte << 24 + cost_in_last_bytes.\n\ncost_in_first_byte([OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – Receives a better cost found by the CostIntegrator at the\nprevious timestep.\nMessages the first byte of the new best cost.\n\nOutPorts–\n\n--------–\n\ncost_out(Forwards to an upstream process the better cost notified by the) – CostIntegrator.\n\nsolution_out(Forwards to an upstream process the better variable assignment) – found by the solver network.\n\nsend_pause_request(Notifies upstream process to request execution to pause.) –\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nint\n``````\n\n``````\nPyInPort\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nReadGate\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nndarray\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html",
    "title": "lava.lib.optimization.solvers.generic.scif — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract fixed point implementation of Stochastic Constraint\nIntegrate and Fire (SCIF) neuron for solving QUBO and CSP problems.\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[PyModelAbstractScifFixed](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.models.PyModelAbstractScifFixed)`PyModelAbstractScifFixed`PyModelAbstractScifFixed\n\nConcrete implementation of Stochastic Constraint Integrate and\nFire (SCIF) neuron for solving CSP problems.\n\nDerives fromPyModelAbstractScifFixedPyModelAbstractScifFixed.\n\nalias of[CspScif](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.process.CspScif)`CspScif`CspScif\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[PyModelAbstractScifFixed](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.models.PyModelAbstractScifFixed)`PyModelAbstractScifFixed`PyModelAbstractScifFixed\n\nConcrete implementation of Stochastic Constraint Integrate and\nFire (SCIF) neuron for solving QUBO problems.\n\nDerives fromPyModelAbstractScifFixedPyModelAbstractScifFixed.\n\nalias of[QuboScif](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.process.QuboScif)`QuboScif`QuboScif\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\n*Deprecated*Concrete implementation of Stochastic Constraint\nIntegrate and Fire (SCIF) neuron for solving QUBO problems.\n\nalias of[QuboScif](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.process.QuboScif)`QuboScif`QuboScif\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nAbstract Process for Stochastic Constraint Integrate-and-Fire\n(SCIF) neurons.\n\nBases:[AbstractScif](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.process.AbstractScif)`AbstractScif`AbstractScif\n\nStochastic Constraint Integrate-and-Fire neurons to solve CSPs.\n\nBases:[AbstractScif](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.scif.html#lava.lib.optimization.solvers.generic.scif.process.AbstractScif)`AbstractScif`AbstractScif\n\nStochastic Constraint Integrate-and-Fire neurons to solve QUBO\nproblems.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nndarray\n``````\n\n``````\nPyModelAbstractScifFixed\n``````\n\n``````\nCspScif\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nQuboScif\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nAbstractScif\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.html",
    "title": "lava.lib.optimization.solvers — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.qp.html",
    "title": "lava.lib.optimization.solvers.qp — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.utils.generators.html",
    "title": "lava.lib.optimization.utils.generators — Lava  documentation",
    "content": "Bases:`object`object\n\nUtility class to instantiate Maximum Independent Set problems and\nconvert them to a QUBO formulation.\n\nReturns the adjacency matrix of the graph.\n\nFind and return the maximum independent set of a graph based on its\nadjacency matrix.\n\n[*](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.utils.generators.html#id1)*Please note that this function addresses the maximum and not just\nthe maximal independent set. A maximal independent set is an\nindependent set that is not a subset of any other independent set.\nThe largest of these sets is the maximum independent set, which is\ndetermined by the present function.*Uses Networkx to solve the\nequivalent maximum clique problem.\n\nGet a graph whose maximum clique corresponds to the maximum independent\nset of that defined by the input connectivity matrix.\nThe  maximum independent set for the graph G={V,E} is equivalent to the\nmaximum-clique of the graph H={V,E_c}, where E_c is the complement of E.\n\nsolution– Vector of length equal to the number of vertices in the graph.\nThe ith entry of the vector determines if the ith vertex is a\nmember of the MIS.\n\nArray[binary]\n\nInstantiate a new MIS problem, based on a random uniform graph sampled\nwith the given paramters.\n\nnum_vertices(int) – Number of vertices of the random graph.\n\ndensity(float) – Density of the random adjacency matrix.\n\nseed(int) – Seed for random graph generation.\n\n[MISProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.utils.generators.html#lava.lib.optimization.utils.generators.mis.MISProblem)`MISProblem`MISProblem\n\nInstantiate a new MIS problem, based on a random Watts-Strogatz graph\nsampled with the given parameters.\n\nnum_vertices(int) – Number of vertices of the random graph.\n\nnum_neighbors(int) – Each node is joined with its k nearest neighbors.\n\nconnection_prob(float) – Connection probability between different vertices.\n\nseed(int) – Seed for random graph generation.\n\n[MISProblem](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.utils.generators.html#lava.lib.optimization.utils.generators.mis.MISProblem)`MISProblem`MISProblem\n\nCreates a QUBO whose solution corresponds to the maximum independent\nset (MIS) of the graph defined by the input adjacency matrix.\n\n[QUBO](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.problems.html#lava.lib.optimization.problems.problems.QUBO)`QUBO`QUBO\n\nmin x^T * Q * x ,\n\nx_i = 1 if vertex i is part of the MIS\nx_i = 0 if vertex i is not part of the MIS,\n\nQ_ii = w_diag\nQ_ij = w_off (for i!=j) .\n\nReturns the complement graph in networkx format.\n\n`Graph`Graph\n\nReturns the adjacency matrix of the complement graph.\n\n`ndarray`ndarray\n\nReturns the graph in networkx format.\n\n`Graph`Graph\n\nCreates a QUBO whose solution corresponds to the maximum independent\nset (MIS) of the graph defined by the input adjacency matrix.\n\n`ndarray`ndarray\n\nmin x^T * Q * x ,\n\nx_i = 1 if vertex i is part of the MIS\nx_i = 0 if vertex i is not part of the MIS,\n\nQ_ii = w_diag\nQ_ij = w_off (for i!=j) .\n\nReturns the number of edges in the graph.\n\nReturns the number of vertices in the graph.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nMISProblem\n``````\n\n``````\nQUBO\n``````"
  },
  {
    "url": "https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.utils.html",
    "title": "lava.lib.optimization.utils — Lava  documentation",
    "content": "Bases:`object`object\n\nUtility class to optimize hyper-parameters by random search.\n\nReturns data on all hyper-parameters evaluations as a structured\nnumpy array.\n\nPerform random search to optimize solver hyper-parameters based on a\nfitness function.\n\nsolver([OptimizationSolver](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.OptimizationSolver)OptimizationSolver) – Optimization solver to use for solving the problem.\n\nfitness_fn(ty.Callable[[[SolverReport](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverReport)SolverReport],float]) – Fitness function to evaluate a given set of hyper-parameters,\ntaking as input a SolverReport instance (refers to its documentation\nfor the available parameters). This is the function that is\nmaximized by the SolverTuner.\n\nfitness_target(float,optional) – Fitness target to reach. If this is not passed, the full grid is\nexplored before stopping search.\n\nconfig([SolverConfig](https://lava-nc.org/lava-lib-optimization/lava.lib.optimization.solvers.generic.html#lava.lib.optimization.solvers.generic.solver.SolverConfig)SolverConfig,optional) – Solver configuration to be used.\n\nbest_hyperparams(ty.Dict) – Dictionary containing the hyper-parameters with the highest fitness.success(bool) – Flag signaling if the fitness_target has been reached. If no\nfitness_target is passed, the flag is True.\n\nbest_hyperparams(ty.Dict) – Dictionary containing the hyper-parameters with the highest fitness.\n\nsuccess(bool) – Flag signaling if the fitness_target has been reached. If no\nfitness_target is passed, the flag is True.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial01_installing_lava.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial02_process_models.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial02_processes.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial03_process_models.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial04_execution.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial05_connect_processes.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/in_depth/tutorial09_custom_learning_rules.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/lava.html",
    "title": "Lava — Lava  documentation",
    "content": "This page gives an overview over all public Lava modules, classes and functions.\n\nLava is divided into the sub-packages:\n\n[The process library](https://lava-nc.org/lava/lava.proc.html#lava-process-library)The process librarycontaining commonly used[Processes](https://lava-nc.org/lava/lava.magma.core.process.html#lava-magma-core-process-process)Processesand[ProcessModels](https://lava-nc.org/lava/lava.magma.core.model.html#lava-magma-core-model-model)ProcessModels.\n\n[Magma](https://lava-nc.org/lava/lava.magma.html#magma)Magma, containing the main components of Lava:\n\n[Magma core](https://lava-nc.org/lava/lava.magma.core.html#module-lava.magma.core)Magma corebase classes, definitions and functionality\n\n[Magma compiler](https://lava-nc.org/lava/lava.magma.compiler.html#module-lava.magma.compiler)Magma compilercompiling and building the network and communication channels\n\n[Magma runtime](https://lava-nc.org/lava/lava.magma.runtime.html#module-lava.magma.runtime)Magma runtimeproviding a frontend for execution and control\n\nLava’s fundamental concepts and key components are described in[Lava Architecture](https://lava-nc.org/lava_architecture_overview.html#lava-architecture)Lava Architecture.\n\nExplanatory tutorials and example code can be found in thein-depth tutorialsand in theEnd-to-end Tutorial notebooks.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.compiler.builders.html",
    "title": "lava.magma.compiler.builders — Lava  documentation",
    "content": "Bases:[AbstractChannelBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractChannelBuilder)`AbstractChannelBuilder`AbstractChannelBuilder,[WatchdogEnabledMixin](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.channel_builder.WatchdogEnabledMixin)`WatchdogEnabledMixin`WatchdogEnabledMixin\n\nA ChannelBuilder assuming Python multi-processing is used as messaging\nand multi processing backbone.\n\nGiven the message passing framework builds a channel\n\nmessaging_infrastructure([MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)MessageInfrastructureInterface) –\n\nwatchdog_manager(WatchdogManager) –\n\nChannel\n\n[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)Channel\n\nException– Can’t build channel of type specified\n\nBases:[AbstractChannelBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractChannelBuilder)`AbstractChannelBuilder`AbstractChannelBuilder\n\nA ChannelBuilder for CNc and NcC Channels with NxBoard as\nthe messaging\ninfrastructure.\n\nGiven the message passing framework builds a channel\n\nmessaging_infrastructure([MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)MessageInfrastructureInterface) –\n\nChannel\n\n[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)Channel\n\nException– Can’t build channel of type specified\n\nBases:[ChannelBuilderNx](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.channel_builder.ChannelBuilderNx)`ChannelBuilderNx`ChannelBuilderNx\n\nA ChannelBuilder for PyNc and NcPy Channels with NxBoard as the messaging\ninfrastructure.\n\nGiven the message passing framework builds a channel\n\nmessaging_infrastructure([MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)MessageInfrastructureInterface) –\n\nChannel\n\n[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)Channel\n\nException– Can’t build channel of type specified\n\nBases:[AbstractChannelBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractChannelBuilder)`AbstractChannelBuilder`AbstractChannelBuilder,[WatchdogEnabledMixin](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.channel_builder.WatchdogEnabledMixin)`WatchdogEnabledMixin`WatchdogEnabledMixin\n\nA RuntimeChannelBuilder assuming Python multi-processing is\nused as messaging and multi processing backbone.\n\nGiven the message passing framework builds a channel\n\nmessaging_infrastructure([MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)MessageInfrastructureInterface) –\n\nwatchdog_manager(WatchdogManager) –\n\nPyPyChannel\n\n[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)Channel\n\nException– Can’t build channel of type specified\n\nBases:[AbstractChannelBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractChannelBuilder)`AbstractChannelBuilder`AbstractChannelBuilder,[WatchdogEnabledMixin](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.channel_builder.WatchdogEnabledMixin)`WatchdogEnabledMixin`WatchdogEnabledMixin\n\nA RuntimeServiceChannelBuilder assuming Python multi-processing is used\nas messaging and multi processing backbone.\n\nGiven the message passing framework builds a channel\n\nmessaging_infrastructure([MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)MessageInfrastructureInterface) –\n\nwatchdog_manager(WatchdogManager) –\n\nPyPyChannel\n\n[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)Channel\n\nException– Can’t build channel of type specified\n\nBases:`object`object\n\n`Tuple`Tuple[`Watchdog`Watchdog,`Watchdog`Watchdog,`Watchdog`Watchdog,`Watchdog`Watchdog]\n\n`Watchdog`Watchdog\n\nBases:`ABC`ABC\n\nBuilders interface for building processes in a given backend.\n\nBuild the actual process.\n\nBases:`ABC`ABC\n\nAn AbstractChannelBuilder is the base type for\nchannel builders which build communication channels\nbetween services and processes\n\nBases:[AbstractBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractBuilder)`AbstractBuilder`AbstractBuilder\n\nAn AbstractProcessBuilder is the base type for process builders.\n\nProcess builders instantiate and initialize a ProcessModel.\n\nproc_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – ProcessModel class of the process to build.\n\nmodel_id(int) – model_id represents the ProcessModel ID to build.\n\ncompiler to create a ProcessBuilder during the compilation of\nProcessModels.\n\nvariables(ty.List[[VarInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.VarInitializer)VarInitializer]) –\n\nBases:[Resource](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.Resource)`Resource`Resource\n\nSignifies a compiled resource held by the builder. Must be\nserializable if the builder is being serialized after compilation\nbefore mapping\n\nReturn the logical address of this compiled resource.\n\nBases:[Resource](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.Resource)`Resource`Resource\n\nSignifies a physical resource held by the builder.\nMust be serializable.\n\nReturn the physical address of this mapped resource.\n\nBases:`ABC`ABC\n\nGiven hw, write this compiled resource\n\nBases:`ABC`ABC\n\nBases:[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder\n\nA PyProcessBuilder instantiates and initializes a PyProcessModel.\n\nThe compiler creates a PyProcessBuilder for each PyProcessModel. In turn,\nthe runtime, loads a PyProcessBuilder onto a compute node where it builds\nthe PyProcessModel and its associated ports.\n\nIn order to build the PyProcessModel, the builder inspects all LavaType\nclass variables of a PyProcessModel, creates the corresponding data type\nwith the specified properties, the shape and the initial value provided by\nthe Lava Var. In addition, the builder creates the required PyPort\ninstances. Finally, the builder assigns both port and variable\nimplementations to the PyProcModel.\n\nOnce the PyProcessModel is built, it is the RuntimeService’s job to\nconnect channels to ports and start the process.\n\nNote: For unit testing it should be possible to build processes locally\ninstead of on a remote node. For pure atomic unit testing a ProcessModel\nlocally, PyInPorts and PyOutPorts must be fed manually with data.\n\nAppends a mapping from a PyPort ID to a CSP port. This is used\nto associate a CSP port in a PyPort with transformation functions\nthat implement the behavior of virtual ports.\n\npy_port_id(str) – ID of the PyPort that contains the CSP on the other side of the\nchannel of ‘csp_port’\n\ncsp_port([AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort) – a CSP port\n\nBuilds a PyProcModel at runtime within Runtime.\n\nThe Compiler initializes the PyProcBuilder with the ProcModel,\nVarInitializers and PortInitializers.\nThe Runtime builds the channels and CSP ports between all ports,\nassigns them to builder.\n\nAt deployment to a node, the Builder.build(..) gets executed\nresulting in the following:\n\nProcModel gets instantiated\n\nVars are initialized and assigned to ProcModel\n\nPyPorts are initialized (with CSP ports) and assigned to ProcModel\n\n[AbstractPyProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.AbstractPyProcessModel)AbstractPyProcessModel\n\nNotImplementedError–\n\nChecks that Vars and PyPorts assigned from Process have a\ncorresponding LavaPyType.\n\nAssertionError– No LavaPyType found in ProcModel\n\nChecks correctness of LavaPyTypes.\n\nAny Py{In/Out/Ref}Ports must be strict sub-types of Py{In/Out/Ref}Ports.\n\nAppends the given list of CspPorts to the ProcessModel. Used by the\nruntime to configure csp ports during initialization (_build_channels).\n\ncsp_ports(ty.List[[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort]) –\n\nAssertionError– PyProcessModel has no port of that name\n\ncompiler to create a ProcessBuilder during the compilation of\nProcessModels.\n\npy_ports(ty.List[[PortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.PortInitializer)PortInitializer]) –\n\ncheck(bool,optional) – , by default True\n\ncompiler to create a ProcessBuilder during the compilation of\nProcessModels.\n\nref_ports(ty.List[[PortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.PortInitializer)PortInitializer]) –\n\nSet RS CSP Ports\n\ncsp_ports(ty.List[[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort]) –\n\ncompiler to create a ProcessBuilder during the compilation of\nProcessModels.\n\nvar_ports(ty.List[[VarPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.VarPortInitializer)VarPortInitializer]) –\n\nBases:`object`object\n\nBases:`object`object\n\nRuntimeService builders instantiate and initialize a RuntimeService.\n\nrs_class(AbstractRuntimeService classofthe runtime service to build.) –\n\nsync_protocol(AbstractSyncProtocol Synchronizer class that) – implements a protocol in a domain.\n\nBuild the runtime service\n\n[AbstractRuntimeService](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.html#lava.magma.runtime.runtime_services.interfaces.AbstractRuntimeService)`AbstractRuntimeService`AbstractRuntimeService\n\nA concreate instance of AbstractRuntimeService[PyRuntimeService or NxSdkRuntimeService]\n\nA concreate instance of AbstractRuntimeService\n\n[PyRuntimeService or NxSdkRuntimeService]\n\nReturn runtime service id.\n\nSet CSP Ports\n\ncsp_ports(ty.List[[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort]) –\n\nSet CSP Process Ports\n\ncsp_ports(ty.List[[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort]) –\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractChannelBuilder\n``````\n\n``````\nWatchdogEnabledMixin\n``````\n\n``````\nChannelType\n``````\n\n``````\nPortInitializer\n``````\n\n``````\nChannelBuilderNx\n``````\n\n``````\nUnion\n``````\n\n``````\nRuntimeServiceBuilder\n``````\n\n``````\nType\n``````\n\n``````\nAbstractProcessModel\n``````\n\n``````\nAbstractBuilder\n``````\n\n``````\nResource\n``````\n\n``````\nAbstractProcessBuilder\n``````\n\n``````\nAbstractRuntimeService\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.compiler.channels.html",
    "title": "lava.magma.compiler.channels — Lava  documentation",
    "content": "Bases:`ABC`ABC\n\nAbstract base class for CSP channel.\n\n`bool`bool\n\nBases:[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)`AbstractCspPort`AbstractCspPort\n\n`ndarray`ndarray\n\nBases:[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)`AbstractCspPort`AbstractCspPort\n\nBases:`ABC`ABC\n\nBases:`IntEnum`IntEnum\n\nType of a channel given the two process models\n\nBases:[AbstractCspRecvPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspRecvPort)`AbstractCspRecvPort`AbstractCspRecvPort\n\nCspRecvPort is a low level recv port implementation based on CSP\nsemantics. It can be understood as the output port of a CSP channel.\n\nReturn the next token on the channel without acknowledging it. Blocks\nif there is no data on the channel.\n\nReturns True if a ‘recv’ call will not block, and False otherwise.\nDoes not block.\n\nReceive from the channel. Blocks if there is no data on the channel.\n\nStarts the port to listen on a thread\n\nBases:`Queue`Queue\n\nUnderlying queue which backs the CspRecvPort\n\nImplementation from the standard library augmented with ‘peek’ to\noptionally return the head element without removing it.\n\nBases:`object`object\n\nUtility class to allow waiting for multiple channels to become ready\n\nWait for any channel to become ready, then execute the corresponding\ncallable and return the result.\n\n`None`None\n\nBases:[AbstractCspSendPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspSendPort)`AbstractCspSendPort`AbstractCspSendPort\n\nCspSendPort is a low level send port implementation based on CSP\nsemantics. It can be understood as the input port of a CSP channel.\n\nReturns True if a ‘send’ call will not block, and False otherwise.\nDoes not block.\n\nSend data on the channel. May block if the channel is already full.\n\nStarts the port to listen on a thread\n\nBases:`object`object\n\nBases:[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)`Channel`Channel\n\nHelper class to create the set of send and recv port and encapsulate\nthem inside a common structure. We call this a PyPyChannel\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractCspPort\n``````\n\n``````\nAbstractCspRecvPort\n``````\n\n``````\nAbstractCspSendPort\n``````\n\n``````\ndtype\n``````\n\n``````\nint\n``````\n\n``````\nndarray\n``````\n\n``````\nChannel\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.compiler.html",
    "title": "lava.magma.compiler — Lava  documentation",
    "content": "Bases:`dict`dict\n\nThe ChannelMap is used by the SubCompilers during compilation to\ncommunicate how they are planning to partition Processes onto their\navailable resources.\n\nInitializes a ChannelMap from a list of process ProcGroups\nextracting the ports from every process group.\n\nFor every port pair in the process groups a PortPair will be created and\nset as a key on the ChannelMap object which inherits from standard dict,\nthe values are initialized with default Payload having multiplicity\nof 1.\n\nEvery subcompiler will then update the multiplicity according to its\npartitioning process. Payload also include fields for\nparameterization of convolutional processes.\n\nproc_groups(a listofProcessGroups each multiple processes.) –\n\n[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap\n\nchannel_map(A ChannelMap object initialized with the PortPairs found in)the list of process groups given as input.\n\nchannel_map(A ChannelMap object initialized with the PortPairs found in)\n\nthe list of process groups given as input.\n\n`bool`bool\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder\n\nBases:`object`object\n\nLava processes Compiler, called from any process in a process network.\n\nCreates an Executable for the network of processes connected to the\nprocess passed to the compile method.\n\nCompiles all Processes connected to the given Process and the\nchannels defined by their connectivity.\n\nReturns an Executable that contains all Builder instances required to\nexecute the Lava Process network on heterogeneous hardware.\n\nprocess([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – Process from which all connected Processes in the Lava\nProcess network are searched.\n\nrun_cfg([RunConfig](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.RunConfig)RunConfig) – RunConfig that determines which ProcessModels will be selected\nfor Processes.\n\nexecutable– An instance of an Executable that contains all required Builders.\n\n[Executable](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.executable.Executable)Executable\n\nBases:`object`object\n\nBases:[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`ABC`ABC\n\nAbstract class for generating and holding directed graphs needed to\ncreate ProcGroups for compilation.\n\nConcrete subclass must implementget_proc_groups()get_proc_groups()method that outputs a\nlist of ProcGroups, wheretype(ProcGroup) = List[AbstractProcess]type(ProcGroup) = List[AbstractProcess].\n\nDownstream compiler iterates over the list of ProcGroups and invokes\nappropriate sub-compilers depending on the ProcessModel type.\n\n`List`List[`List`List[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess]]\n\nBases:`DiGraph`DiGraph\n\nBase class for directed graphs in the compiler.\n\nThe nodes of instances of this class can be any hashable objects,\nthey need not be of type AbstractProcess. Inherits from NetworkX.DiGraph\nclass.\n\nAnnotate the graph’s nodes according to their degree.\n\nMore specifically, the annotations are based on the degree deficit\nof a node, defined as (out degree - in degree).\n\nSee also\n\n[NodeAnnotation](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.NodeAnnotation)`NodeAnnotation`NodeAnnotation\n\nFind simple cycles and collapse them on a single node iteratively,\nuntil no simple cycles can be found or the entire graph reduces to a\nsingle simple cycle.\n\nout_graph– A copy ofselfselfwith node-replacement surgery complete.\n\n[DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)DiGraphBase\n\nReplace any connected subgraph of the DiGraphBase object with a\nsingle node, while preserving connectivity.\n\nThe new node is a DiGraphBase object, which preserves the internal\nconnectivity of the subgraph.\n\nsubgraph([DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)DiGraphBase) – Subgraph which needs to be collapsed into a single node. It needs\nto be connected..\n\nout_graph– A copy ofselfselfwith node-replacement surgery complete.\n\n[DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)DiGraphBase\n\nCheck if the input DiGraphBase is a DAG by recursive leaf pruning.\n\ngraph([DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)DiGraphBase) – This parameter is only needed for recursion. It holds the residual\ngraph after pruning leaf nodes of the graph from previous\nrecursive step. At the first iterations,graph = Nonegraph = None.\n\n`Tuple`Tuple[`bool`bool,[DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)`DiGraphBase`DiGraphBase]\n\nDAGness of the graph(Bool) – True if the graph is a DAG, else Falsegraph(DiGraphBase) – The residual subgraph that remains after the input graph runs\nout of leaf nodes after all recursions are over.\n\nDAGness of the graph(Bool) – True if the graph is a DAG, else False\n\ngraph(DiGraphBase) – The residual subgraph that remains after the input graph runs\nout of leaf nodes after all recursions are over.\n\nBases:`Enum`Enum\n\nAnnotations for the nodes of directed graphs created for compilation.\nThe node attribute called “degdef” (short for “degree deficit”) of a\nnode takes one of the following seven values:\n\nPUREIN, a pure input Process: in degree of the node = 0\n\nPUREOUT a pure output Process: out degree of the node = 0\n\nISOLATED a Process node with in degree = out degree = 0\n\nINLIKE  an in-like Process: (out degree) - (in degree) < 0\n\nOUTLIKE an out-like Process: (out degree) - (in degree) > 0\n\nNEUTRAL a Process node with in degree = out degree\n\nINVALID a Process node that does not fit any of the above (unlikely)\n\nBases:[DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)`DiGraphBase`DiGraphBase\n\nDirected graph data structure for the compiler which has some nodes\nthat are Lava Processes.\n\nIf a list of Lava Processes is passed to the constructor,\nusingproc_graphproc_graphkeyword argument, a DiGraph will be created with the\nProcesses as nodes and their interconnectivity as edges.\n\nInherits from DiGraphBase, which in turn inherits from NetworkX.DiGraph.\nIf the constructor is passed*args*argsand**kwargs**kwargsthat create parts of a\nNetworkX.DiGraph, then the graph with Process nodes is created in\naddition to (and disjoint from) the existing graph elements.\n\nConvert ProcDiGraph to a DiGraph whose nodes are just the process\nids of Processes from the original ProcDiGraph.\n\nThis utility method is useful to compare two ProcDiGraphs without\nactually using entire AbstractProcess objects as nodes.\n\nprocid_graph– A graph with nodes = process ids of Processes forming nodes of\nProcDiGraph. Each node retains the attributes of the original nodes.\n\n[DiGraphBase](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.DiGraphBase)DiGraphBase\n\nBases:[AbstractProcGroupDiGraphs](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.AbstractProcGroupDiGraphs)`AbstractProcGroupDiGraphs`AbstractProcGroupDiGraphs\n\nConcrete subclass ofAbstractProcGroupDiGraphsAbstractProcGroupDiGraphsthat generates\nand holds various directed graphs needed to generate ProcGroups needed\nfor compilation.\n\nAll sub-compilers need the following tasks to be complete before they are\ninstantiated:\n\nUsing the “base” Process, on which proc.run(...) was\ncalled, a list of all processes connected to the base process is\ngenerated by traversing all InPort and OutPort connectivity.\n\nUsing the list in step 1, a directed graph is generated with\nProcesses as nodes. As the exact behaviour of Processes is not\nknown (i.e., ProcessModels), the directed graph is ‘raw’,\nwithout resolving any hierarchical Processes.\n\nUsing a RunConfig along with the list of Processes in step 1,\na dictionary mapping Process to ProcessModel is created. Then\nProcessModels are assigned as attributes to the Process instances.\n\nUsing the dict mapping Processes to ProcessModels, create a\ndirected graph with Processes as nodes. By this point,\nhierarchical Processes are known and resolved using their\nSubProcessModels (therefore the name). As these Processes do\nnot feature in the dict mapping, they do not constitute a node in\nthe graph.\n\nUsing the ResolvedProcDiGraph generated in step 4, find all\nProcess nodes with the same ProcessModel type, which are\nneighbours of each other (in the sense of Graph Theory), and\ncollapse/condense them into a single node, while preserving the\nconnectivity.\n\nUsing the IsoModelCondensedDiGraph from step 5, find all node\nthat are not connected via feed-forward topology (i.e., all nodes\nthat are a part of some cyclic structure) and collapse/condense\nthem into a single node, while preserving connectivity. Each node\nof ProcGroupDiGraph thus generated is a ProcGroup.\n\nThe ProcGroupDiGraph, by construction, is a single simple cycle or a\nDAG. In the first case, the simple cycle is broken at an arbitrary\nedge and List[ProcGroup] is generated from the list of its nodes.\nIn the case that ProcGroupDiGraph is a DAG, it is ordered using\ntopological sorting and List[ProcGroup] is produced by reversing the\nsorted order of nodes.\n\nThe Process on which Process.run(...) is called.\n\nCreate a list of process groups sorted in compilation order.\n\nThe compilation order is obtained by reversing the topologically sorted\norder on compile_group_graph. Nodes of compile_group_graph can be\ngraphs themselves. These are flattened into list of processes\ncomprising a ProcGroup.\n\nThis is the interface to the downstream parts of the compiler.\n\nproc_groups– A list of ProcGroup. The object ProcGroup is itself a short-hand\nfor List[AbstractProcess]. Each ProcGroup is processed as a\nsingle entity by the downstream parts of the compiler.\n\nList[ProcGroup]\n\nDirected graph derived fromProcGroupDiGraphs.resolved_proc_graphProcGroupDiGraphs.resolved_proc_graph,\nsuch that allconnectedProcesses with same type of ProcessModel\nare collapsed/condensed onto a single node in the graph.\n\nNumber of leaf Processes, after ProcessModel discovery and\nSubProcessModel resolution/expansion.\n\nThis is the number of nodes inresolved_proc_digraphresolved_proc_digraph\n\nSee also\n\n[resolved_proc_digraph](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.ProcGroupDiGraphs.resolved_proc_digraph)`resolved_proc_digraph`resolved_proc_digraph\n\nNumber of Processes before ProcessModel discovery.\n\nThis is the number of nodes inraw_proc_digraphraw_proc_digraph.\n\nSee also\n\n[raw_proc_digraph](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.compiler_graphs.ProcGroupDiGraphs.raw_proc_digraph)`raw_proc_digraph`raw_proc_digraph\n\nDirected graph derived fromProcGroupDiGraphs.isomodel_condensed_graphProcGroupDiGraphs.isomodel_condensed_graph, such that all nodes\ninvolved in any kind of non-linear/feedback/cyclical topology are\ncollapsed/condensed onto a single node.\n\nA proc_group_digraph - by construction - is a single simple cycle or\na DAG.\n\nDirected graph of all connected Processes _before_ ProcessModel\ndiscovery.\n\nThis graph is generated using Process-level connectivity before any\nProcessModels are discovered. Any sub-Processes inside a\nSubProcessModel of a hierarchical Process will not show up in this\ngraph.\n\nDirected graph of all connected Processes _after_ ProcessModel\ndiscovery.\n\nThis graph is generated after discovering all ProcessModels and\nresolving/building SubProcessModels. Therefore, no hierarchical\nProcesses show up in this graph. They are substituted by the\nsub-Processes inside their SubProcessModels.\n\nBases:`Enum`Enum\n\nEnumeration of different types of ProcessModels: Py, C, Nc, etc.\n\nAnnotate folded views and propagate them recursively\n\nFind all processes that are connected toprocproc.\n\nProcesses are connected via different kinds of Ports to other\nprocesses. This method starts at the givenprocprocand traverses the\ngraph along connections to find all connected processes.\n\nDuring compilation, this method is calledbeforediscovering\nProcessModels implementing Processes.\n\nproc([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – Base process starting which the discovery of all connected Processes\nbegins.\n\nseen_procs(List[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess]) – A list of Processes visited during traversal of connected Processes.\nUsed for making the method recursive. This parameter is set toNoneNoneat the time of the first call.\n\nseen_procs– A list of all discovered Processes.\n\nList[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess]\n\nSimpler way to flatten nested lists.\n\n`List`List\n\nRecursively flatten a list of lists.\n\nll(list) – Any list of lists (of any depth)\n\nll– Flattened list\n\nlist\n\nNotes\n\nTaken from:[https://stackabuse.com/python-how-to-flatten-list-of-lists/](https://stackabuse.com/python-how-to-flatten-list-of-lists/)https://stackabuse.com/python-how-to-flatten-list-of-lists/\n\nBases:[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder\n\nBases:[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder\n\nGiven a dictionary of process to builders, returns a tuple of\nprocess to builder dictionaries for Py, C and Nc processes.\n\nBases:`Exception`Exception\n\nBases:`Exception`Exception\n\nBases:`object`object\n\nProduced by compiler and contains everything the Runtime needs to run\nprocess.\n\nThis includes all ProcessModels of sub processes, RuntimeService\nprocesses for the various nodes in the system and channel configurations.\nAn Executable should be serializable so it can be saved and loaded at a\nlater point.\n\nBases:`ABC`ABC\n\nInterface to make entity mappable.\n\nList of LogicalAddresses.\n\naddr(ListofPhysicalAddresses to be assigned to the mappable.) –\n\nBases:`object`object\n\nBases:`object`object\n\nAssigns virtual addresses to different processes, mappable by mapping\nlogical addresses to virtual addresses.\n\nThis function gets called from the Compiler class once the partition\nis done. It maps logical addresses to virtual addresses.\n\nexecutable(Compiled Executable) –\n\n`None`None\n\nBases:`object`object\n\nBases:`object`object\n\nA Node represents a physical compute node on which one or more\nprocesses execute.\n\nNodes are of a specific type and hold references to all processes mapped\nto a node.\n\nBases:`UserList`UserList\n\nA NodeConfig is a collection of Nodes. Nodes represent a physical\ncompute node on which one or more processes execute.\n\nA NodeCfg has a list of all ‘nodes’ and a ‘node_map’ that maps each\nprocess to its respective node.\n\nAppends a new node to the NodeConfig.\n\nReturns list of all nodes of the NodeConfig.\n\nBases:[LoihiIOPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiIOPortInitializer)`LoihiIOPortInitializer`LoihiIOPortInitializer\n\nBases:`IntEnum`IntEnum\n\nEncoding type of the connected port - Required in case of C_PY\n\nBases:`IntEnum`IntEnum\n\nTypes of port connectivity; direction does not matter\n\nBases:[LoihiPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiPortInitializer)`LoihiPortInitializer`LoihiPortInitializer\n\nPort Initializer for a I/O Port for C/NC Models\n\nBases:[LoihiIOPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiIOPortInitializer)`LoihiIOPortInitializer`LoihiIOPortInitializer\n\nPort Initializer for a InPort for C/NC Models\n\nBases:[LoihiIOPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiIOPortInitializer)`LoihiIOPortInitializer`LoihiIOPortInitializer\n\nPort Initializer for a OutPort for C/NC Models\n\nBases:[PortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.PortInitializer)`PortInitializer`PortInitializer,[Mappable](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.mappable_interface.Mappable)`Mappable`Mappable\n\nThis address needs to be defined based on var model\n\nReturns logical address of the port initializer.\n\nSets physical address of the port initializer\n:type addrs:`List`List[[NcVirtualAddress](https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html#lava.magma.compiler.subcompilers.address.NcVirtualAddress)`NcVirtualAddress`NcVirtualAddress]\n:param addrs:\n:type addrs: List of address\n\nBases:[LoihiCInPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiCInPortInitializer)`LoihiCInPortInitializer`LoihiCInPortInitializer\n\nBases:[VarInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.VarInitializer)`VarInitializer`VarInitializer\n\nBases:[LoihiPortInitializer](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.utils.LoihiPortInitializer)`LoihiPortInitializer`LoihiPortInitializer\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`ABC`ABC\n\nBases:`object`object\n\nEncoding for axon field\n\nBases:`object`object\n\nEncodes ptr, len, base\n\nBases:[LoihiVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiVarModel)`LoihiVarModel`LoihiVarModel\n\nBases:`object`object\n\nEncoding for chip field\n\nBases:[AbstractVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.AbstractVarModel)`AbstractVarModel`AbstractVarModel,[Mappable](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.mappable_interface.Mappable)`Mappable`Mappable\n\nReturns logical address of the port initializer.\n\nSets physical address of the port initializer\n:type addrs:`List`List[[NcVirtualAddress](https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html#lava.magma.compiler.subcompilers.address.NcVirtualAddress)`NcVirtualAddress`NcVirtualAddress]\n:param addrs:\n:type addrs: List of address\n\nBases:[LoihiNeuronVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiNeuronVarModel)`LoihiNeuronVarModel`LoihiNeuronVarModel\n\nBases:`object`object\n\nEncodes a core xyp\n\nBases:`object`object\n\nBases:`object`object\n\nBases:[LoihiAddress](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiAddress)`LoihiAddress`LoihiAddress\n\nBases:[LoihiAddress](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiAddress)`LoihiAddress`LoihiAddress\n\nBases:[LoihiVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiVarModel)`LoihiVarModel`LoihiVarModel\n\nBases:[LoihiAddress](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiAddress)`LoihiAddress`LoihiAddress\n\nBases:[LoihiVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiVarModel)`LoihiVarModel`LoihiVarModel\n\nBases:[AbstractVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.AbstractVarModel)`AbstractVarModel`AbstractVarModel,[Mappable](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.mappable_interface.Mappable)`Mappable`Mappable\n\nReturns logical address of the port initializer.\n\nSets physical address of the port initializer\n:type addrs:`List`List[[NcVirtualAddress](https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html#lava.magma.compiler.subcompilers.address.NcVirtualAddress)`NcVirtualAddress`NcVirtualAddress]\n:param addrs:\n:type addrs: List of address\n\nBases:[NcSpikeIOVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.NcSpikeIOVarModel)`NcSpikeIOVarModel`NcSpikeIOVarModel\n\nBases:[NcVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.NcVarModel)`NcVarModel`NcVarModel\n\nBases:[LoihiVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.LoihiVarModel)`LoihiVarModel`LoihiVarModel\n\nBases:[AbstractVarModel](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.var_model.AbstractVarModel)`AbstractVarModel`AbstractVarModel\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nUsed by SpikeBlock to determine when to inject spikes\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nChannelMap\n``````\n\n``````\nPortInitializer\n``````\n\n``````\nint\n``````\n\n``````\nOptional\n``````\n\n``````\nTuple\n``````\n\n``````\n...\n``````\n\n``````\nAbstractDstPort\n``````\n\n``````\nAbstractSrcPort\n``````\n\n``````\nAbstractProcessBuilder\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nNodeAnnotation\n``````\n\n``````\nDiGraphBase\n``````\n\n``````\nAbstractProcGroupDiGraphs\n``````\n\n``````\nresolved_proc_digraph\n``````\n\n``````\nraw_proc_digraph\n``````\n\n``````\nLoihiIOPortInitializer\n``````\n\n``````\nLoihiPortInitializer\n``````\n\n``````\nLoihiConnectedPortEncodingType\n``````\n\n``````\nLoihiConnectedPortType\n``````\n\n``````\nSpikeType\n``````\n\n``````\nMappable\n``````\n\n``````\nNcVirtualAddress\n``````\n\n``````\nLoihiVarModel\n``````\n\n``````\nLoihiCInPortInitializer\n``````\n\n``````\nConnectionConfig\n``````\n\n``````\nVarInitializer\n``````\n\n``````\ntype\n``````\n\n``````\nstr\n``````\n\n``````\nDict\n``````\n\n``````\nList\n``````\n\n``````\npartial\n``````\n\n``````\nAny\n``````\n\n``````\nByteEncoder\n``````\n\n``````\nAbstractVarModel\n``````\n\n``````\nLoihiNeuronVarModel\n``````\n\n``````\nLoihiAddress\n``````\n\n``````\nNcSpikeIOVarModel\n``````\n\n``````\nNcVarModel\n``````\n\n``````\nAxonEncoder\n``````\n\n``````\nChipEncoder\n``````\n\n``````\nCoreEncoder\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html",
    "title": "lava.magma.compiler.subcompilers — Lava  documentation",
    "content": "Bases:[ResourceAddress](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.ResourceAddress)`ResourceAddress`ResourceAddress\n\nRepresents Logical Id of a resource.\n\nBases:[ResourceAddress](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.ResourceAddress)`ResourceAddress`ResourceAddress\n\nRepresents Virtual Id of a resource.\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nCreate ChannelBuilders from a dict-like ChannelMap object with connected\nports as keys.\n\nChannelBuilders allow Runtime to build channels between Process ports.\n\nThe from_channel_map method creates a ChannelBuilder for every connection\nfrom a source to a destination port in the graph of processes. OutPorts and\nRefPorts are considered source ports while InPorts and VarPorts are\nconsidered destination ports. A ChannelBuilder is only created for\nterminal connections from one leaf process to another. Intermediate\nports of a hierarchical process are ignored.\n\nOnce the Runtime has build the channel it can assign the\ncorresponding CSP ports to the ProcessBuilder\n(i.e. PyProcBuilder.set_csp_ports(..)) and deploy the Process to the\nappropriate compute node.\n\nCreate builders for multiprocessing channels between ports in\nconnected processes.\n\nchannel_map(A dict-like object with a PortPair key for every channel in) – the process network.\n\ncompile_config(ty.Optional[ty.Dict[str,ty.Any]]) – Dictionary that may contain configuration options for the overall\nCompiler as well as all SubCompilers.\n\n`List`List[[ChannelBuilderMp](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.channel_builder.ChannelBuilderMp)`ChannelBuilderMp`ChannelBuilderMp]\n\nA list of ChannelBuilders with a build method that allows the runtime tobuild the actual channels between connected ports.\n\nA list of ChannelBuilders with a build method that allows the runtime to\n\nbuild the actual channels between connected ports.\n\nReturns the d_type of a Process Port, as specified in the\ncorresponding PortImplementation of the ProcessModel implementing the\nProcess\n\n`Any`Any\n\nBases:`object`object\n\n`None`None\n\n`None`None\n\n`None`None\n\n`None`None\n\n`None`None\n\n`None`None\n\nBases:`IntEnum`IntEnum\n\nAn enumeration.\n\nAllocate embedded cores in normal order 0, 1, 2\n\nAllocate embedded cores in reverse order 2, 1, 0. This is useful in\nsituations in case of certain tasks which take longer than others and\nneed to be scheduled on embedded core 0 to ensure nxcore does not stop\ncommunicating on channels\n\nBases:`Exception`Exception\n\nBases:`ABC`ABC\n\nInterface for SubCompilers. Their job is to compile connected groups of\nProcesses, whose ProcessModels can be executed on the same type of\nbackend.\n\nPartitions all Processes in the SubCompiler’s ProcGroup onto the\navailable resources.\n\n[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap\n\nAfter compilation, creates and returns builders for all Processes.\n\n`Tuple`Tuple[`Dict`Dict[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess,[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder],[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap]\n\nBases:[AbstractSubCompiler](https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html#lava.magma.compiler.subcompilers.interfaces.AbstractSubCompiler)`AbstractSubCompiler`AbstractSubCompiler\n\nPartitions all Processes in the SubCompiler’s ProcGroup onto the\navailable resources.\n\n[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap\n\nAfter compilation, creates and returns builders for all Processes.\n\n`Tuple`Tuple[`Dict`Dict[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess,[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder],[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap]\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nResourceAddress\n``````\n\n``````\nint\n``````\n\n``````\nChannelBuilderMp\n``````\n\n``````\nChannelMap\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nAbstractProcessBuilder\n``````\n\n``````\nAbstractSubCompiler\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.compiler.subcompilers.py.html",
    "title": "lava.magma.compiler.subcompilers.py — Lava  documentation",
    "content": "Bases:`object`object\n\nBases:`object`object\n\n`_Offset`_Offset\n\n`int`int\n\nBases:[SubCompiler](https://lava-nc.org/lava/lava.magma.compiler.subcompilers.html#lava.magma.compiler.subcompilers.interfaces.SubCompiler)`SubCompiler`SubCompiler\n\nPartitions all Processes in the SubCompiler’s ProcGroup onto the\navailable resources.\n\n[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap\n\nAfter compilation, creates and returns builders for all Processes.\n\n`Tuple`Tuple[`Dict`Dict[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess,[AbstractProcessBuilder](https://lava-nc.org/lava/lava.magma.compiler.builders.html#lava.magma.compiler.builders.interfaces.AbstractProcessBuilder)`AbstractProcessBuilder`AbstractProcessBuilder],[ChannelMap](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.channel_map.ChannelMap)`ChannelMap`ChannelMap]\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nOptional\n``````\n\n``````\n_Offset\n``````\n\n``````\nSubCompiler\n``````\n\n``````\nChannelMap\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nAbstractProcessBuilder\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.html",
    "title": "lava.magma.core — Lava  documentation",
    "content": "Bases:`ABC`ABC\n\nBase class for callback functions which are executed before\nand after a run in the runtime service. The base class provides\nthe infrastructure to communicate information from runtime to\nruntime service and vice versa as well as the abstract pre- and\npost-callback methods, which needs to be overrwritten by the\nspecial CallbackFx classes for each compute ressource.\n\nTODO: implement runtime <-> runtime_service channel communication.\n\n`ndarray`ndarray\n\n`ndarray`ndarray\n\nBases:[NxSdkCallbackFx](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.callback_fx.NxSdkCallbackFx)`NxSdkCallbackFx`NxSdkCallbackFx\n\nNxSDK callback function to execute iterable of function pointers\nas pre and post run.\n\n`None`None\n\n`None`None\n\nBases:`object`object\n\nBases:[CallbackFx](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.callback_fx.CallbackFx)`CallbackFx`CallbackFx\n\nAbstract class for callback functions processed in the\nNxSdkRuntimeSercice pre- and post run.\n\nTODO: implement runtime <-> runtime_service channel communication.\n\nDecorates ProcessModel class by adding the class of the Process and\nSyncProtocol that this ProcessModel implements as a class variable.\n\n‘implements’ will fail if an attempt is made to overwrite an already set\nProcess or SyncProtocol class of a parent class.\n\nproc(The Process class that the ProcessModel implements.) –\n\nprotocol(The SyncProtocol tht the ProcessModel implements.) –\n\nDecorator for ProcessModel classes that adds class variable to\nProcessModel class that specifies which resources the ProcessModel\nrequires.\nIn order to express optionality between one or more resources, include\nthem in a list or tuple.\n\n-> Requires Res1 and Res2 and one of Res3 or Res4\n\nDecorator for ProcessModel to add a class variable (a list of tags) to\nProcessModel class, which further distinguishes ProcessModels\nimplementing the same Process, with the same type and requiring the same\nComputeResources.\n\nFor example, a user may write multiple ProcessModels in Python (PyProcessModelsPyProcessModels), requiring CPU for execution (@requires(CPU)@requires(CPU)). The\ncompiler selects the appropriate ProcessModel viaRunConfigRunConfigusing\nthe keywords stored in the list of tags set by this decorator.\n\nThe list of tags is additive over inheritance. Which means, if@tag@tagdecorates a child class, whose parent is already decorated, then the new\nkeywords are appended to the tag-list inherited from the parent.\n\nargs(keywords that tag a ProcessModel) –\n\nDecorated class\n\nExamples\n\nThese tags identify a particular ProcessModel as being\nbit-accurate with Loihi hardware platform. This means that,\nthe numerical output produced by such a ProcessModel on a CPU would be\nthe same as on Loihi.\n\nBases:[AbstractResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractResource)`AbstractResource`AbstractResource\n\nA compute resource, for example a particular type of neuromorphic\nprocessor or CPU.\n\nBases:`ABC`ABC\n\nA node is a resource that has other compute or peripheral resources.\n\nBases:[AbstractResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractResource)`AbstractResource`AbstractResource\n\nA hardware resource that is a peripheral device.\n\nBases:`ABC`ABC\n\nA hardware resource like a compute resource (e.g., a particular type\nof neuromorphic processor or a CPU), peripheral device, or complete\nsystem that is required for a ProcessModel.\n\nEach ProcessModel lists its required hardware resources with the\n@requires decorator.\n\nBases:[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)`AbstractComputeResource`AbstractComputeResource\n\nA central processing unit on a regular computer or laptop.\n\nBases:[AbstractPeripheralResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractPeripheralResource)`AbstractPeripheralResource`AbstractPeripheralResource\n\nAn event-based dynamic vision sensor (DVS).\n\nBases:[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)`AbstractComputeResource`AbstractComputeResource\n\nAn embedded central processing unit that is part of a neuromorphic\nchip.\n\nBases:[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)`AbstractComputeResource`AbstractComputeResource\n\nA graphical processing unit.\n\nBases:[AbstractNode](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractNode)`AbstractNode`AbstractNode\n\nA generic resource with a regular CPU and a hard drive.\n\nBases:[AbstractPeripheralResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractPeripheralResource)`AbstractPeripheralResource`AbstractPeripheralResource\n\nA hard drive in a computer.\n\nBases:[GenericNode](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.GenericNode)`GenericNode`GenericNode\n\nThe node on which user executes code, perhaps because processes\nrequire access to specific disk location.\n\nBases:[AbstractPeripheralResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractPeripheralResource)`AbstractPeripheralResource`AbstractPeripheralResource\n\nA hard drive attached to a HeadNode (the node on which a user executes\ncode).\n\nBases:[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)`AbstractComputeResource`AbstractComputeResource\n\nA central processing unit on a special host system that holds\nneuromorphic devices.\n\nBases:[Loihi1System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi1System)`Loihi1System`Loihi1System\n\nA KapohoBay system (USB form-factor) that consists of two Loihi 1 chips\nwith Lakemont processors.\n\nBases:[Loihi2System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi2System)`Loihi2System`Loihi2System\n\nThe smallest form-factor system with four Loihi 2 chips.\n\nBases:[ECPU](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.ECPU)`ECPU`ECPU\n\nA Lakemont embedded central processing unit.\n\nBases:[NeuroCore](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.NeuroCore)`NeuroCore`NeuroCore\n\nA neuromorphic core on a Loihi 1 chip.\n\nBases:[AbstractNode](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractNode)`AbstractNode`AbstractNode\n\nA neuromorphic system that carries Loihi 1 chips.\n\nBases:[NeuroCore](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.NeuroCore)`NeuroCore`NeuroCore\n\nA neuromorphic core on a Loihi 2 chip.\n\nBases:[AbstractNode](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractNode)`AbstractNode`AbstractNode\n\nA neuromorphic system that carries Loihi 2 chips.\n\nBases:[Loihi1System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi1System)`Loihi1System`Loihi1System\n\nA Nahuku system that carries up to 32 Loihi 1 chips.\n\nBases:[AbstractComputeResource](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.AbstractComputeResource)`AbstractComputeResource`AbstractComputeResource\n\nA neuromorphic core.\n\nBases:[Loihi2System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi2System)`Loihi2System`Loihi2System\n\nDevelopment and test system with a single Loihi 2 chip.\n\nBases:[ECPU](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.ECPU)`ECPU`ECPU\n\nA Powell Bute embedded central processing unit.\n\nBases:[Loihi1System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi1System)`Loihi1System`Loihi1System\n\nA system configurable to have one or more Nahuku sub systems.\n\nBases:[Loihi2System](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.resources.Loihi2System)`Loihi2System`Loihi2System\n\nBases:`ABC`ABC\n\nBase class for run conditions.\n\nRunConditions specify for how long a process will run.\n\nblocking(bool) – If set to True, blocks further commands from execution until returns.\n\nBases:[AbstractRunCondition](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_conditions.AbstractRunCondition)`AbstractRunCondition`AbstractRunCondition\n\nRuns a Process continuously without a time step limit (non-blocking).\n\nUsing this RunCondition, the runtime runs continuously and non-blocking.\nThis means that the runtime must be paused or stopped manually by callingpause()pause()orstop()stop()from the running process.\nThe runtime can be continued afterpause()pause()by callingrun()run()again.\n\nBases:[AbstractRunCondition](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_conditions.AbstractRunCondition)`AbstractRunCondition`AbstractRunCondition\n\nRuns a process for a specified number of time steps with respect to a\nSyncDomain assigned to any sub processes.\n\nnum_steps(int) – Number of steps to be run with respect to the SyncDomain.\n\nblocking(bool) – If set to True, blocks further commands from execution until returns.\n(Default = True)\n\nBases:`object`object\n\nBases:[AbstractLoihiRunCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.AbstractLoihiRunCfg)`AbstractLoihiRunCfg`AbstractLoihiRunCfg\n\nBases:[RunConfig](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.RunConfig)`RunConfig`RunConfig\n\nSelects the appropriate ProcessModel for Loihi RunConfigs.\n\nThe following set of rules is applied, in that order of precedence:\n\n1. A dictionary of exceptionsexception_proc_model_mapexception_proc_model_mapis checked first,\nin which user specifies key-value pairs{Process: ProcessModel}{Process: ProcessModel}and theProcessModelProcessModelis returned.\n\nIf there is only 1ProcessModelProcessModelavailable:\n\nIf the user does not specifically ask for any tags,\ntheProcessModelProcessModelis returned\n\nIf the user asks for a specific tag, then theProcessModelProcessModelis\nreturned only if the tag is found in its list of tags.\n\nIf there are multiple[`](https://lava-nc.org/lava/lava.magma.core.html#id1)`ProcessModel`s available:\n\nIf the user asks specifically to look for[`](https://lava-nc.org/lava/lava.magma.core.html#id3)`SubProcessModel`s and\nthey are available:\n\nIf there is only 1SubProcessModelSubProcessModelavailable, it is returned\n\nIf the user did not ask for any specific tags, the first\navailableSubProcessModelSubProcessModelis returned\n\nIf user asked for a specific tag, the first validSubProcessModelSubProcessModelis returned, which has the tag in its tag-list\n\nIf user did not explicitly ask for[`](https://lava-nc.org/lava/lava.magma.core.html#id5)`SubProcessModel`s:\n\nIf the user did not also ask for any specific tag, then the\nfirst available ProcessModel is returned that requires the\ncorrect computing hardware.\n\nIf the user asked for a specific tag,\nthe hardware-specific ProcessModel which has the tag in its\ntag-list is returned\n\ncustom_sync_domains(List[[SyncDomain](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.domain.SyncDomain)SyncDomain]) – list of synchronization domains\n\nselect_tag(str) – The RunConfig will select only ProcessModels that have the tag\n‘select_tag’.\nExample: By setting select_tag=”fixed_pt”, it will select ProcessModels\nthat implement a fixed-point implementation of the Lava Processes in\nthe architecture that is to be executed.\n\nselect_sub_proc_model(bool) – When set to True, hierarchical SubProcessModels are selected over\nLeafProcessModels, where available.\n\nexception_proc_model_map((Dict[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess,[AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel])) – explicit dictionary of {Process: ProcessModel} classes, provided as\nexceptions to the ProcessModel selection logic. The choices made in this\ndict are respected over any logic. For example, {Dense: PyDenseModel}.\nNote that this is a dict mapping classnames to classnames.\n\nloglevel(int) – sets level of event logging, as defined by Python’s ‘logging’\nfacility. Default: logging.WARNING\n\nSelects an appropriate ProcessModel from a list of ProcessModels for\na Process, based on user requests.\n\nprocess([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – Process for which ProcessModel is selected\n\nproc_models(List[[AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel]) – List of ProcessModels to select from\n\nSelected ProcessModel class\n\nBases:[AbstractLoihiRunCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.AbstractLoihiRunCfg)`AbstractLoihiRunCfg`AbstractLoihiRunCfg\n\nBases:`object`object\n\nBases:[AbstractLoihiHWRunCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.AbstractLoihiHWRunCfg)`AbstractLoihiHWRunCfg`AbstractLoihiHWRunCfg\n\nA RunConfig for executing model on Loihi1 HW.\nFor Loihi1 HW configurations, the preferred ProcModels are NcProcModels\nthat can run on a NeuroCore of a Loihi1NeuroCore\nor, if none is found, CProcModels. This preference can be overwritten by\na tag provided by the user. This RunConfig will default to a PyProcModel\nif no Loihi1-compatible ProcModel is being found.\n.\n\nBases:[AbstractLoihiSimRunCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.AbstractLoihiSimRunCfg)`AbstractLoihiSimRunCfg`AbstractLoihiSimRunCfg\n\nRun configuration selects appropriate ProcessModel – eitherSubProcessModelSubProcessModelfor a hierarchical Process or else aPyProcessModelPyProcessModelfor a standard Process.\n\nBases:[AbstractLoihiHWRunCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.AbstractLoihiHWRunCfg)`AbstractLoihiHWRunCfg`AbstractLoihiHWRunCfg\n\nA RunConfig for executing model on Loihi2 HW.\nFor Loihi2 HW configurations, the preferred ProcModels are NcProcModels\nthat can run on a NeuroCore of a Loihi2NeuroCore\nor, if none is found, CProcModels. This preference can be overwritten by\na tag provided by the user. This RunConfig will default to a PyProcModel\nif no Loihi2-compatible ProcModel is being found.\n\nBases:[Loihi1SimCfg](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.Loihi1SimCfg)`Loihi1SimCfg`Loihi1SimCfg\n\nA RunConfig for simulating a Loihi 2 model CPU/GPU.\n\nBases:`ABC`ABC\n\nBasic run configuration and base class for other run configurations.\n\nA RunConfig specifies how to execute Processes on a specific hardware\nbackend. Its main purpose is to select the appropriate ProcessModels\ngiven the Processes to be executed and the given tags (i.e. bit-accurate,\nfloating, etc) using theselect()select()function.\n\nA RunConfig allows the user to guide the compiler in its choice of\nProcessModels. When the user compiles/runs a Process for the first time,\na specific RunConfig must be provided. The compiler will\nfollow the selection rules laid out in the select() method of the\nRunConfig to choose the optimal ProcessModel for the Process.\n\nA RunConfig can filter the ProcessModels by various criteria.\nExamples include the preferred computing resource or user-defined tags.\nIt may also specify how many computing nodes of a certain type,\nlike embedded CPUs, will be available. This will allow to allocate all\nRuntimeService processes during compilation. A RunConfig can also give hints\nto the compiler which computational nodes are required and which are\nexcluded.\n\ncustom_sync_domains(List[[SyncDomain](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.domain.SyncDomain)SyncDomain]) – List of user-specified synchronization domains.\n\nloglevel(int) – Sets level of event logging, as defined by Python’s ‘logging’\nfacility. Default: logging.WARNING\n\nExcludes given nodes from consideration by compiler.\n\nRequires that compiler maps processes to given nodes.\n\nty.Type[[AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel]\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>@implements(proc=ExampleProcess)>>>@tag('bit-accurate','loihi')>>>classExampleProcModel(AbstractProcessModel):...\n``````\n\n``````\nNxSdkCallbackFx\n``````\n\n``````\nCallbackFx\n``````\n\n``````\nAbstractResource\n``````\n\n``````\nAbstractComputeResource\n``````\n\n``````\nAbstractPeripheralResource\n``````\n\n``````\nAbstractNode\n``````\n\n``````\nGenericNode\n``````\n\n``````\nLoihi1System\n``````\n\n``````\nLoihi2System\n``````\n\n``````\nECPU\n``````\n\n``````\nNeuroCore\n``````\n\n``````\nAbstractRunCondition\n``````\n\n``````\nAbstractLoihiRunCfg\n``````\n\n``````\nRunConfig\n``````\n\n``````\nAbstractLoihiHWRunCfg\n``````\n\n``````\nAbstractLoihiSimRunCfg\n``````\n\n``````\nLoihi1SimCfg\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.learning.html",
    "title": "lava.magma.core.learning — Lava  documentation",
    "content": "Bases:`IntEnum`IntEnum\n\nAn enumeration.\n\nBases:[LoihiLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.LoihiLearningRule)`LoihiLearningRule`LoihiLearningRule\n\nBases:[LoihiLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.LoihiLearningRule)`LoihiLearningRule`LoihiLearningRule\n\nEncapsulation of learning-related information according to Loihi.\n\nBy using the three-factor (3F) learning rule all post-synaptic traces\nare handled by the post-synaptic neuron. Hence, the y_impulses and y_taus\nare set to 0 resp. 2^32 to the corresponding low-pass filter.\n\nA LoihiLearningRule object has the following main objectives:\n(1) Given string representations of learning rules (equations) describing\ndynamics of the three synaptic variables (weight, delay, tag),\ngenerate adequate ProductSeries representations and store them.\n\n(2) Store other learning-related information such as:\nimpulse values by which to update traces upon spikes;\ntime constants by which to decay traces over time;\nthe length of the learning epoch;\na dict with dependencies as keys and the set of all traces appearing\nwith them in the specified learning rules as values;\nthe set of traces used by all specified learning rules.\n\nFrom the user’s perspective, a LoihiLearningRule object is to be used\nas follows:\n(1) Instantiate an implementation of LoihiLearningRule object with\nlearning rules given in string format for all three synaptic variables\n(dw, dd, dt), as well as trace configuration parameters (impulse, decay)\nfor all available traces (x1, x2, y1), and the learning epoch length.\n\n(2) The LoihiLearningRule object encapsulating learning-related information\nis then passed to the Dense Process as instantiation argument.\n\n(3) It will internally be used by ProcessModels to derive the operations\nto be executed in the learning phase (Py and Nc).\n\ndw(str) – ProductSeries representation of synaptic weight learning rule.\n\ndd(str) – ProductSeries representation of synaptic delay learning rule.\n\ndt(str) – ProductSeries representation of synaptic tag learning rule.\n\nx1_impulse(float) – Impulse by which x1 increases upon each pre-synaptic spike.\n\nx1_tau(int) – Time constant by which x1 trace decays exponentially over time.\n\nx2_impulse(float) – Impulse by which x2 increases upon each pre-synaptic spike.\n\nx2_tau(int) – Time constant by which x2 trace decays exponentially over time.\n\nt_epoch(int) – Duration of learning epoch.\n\nrng_seed(int) – Seed for the random number generators. If None, seed will be\nchosen randomly. Only used in fixed point implementations.\n\nBases:`object`object\n\nEncapsulation of learning-related information according to Loihi.\n\nA LoihiLearningRule object has the following main objectives:\n(1) Given string representations of learning rules (equations) describing\ndynamics of the three synaptic variables (weight, delay, tag),\ngenerate adequate ProductSeries representations and store them.\n\n(2) Store other learning-related information such as:\nimpulse values by which to update traces upon spikes;\ntime constants by which to decay traces over time;\nthe length of the learning epoch;\na dict with dependencies as keys and the set of all traces appearing\nwith them in the specified learning rules as values;\nthe set of traces used by all specified learning rules.\n\nFrom the user’s perspective, a LoihiLearningRule object is to be used\nas follows:\n(1) Instantiate an implementation of LoihiLearningRule object with\nlearning rules given in string format for all three synaptic variables\n(dw, dd, dt), as well as trace configuration parameters (impulse, decay)\nfor all available traces (x1, x2, y1), and the learning epoch length.\n\n(2) The LoihiLearningRule object encapsulating learning-related information\nis then passed to the Dense Process as instantiation argument.\n\n(3) It will internally be used by ProcessModels to derive the operations\nto be executed in the learning phase (Py and Nc).\n\ndw(str) – ProductSeries representation of synaptic weight learning rule.\n\ndd(str) – ProductSeries representation of synaptic delay learning rule.\n\ndt(str) – ProductSeries representation of synaptic tag learning rule.\n\nx1_impulse(float) – Impulse by which x1 increases upon each pre-synaptic spike.\n\nx1_tau(int) – Time constant by which x1 trace decays exponentially over time.\n\nx2_impulse(float) – Impulse by which x2 increases upon each pre-synaptic spike.\n\nx2_tau(int) – Time constant by which x2 trace decays exponentially over time.\n\ny1_impulse(float) – Impulse by which y1 increases upon each post-synaptic spike.\n\ny1_tau(int) – Time constant by which y1 trace decays exponentially over time.\n\ny2_impulse(float) – Impulse by which y2 increases upon each post-synaptic spike.\n\ny2_tau(int) – Time constant by which y2 trace decays exponentially over time.\n\ny3_impulse(float) – Impulse by which y3 increases upon each post-synaptic spike.\n\ny3_tau(int) – Time constant by which y3 trace decays exponentially over time.\n\nt_epoch(int) – Duration of learning epoch.\n\nrng_seed(int) – Seed for the random number generators. If None, seed will be\nchosen randomly. Only used in fixed point implementations.\n\nGet the active ProductSeries dict, containing ProductSeries\nassociated to string learning rules that were not None.\n\nMapped by target name: either one of (dw, dd, dt)\n\nactive_product_series– Active ProductSeries dict.\n\ndict\n\nGet the set of all active traces in all ProductSeries of\nthis LoihiLearningRule.\n\nactive_traces– Set of all active traces.\n\nset\n\nGet the dict of active traces per dependency associated with\nall ProductSeries of this LoihiLearningRule.\n\nactive_traces_per_dependency– Set of active traces per dependency in the list of ProductSeries.\n\ndict\n\nGet the ProductSeries associated with the “dd” target.\n\ndd– ProductSeries associated with the “dd” target.\n\n[ProductSeries](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.product_series.ProductSeries)ProductSeries, optional\n\nGet the decimate exponent of this LoihiLearningRule.\n\ndecimate_exponent– Decimate exponent of this LoihiLearningRule.\n\nint, optional\n\nGet the ProductSeries associated with the “dt” target.\n\ndt– ProductSeries associated with the “dt” target.\n\n[ProductSeries](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.product_series.ProductSeries)ProductSeries, optional\n\nGet the ProductSeries associated with the “dw” target.\n\ndw– ProductSeries associated with the “dw” target.\n\n[ProductSeries](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.product_series.ProductSeries)ProductSeries, optional\n\nGet the epoch length.\n\nt_epoch– Epoch length.\n\nint\n\nGet the impulse value for x1 trace.\n\nx1_impulse– Impulse value for x1 trace.\n\nfloat\n\nGet the tau value for x1 trace.\n\nx1_tau– Tau value for x1 trace.\n\nint\n\nGet the impulse value for x2 trace.\n\nx2_impulse– Impulse value for x2 trace.\n\nfloat\n\nGet the tau value for x2 trace.\n\nx2_tau– Tau value for x2 trace.\n\nint\n\nGet the impulse value for y1 trace.\n\ny1_impulse– Impulse value for y1 trace.\n\nfloat\n\nGet the tau value for y1 trace.\n\ny1_tau– Tau value for y1 trace.\n\nint\n\nGet the impulse value for y2 trace.\n\ny2_impulse– Impulse value for y2 trace.\n\nfloat\n\nGet the tau value for y2 trace.\n\ny2_tau– Tau value for y2 trace.\n\nint\n\nGet the impulse value for y3 trace.\n\ny3_impulse– Impulse value for y3 trace.\n\nfloat\n\nGet the tau value for y3 trace.\n\ny3_tau– Tau value for y3 trace.\n\nint\n\nBases:`object`object\n\nThe LearningRuleApplier is a Python-specific representation of learning\nrules. It is associated with a ProductSeries.\n\nLearningRuleApplier implementations have to define an apply method, which\ntells how the learning rule represented by the associated ProductSeries,\ngiven states of Dependencies and Factors passed as arguments, is to be\nevaluated.\n\nproduct_series([ProductSeries](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.product_series.ProductSeries)ProductSeries) – ProductSeries associated with this LearningRuleApplier.\n\n`ndarray`ndarray\n\nBases:[AbstractLearningRuleApplier](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule_applier.AbstractLearningRuleApplier)`AbstractLearningRuleApplier`AbstractLearningRuleApplier\n\nThe LearningRuleFixedApplier is an implementation of\nAbstractLearningRuleApplier to be used with the\nPyFixedLearningDenseProcessModel.\n\nContrary to LearningRuleFloatApplier, there is no applier string constructed\nat initialization for LearningRuleFixedApplier.\nThe apply method has to loop through all Products/Factors of the associated\nProductSeries and accumulate results of synaptic variable update computation\nalong the way.\n\nThis is due to the fact that it is not straightforward to construct such a\nstring, in the fixed-point case, as there are intermediary stateful\nbit-shifts happening between steps of the computation, which can’t be\ntranslated to string operations.\n\nApply the learning rule represented by this LearningRuleFixedApplier.\n\nWhen called from the PyFixedLearningDenseProcessModel, applier_args\ncontains variables with the following names :\n{“shape”, “x0”, “y0”, “u”, “weights”, “tag_2”, “tag_1”,\n“x_traces”, “y_traces”}\n\nAll variables apart from “shape”, “u” are numpy arrays.\n“shape” is a tuple.\n“u” is a scalar.\n\ninit_accumulator(np.ndarray) – Shifted values of the synaptic variable before learning rule\napplication.\n\nresult– Shifted values of the synaptic variable after learning rule\napplication.\n\nnp.ndarray\n\nBases:[AbstractLearningRuleApplier](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule_applier.AbstractLearningRuleApplier)`AbstractLearningRuleApplier`AbstractLearningRuleApplier\n\nThe LearningRuleFloatApplier is an implementation of\nAbstractLearningRuleApplier to be used with the\nPyFloatLearningDenseProcessModel.\n\nAt initialization, it goes through the associated ProductSeries and derives\na string representation of the learning rule where Dependencies and Factors\nare written in a way that is coherent with the names of the state variables\nthey are associated to in the arguments of the apply method.\n\nThis string is compiled at initialization and evaluated at every call to\napply.\n\nExample:\ndw = “-2 * x0 * y1 + 4 * y0 * x1 + u0 * w”\n\napplier_str = “-2 * x0 * traces[0][2] + 4 * y0 * traces[1][0] + u * weights”\n\nApply the learning rule represented by this LearningRuleFloatApplier.\n\nWhen called from the PyFloatLearningDenseProcessModel, applier_args\ncontains variables with the following names :\n{“x0”, “y0”, “u”, “weights”, “tag_2”, “tag_1”, “np”, “traces”}\n\nAll variables apart from “u”, “np” are numpy arrays.\n\n“u” is a scalar.\n“np” is a reference to numpy as it is needed for the evaluation of\n“np.sign()” types of call inside the applier string.\n\ninit_accumulator(np.ndarray) – Values of the synaptic variable before learning rule application.\n\nresult– Values of the synaptic variable after learning rule application.\n\nnp.ndarray\n\nBases:`object`object\n\nFactor representation of a single factor found in a Product.\n\nA Factor is a custom data structure holding information on:\n(1) State variable used by this Factor.\n(2) An optional constant added to the state variable.\n(3) Flag specifying if this Factor is a sgn() factor.\n\nstate_var(str) – State variable involved in this Factor.\n\nconst(int,optional) – Constant involved in this Factor.\n\nis_sgn(bool) – Flag specifying if this Factor involves the sgn() function.\n\nGet the constant involved in this Factor.\n\nconst– Constant involved in this Factor.\n\nint, optional\n\nGet factor type string of this Factor.\n\nfactor_type– Factor type string.\n\nstr\n\nCheck if this Factor has a constant.\n\nhas_constant– Flag specifying if this Factor has a constant or not.\n\nbool\n\nGet the is_sgn flag involved in this Factor, specifying if\nthis Factor is an sgn Factor.\n\nis_sgn– Flag specifying if this Factor is an sgn Factor.\n\nbool\n\nGet the state variable involved in this Factor.\n\nstate_var– State variable involved in this Factor.\n\nstr\n\nBases:`object`object\n\nProduct representation of a single product found in a ProductSeries.\n\nA Product is a custom data structure holding information on:\n(1) Synaptic variable affected by the learning rule (target).\n(2) Dependency of the Product.\n(3) Mantissa of the scaling factor associated with the Product.\n(4) Exponent of the scaling factor associated with the Product.\n(5) List of Factors.\n(6) Decimate exponent used if the Dependency is uk.\n\ntarget(str) – Left-hand side of learning rule equation in which the product appears.\nEither one of (dw, dd, dt).\n\ndependency(str) – Dependency used for this Product.\n\ns_mantissa(int) – Mantissa of the scaling constant for this Product.\n\ns_exp(int) – Exponent of the scaling constant for this Product.\n\nfactors(list) – List of Factor objects for this Product.\n\ndecimate_exponent(int,optional) – Decimate exponent used, if dependency is uk.\n\nGet the decimate exponent of this Product.\n\nWill be None if the dependency is not “u”.\n\ndecimate_exponent– Decimate exponent of this Product.\n\nint, optional\n\nGet the dependency of this Product.\n\ndependency– Dependency of this Product.\n\nstr\n\nGet the list of Factors involved in this Product.\n\nfactors– List of Factors involved in this Product.\n\nlist\n\nGet the exponent of the scaling factor of this Product.\n\ns_exp– Exponent of the scaling factor of this Product.\n\nstr\n\nGet the mantissa of the scaling factor of this Product.\n\ns_mantissa– Mantissa of the scaling factor of this Product.\n\nstr\n\nGet the target of this Product.\n\ntarget– Target of this Product.\n\nstr\n\nBases:`object`object\n\nProductSeries representation of a single learning rule.\n\nA ProductSeries is a custom data structure holding information on:\n(1) Synaptic variable affected by the learning rule (target).\n(2) Decimate exponent used in uk dependencies, if any.\n(3) List of Products.\n(4) Dict with dependencies as keys and the set of all traces appearing\nwith them in this ProductSeries.\n\ntarget(str) – Left-hand side of learning rule equation. Either one of (dw, dd, dt).\n\ndecimate_exponent(int,optional) – Decimate exponent used in uk dependencies, if any.\n\nproducts(list) – List of Products.\n\nDict mapping active traces to the set of dependencies they appear with.\n\ndict\n\nGet the dict of active traces per dependency associated with\nthis ProductSeries.\n\nactive_traces_per_dependency– Set of active traces per dependency in the list of ProductSeries.\n\ndict\n\nGet the decimate exponent of this ProductSeries.\n\ndecimate_exponent– Decimate exponent of this ProductSeries.\n\nint, optional\n\nGet the list of Products involved in this ProductSeries.\n\nproducts– List of Products involved in this ProductSeries.\n\nlist\n\nGet the target of this ProductSeries.\n\ntarget– Target of this ProductSeries.\n\nstr\n\nBases:`object`object\n\nSuper class for random generators.\n\nBases:[AbstractRandomGenerator](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.random.AbstractRandomGenerator)`AbstractRandomGenerator`AbstractRandomGenerator\n\nSynaptic variable random generator.\n\nA ConnVarRandom generator holds randomly generated numbers for:\n(1) Stochastic rounding after learning rule application (float).\n\nA call to the advance method generates new random numbers for each of these.\n\nseed_stochastic_rounding(optional,int) – Seed for random generator of stochastic rounding after learning rule\napplication.\n\nGenerate new random numbers for:\n(1) Stochastic rounding after learning rule application.\n\n`None`None\n\nGet randomly generated number for stochastic rounding after\nlearning rule application.\n\nrandom_stochastic_round– Randomly generated number for stochastic rounding after\nlearning rule application.\n\nfloat\n\nBases:[AbstractRandomGenerator](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.random.AbstractRandomGenerator)`AbstractRandomGenerator`AbstractRandomGenerator\n\nTrace random generator.\n\nA TraceRandom generator holds randomly generated numbers for:\n(1) Stochastic rounding after trace decay (float).\n(2) Stochastic rounding after impulse addition (integer).\n\nA call to the advance method generates new random numbers for each of these.\n\nseed_trace_decay(optional,int) – Seed for random generator of stochastic rounding after trace decay.\n\nseed_impulse_addition(optional,int) – Seed for random generator of stochastic rounding after impulse addition.\n\nGenerate new random numbers for:\n(1) Stochastic rounding after trace decay.\n(2) Stochastic rounding after impulse addition.\n\n`None`None\n\nGet randomly generated number for stochastic rounding after\nimpulse addition.\n\nrandom_trace_decay– Randomly generated number for stochastic rounding after\nimpulse addition.\n\nint\n\nGet randomly generated number for stochastic rounding after\ntrace decay.\n\nrandom_trace_decay– Randomly generated number for stochastic rounding after trace decay.\n\nfloat\n\nBases:[Operator](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Operator)`Operator`Operator\n\nSymbol representing the addition operator.\n\nFactory method for creating Addition symbols.\n\nMatches an expression to the regular expressions\n“+”.\n\nReturn a Addition Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Addition](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Addition)`Addition`Addition],`str`str]\n\nsymbol(Addition, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Addition, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[Expression](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Expression)`Expression`Expression\n\nSymbol representing a bracket expression of the form : (...).\n\nFactory method for creating BracketExpression symbols.\n\nMatches an expression to the regular expression\n“(”. If there is a match, find the sub expression.\n\nReturn a BracketExpression Symbol if there is a match and\nclosing bracket and the rest of the expression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[BracketExpression](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.BracketExpression)`BracketExpression`BracketExpression],`str`str]\n\nsymbol(BracketExpression, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(BracketExpression, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nFind sub-expression of an expression, assuming the expression is\nof the form “(...)”, representing a BracketExpression.\n\nexpr(str) – String expression.\n\nsub_expr– String sub-expression.\n\nstr\n\nBases:[FactorSym](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.FactorSym)`FactorSym`FactorSym\n\nAbstract super class for dependency Symbols.\n\nBases:[FactorSym](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.FactorSym)`FactorSym`FactorSym\n\nAbstract super class for multi-symbol Symbols.\n\n`str`str\n\nGet sub-expression of this Expression.\n\nsub_expr– String sub-expression.\n\nstr\n\nGet SymbolList associated with sub-expression of this Expression.\n\nsymbol_list– SymbolList.\n\n[SymbolList](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.SymbolList)SymbolList\n\nBases:[Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)`Symbol`Symbol\n\nAbstract super class for factor Symbols.\n\nBases:[Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)`Symbol`Symbol\n\nSymbol representing a literal.\n\nGet base of this Literal.\n\nbase– Base of this literal.\n\nint\n\nGet exponent of this Literal.\n\nexponent– Exponent of this literal.\n\nint\n\nFactory method for creating Literal symbols.\n\nMatches an expression to the regular expressions\n“[+-]?d+*2^[+-]?d+”, “[+-]?2^[+-]?d+”, “[+-]?d+”.\n\nReturn a Literal Symbol if there is a match and\nclosing bracket and the rest of the expression.\n\nexpr(str) – String expression.\n\nsymbol(Literal, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Literal, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nGet literal type of this Literal.\n\nliteral_type– Literal type of this literal.\n\nint\n\nGet mantissa of this Literal.\n\nmantissa– Mantissa of this literal.\n\nint\n\nExtract mantissa, base and exponent of this Literal from string\nexpression and store them.\n\n`None`None\n\nGet the integer value represented by this Literal.\n\nval– Integer value of this Literal.\n\nint\n\nBases:[Operator](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Operator)`Operator`Operator\n\nSymbol representing the multiplication operator.\n\nFactory method for creating Multiplication symbols.\n\nMatches an expression to the regular expressions\n“*”.\n\nReturn a Multiplication Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Multiplication](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Multiplication)`Multiplication`Multiplication],`str`str]\n\nsymbol(Multiplication, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Multiplication, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)`Symbol`Symbol\n\nAbstract super class for operator Symbols.\n\nBases:[Expression](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Expression)`Expression`Expression\n\nSymbol representing a sign expression of the form :  sgn(...).\n\nFactory method for creating SgnExpression symbols.\n\nMatches an expression to the regular expression\n“sgn(”. If there is a match, find the sub expression.\n\nReturn a SgnExpression Symbol if there is a match and\nclosing bracket and the rest of the expression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[SgnExpression](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.SgnExpression)`SgnExpression`SgnExpression],`str`str]\n\nsymbol(SgnExpression, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(SgnExpression, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nFind sub-expression of an expression, assuming the expression is\nof the form “sgn(...)”, representing a SgnExpression.\n\nexpr(str) – String expression.\n\nsub_expr– String sub-expression.\n\nstr\n\nBases:[Operator](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Operator)`Operator`Operator\n\nSymbol representing the subtraction operator.\n\nFactory method for creating Subtraction symbols.\n\nMatches an expression to the regular expressions\n“-”.\n\nReturn a Subtraction Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Subtraction](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Subtraction)`Subtraction`Subtraction],`str`str]\n\nsymbol(Subtraction, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Subtraction, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:`ABC`ABC\n\nSuper class for all possible symbols.\n\nGet expression of the Symbol.\n\nexpr– String expression.\n\nstr\n\nFactory method for creating symbols.\n\nMatches an expression to a regular expression and if there is a match,\nreturn a symbol of the matching part of the expression\nas well as the rest of the expression.\n\nexpr(str) – String expression.\n\nreg_expr(str) – Regular expression.\n\nsymbol([Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)Symbol) – Uninitialized symbol\n\n`Tuple`Tuple[`Optional`Optional[[Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)`Symbol`Symbol],`str`str]\n\nsymbol(Symbol, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Symbol, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)`Symbol`Symbol\n\nRepresents as list of Symbols.\n\nAppend a Symbol to the SymbolList’s list and the Symbol’s\nexpression to the SymbolList’s expression.\n\nsymbol([Symbol](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Symbol)Symbol) – Symbol object.\n\n`None`None\n\nGet list of the SymbolList.\n\nlist– List of Symbol objects.\n\nlist\n\nBases:`object`object\n\nThe SymbolicEquation represents a learning rule as a set of symbols.\n\nIt provides means to generate a SymbolicEquation from a string following\na fixed syntax.\n\ntarget(str) – Target of the learning rule to be represented by this SymbolicEquation.\n\nstr_learning_rule(str) – Learning rule in string format to be represented by\nthis SymbolicEquation.\n\nGet SymbolList of this SymbolicEquation.\n\nsymbol_list– SymbolList of this SymbolicEquation.\n\n[SymbolList](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.SymbolList)SymbolList\n\nGet target of this SymbolicEquation.\n\ntarget– Target of this SymbolicEquation.\n\nstr\n\nBases:[Dependency](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Dependency)`Dependency`Dependency\n\nSymbol representing the uk dependency.\n\nGet decimate exponent of this Uk.\n\ndecimate_exponent– Decimate exponent.\n\nint\n\nFactory method for creating Uk symbols.\n\nMatches an expression to the regular expressions\n“ud”.\n\nReturn a Uk Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Uk](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Uk)`Uk`Uk],`str`str]\n\nsymbol(Uk, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Uk, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[FactorSym](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.FactorSym)`FactorSym`FactorSym\n\nSymbol representing traces and synaptic variable factors.\n\nFactory method for creating Variable symbols.\n\nMatches an expression to the regular expressions\n“x[12]”, “y[123]”, “w”, “d”, “t”.\n\nReturn a Variable Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Variable](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Variable)`Variable`Variable],`str`str]\n\nsymbol(Variable, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Variable, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[Dependency](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Dependency)`Dependency`Dependency\n\nSymbol representing the x0 dependency.\n\nFactory method for creating X0 symbols.\n\nMatches an expression to the regular expressions\n“x0”.\n\nReturn a X0 Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[X0](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.X0)`X0`X0],`str`str]\n\nsymbol(X0, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(X0, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nBases:[Dependency](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Dependency)`Dependency`Dependency\n\nSymbol representing the y0 dependency.\n\nFactory method for creating Y0 symbols.\n\nMatches an expression to the regular expressions\n“y0”.\n\nReturn a Y0 Symbol if there is a match and the rest of the\nexpression.\n\nexpr(str) – String expression.\n\n`Tuple`Tuple[`Optional`Optional[[Y0](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.symbolic_equation.Y0)`Y0`Y0],`str`str]\n\nsymbol(Y0, optional) – Symbol matching regular expression.expr(str) – Remaining string expression after extraction of the symbol.\n\nsymbol(Y0, optional) – Symbol matching regular expression.\n\nexpr(str) – Remaining string expression after extraction of the symbol.\n\nGet nb_bits least-significant bits.\n\nitem(np.ndarrayorint) – Item to apply mask to.\n\nnb_bits(int) – Number of LSBs to keep.\n\nresult– Least-significant bits.\n\nnp.ndarray or int\n\nConvert the floating point representation of the\nlearning parameter to the form mantissa * 2 ^ [+/1]exponent.\n:param learning_parameters: the float value of learning-related parameter\n:type learning_parameters: float\n\nresult– string representation of learning_parameter.\n\nstr\n\nStochastically add 1 to an ndarray at location where random numbers are\nless than given probabilities.\n\nvalues(ndarray) – Values before stochastic rounding.\n\nrandom_numbers(intorfloatorndarray) – Randomly generated number or ndarray of numbers.\n\nprobabilities(ndarray) – Probabilities to stochastically round.\n\nresult– Stochastically rounded values.\n\nndarray\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nLoihiLearningRule\n``````\n\n``````\nAbstractLearningRuleApplier\n``````\n\n``````\nAbstractRandomGenerator\n``````\n\n``````\nOperator\n``````\n\n``````\nAddition\n``````\n\n``````\nExpression\n``````\n\n``````\nBracketExpression\n``````\n\n``````\nFactorSym\n``````\n\n``````\nSymbol\n``````\n\n``````\nMultiplication\n``````\n\n``````\nSgnExpression\n``````\n\n``````\nSubtraction\n``````\n\n``````\nDependency\n``````\n\n``````\nUk\n``````\n\n``````\nVariable\n``````\n\n``````\nX0\n``````\n\n``````\nY0\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.model.html",
    "title": "lava.magma.core.model — Lava  documentation",
    "content": "Bases:`ABC`ABC\n\nReturns all csp ports of the port.\n\nJoin all csp ports\n\nReturns the shape of the port\n\nStart all csp ports.\n\nBases:`ABC`ABC\n\nRepresents a model that implements the behavior of a Process.\n\nProcessModels enable seamless cross-platform execution of Processes. In\nparticular, they enable building applications or algorithms using Processes\nagnostic of the ProcessModel chosen at compile time. There are two\nbroad categories of ProcessModels:\n1. LeafProcessModels allow to implement the behavior of a Process\ndirectly in different languages for a particular compute resource.\nProcessModels specify what Process they implement and what\nSynchronizationProtocol they implement (if necessary for the operation of\nthe Process). In addition, they specify which compute resource they require\nto function. All this information allows the compiler to map a Process\nand its ProcessModel to the appropriate hardware platform.\n2. SubProcessModels allow to implement and compose the behavior of a\nProcess in terms of other Processes. This enables the creation of\nhierarchical Processes and reuse of more primitive ProcessModels to\nrealize more complex ProcessModels. SubProcessModels inherit all compute\nresource requirements from the Processes they instantiate. See\ndocumentation of AbstractProcessModel for more details.\n\nProcessModels are usually not instantiated by the user directly but by\nthe compiler. ProcessModels are expected to have the same variables and\nports as those defined in the Process but with an implementation specific\nto the ProcessModel. I.e. in a PyProcessModel, a Var will be implemented by\na np.ndarray and a Port might be implemented with a PyInputPort.\nThe compiler is supposed to instantiate these ProcModels and initialize\nthose vars and ports given initial values from the Process and\nimplementation details from the ProcModel.\nFor transparency, class attributes and their types should be\nexplicitly defined upfront by the developer of a ProcModel to avoid lint\nwarnings due to unresolved variables or unknown or illegal types.\n\nThis is a proposal of a low-boilerplate code convention to achieve this:\n\nThe same Vars and Ports as defined in the Process must be defined as\nclass variables in the ProcessModels.\n\nThese class variables should be initialized with LavaType objects.\nLavaTypes specify the future class-type of this Var or Port, the numeric\nd_type and precision and maybe dynamic range if different from what\nwould be implied by d_type. The compiler will later read these LavaTypes\ndefined at the class level to initialize concrete class objects from the\ninitial values provided in the Process. During this process, the\ncompiler will create object level attributes with the same name as the\nclass level variables. This should not cause problems as class level and\ninstance level attributes can co-exist. However, instance level\nattributes shadow class level attributes with the same name if they\nexist.\n\nDirect type annotations should be used equal to the class type in\nthe LavaType to suppress type warnings in the rest of the class code\nalthough this leads to a bit of verbosity in the end. We could leave out\nthe class type in the LavaType and infer it from\nProcModel.__annotations__ if the user has not forgotten to specify it.\n\nProcess can communicate arbitrary objects using it’s`proc_params`proc_paramsmember. This should be used when such a need arises. A Process’s`proc_prams`proc_prams(empty dictionary by default) should always be used to\ninitialize it’s ProcessModel.\n\nBases:`IntEnum`IntEnum\n\nTypes of Spike Encoding\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.model.py.html",
    "title": "lava.magma.core.model.py — Lava  documentation",
    "content": "Bases:`object`object\n\nBase class for learning connection ProcessModels.\n\nBases:[PyLearningConnection](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.connection.PyLearningConnection)`PyLearningConnection`PyLearningConnection\n\nFixed-point, bit-approximate implementation of the Connection base\nclass.\n\nThis class implements the learning simulation with integer and fixed\npoint arithmetic but does not implement the exact behavior of Loihi.\nNevertheless, the results are comparable to those by Loihi.\n\nTo summarize the behavior:\n\nSpiking phase:\nrun_spk:\n\n(1) (Dense) Send activations from past time step to post-synaptic\nneuron Process.\n(2) (Dense) Compute activations to be sent on next time step.\n(3) (Dense) Receive spikes from pre-synaptic neuron Process.\n(4) (Dense) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n(5) Receive spikes from post-synaptic neuron Process.\n(6) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n(7) Advance trace random generators.\n\nLearning phase:\nrun_lrn:\n\nAdvance synaptic variable random generators.\n\n(2) Compute updates for each active synaptic variable,\naccording to associated learning rule,\nbased on the state of Vars representing dependencies and factors.\n(3) Update traces based on within-epoch spiking times and trace\nconfiguration parameters (impulse, decay).\n(4) Reset within-epoch spiking times and dependency Vars\n\nNote: The synaptic variable tag_2 currently DOES NOT induce synaptic\ndelay in this connections Process. It can be adapted according to its\nlearning rule (learned), but it will not affect synaptic activity.\n\nproc_params(dict) – Parameters from the ProcessModel\n\nBases:[PyLearningConnection](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.connection.PyLearningConnection)`PyLearningConnection`PyLearningConnection\n\nFloating-point implementation of the Connection Process.\n\nThis ProcessModel constitutes a behavioral implementation of Loihi synapses\nwritten in Python, executing on CPU, and operating in floating-point\narithmetic.\n\nTo summarize the behavior:\n\nSpiking phase:\nrun_spk:\n\n(1) (Dense) Send activations from past time step to post-synaptic\nneuron Process.\n(2) (Dense) Compute activations to be sent on next time step.\n(3) (Dense) Receive spikes from pre-synaptic neuron Process.\n(4) (Dense) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n(5) Receive spikes from post-synaptic neuron Process.\n(6) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n\nLearning phase:\nrun_lrn:\n\n(1) Compute updates for each active synaptic variable,\naccording to associated learning rule,\nbased on the state of Vars representing dependencies and factors.\n(2) Update traces based on within-epoch spiking times and trace\nconfiguration parameters (impulse, decay).\n(3) Reset within-epoch spiking times and dependency Vars\n\nNote: The synaptic variable tag_2 currently DOES NOT induce synaptic\ndelay in this connections Process. It can be adapted according to its\nlearning rule (learned), but it will not affect synaptic activity.\n\nproc_params(dict) – Parameters from the ProcessModel\n\nBases:[AbstractLearningConnection](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.connection.AbstractLearningConnection)`AbstractLearningConnection`AbstractLearningConnection\n\nBase class for learning connection ProcessModels in Python / CPU.\n\nThis class provides commonly used functions for simulating the Loihi\nlearning engine. It is subclasses for floating and fixed point\nsimulations.\n\nTo summarize the behavior:\n\nSpiking phase:\nrun_spk:\n\n(1) (Dense) Send activations from past time step to post-synaptic\nneuron Process.\n(2) (Dense) Compute activations to be sent on next time step.\n(3) (Dense) Receive spikes from pre-synaptic neuron Process.\n(4) (Dense) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n(5) Receive spikes from post-synaptic neuron Process.\n(6) Record within-epoch pre-synaptic spiking time.\nUpdate pre-synaptic traces if more than one spike during the epoch.\n(7) Advance trace random generators.\n\nLearning phase:\nrun_lrn:\n\nAdvance synaptic variable random generators.\n\n(2) Compute updates for each active synaptic variable,\naccording to associated learning rule,\nbased on the state of Vars representing dependencies and factors.\n(3) Update traces based on within-epoch spiking times and trace\nconfiguration parameters (impulse, decay).\n(4) Reset within-epoch spiking times and dependency Vars\n\nNote: The synaptic variable tag_2 currently DOES NOT induce synaptic\ndelay in this connections Process. It can be adapted according to its\nlearning rule (learned), but it will not affect synaptic activity.\n\nproc_params(dict) – Parameters from the ProcessModel\n\n`bool`bool\n\nUpdate the learning rule parameters when on single Var is\nupdated.\n\nFunction to receive and update y1, y2 and y3 traces\nfrom the post-synaptic neuron.\n\ns_in(np.adarray) – Synaptic spike input\n\n`None`None\n\n`None`None\n\nBases:[AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)`AbstractProcessModel`AbstractProcessModel,`ABC`ABC\n\nAbstract interface for Python ProcessModels.\n\na_in: PyInPort =   LavaPyType(PyInPort.VEC_DENSE, float)\ns_out: PyInPort =  LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\nu: np.ndarray =    LavaPyType(np.ndarray, np.int32, precision=24)\nv: np.ndarray =    LavaPyType(np.ndarray, np.int32, precision=24)\nbias: np.ndarray = LavaPyType(np.ndarray, np.int16, precision=12)\ndu: int =          LavaPyType(int, np.uint16, precision=12)\n\nSets attribute in the object. This function is used by the builder\nto add ports to py_ports and var_ports list.\n\nkey(Attribute being set) –\n\nvalue(Valueofthe attribute) –\n\n-------–\n\nAdd various ports to poll for communication on ports\n\nWait for all the ports to shutdown.\n\nThis method is called if a Var is updated. It\ncan be used as callback function to calculate dependent\nchanges.\n\nRetrieves commands from the runtime service and calls their\ncorresponding methods of the ProcessModels.\nAfter calling the method of the ProcessModels, the runtime service\nis informed about completion. The loop ends when the STOP command is\nreceived.\n\nStarts the process model, by spinning up all the ports (mgmt and\npy_ports) and calls the run function.\n\nBases:[AbstractPyProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.AbstractPyProcessModel)`AbstractPyProcessModel`AbstractPyProcessModel\n\nProcess Model for Asynchronous Processes executed on CPU.\n\nThis ProcessModel is used in combination with the AsyncProtocol to\nimplement asynchronous execution. This means that the processes could run\nwith a varying speed and message passing is possible at any time.\n\nIn order to use the PyAsyncProcessModel, therun_async()run_async()function\nmust be implemented which defines the behavior of the underlying Process.\n\nExample\n\nBases:`object`object\n\nDifferent types of response for a RuntimeService Request\n\nSignifies Request of PAUSE\n\nSignifies Request of STOP\n\nSignifies Ack or Finished with the Command\n\nSignifies Error raised\n\nSignifies Execution State to be Paused\n\nSignifies Termination\n\nAdd various ports to poll for communication on ports\n\nChecks if the RS has sent a PAUSE command.\n\n`bool`bool\n\nChecks if the RS has sent a STOP command.\n\n`bool`bool\n\nUser needs to define this function which will run asynchronously when\nRUN command is received.\n\nFactory function that converts Py-Loihi process models\nto equivalent Py-Async definition.\n\npy_loihi_model(ty.Type[[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)PyLoihiProcessModel]) – Py-Loihi model that describes the functional behavior of a process\nusing Loihi Protocol.\n\nEquivalent Py-Async protocol model. The async process model\nclass name is the original loihi process model class name with Async\npostfix.\n\nty.Type[[PyAsyncProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyAsyncProcessModel)PyAsyncProcessModel]\n\nBases:[AbstractPyProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.AbstractPyProcessModel)`AbstractPyProcessModel`AbstractPyProcessModel\n\nProcessModel to simulate a Process on Loihi using CPU.\n\nThe PyLoihiProcessModel implements the same phases of execution as the\nLoihi 1/2 processor but is executed on CPU. See the LoihiProtocol for a\ndescription of the different phases.\n\nExample\n\nBases:`object`object\n\nDifferent States of the State Machine of a Loihi Process\n\nBases:`object`object\n\nDifferent types of response for a RuntimeService Request\n\nSignifies Request of LEARNING\n\nSignifies Request of PAUSE\n\nSignifies Request of PREMPTION after Learning\n\nSignifies Request of PREMPTION before Learning\n\nSignifies Request of STOP\n\nSignfies Ack or Finished with the Command\n\nSignifies Error raised\n\nSignifies Execution State to be Paused\n\nSignifies Termination\n\nAdd various ports to poll for communication on ports\n\nRequired for output ports which should send a signal to advance time\n\nGuard function that determines if lrn phase will get\nexecuted or not for the current timestep.\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\nGuard function that determines if pre lrn mgmt phase will get\nexecuted or not for the current timestep.\n\nFunction that runs in Learning Phase\n\nFunction that runs in Post Lrn Mgmt Phase\n\nFunction that runs in Pre Lrn Mgmt Phase\n\nFunction that runs in Spiking Phase\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nBase class for learning enables neuron models.\n\nImplements ports and vars used by learning enabled neurons. Must be\ninherited by floating point and fixed point implementations.\n\nproc_params(dict) – Process parameters from the neuron model.\n\nBases:[LearningNeuronModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.neuron.LearningNeuronModel)`LearningNeuronModel`LearningNeuronModel\n\nBase class for learning enables neuron models.\n\nImplements ports and vars used by learning enabled neurons for fixed\npoint implementations.\n\nproc_params(dict) – Process parameters from the neuron model.\n\nBases:[LearningNeuronModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.neuron.LearningNeuronModel)`LearningNeuronModel`LearningNeuronModel\n\nBase class for learning enables neuron models.\n\nImplements ports and vars used by learning enabled neurons for floating\npoint implementations.\n\nproc_params(dict) – Process parameters from the neuron model.\n\nBases:[AbstractPyPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractPyPort)`AbstractPyPort`AbstractPyPort\n\nAbstract class of an input/output Port implemented in python.\n\nA PyIOPort can either be an input or an output Port and is the common\nabstraction of PyInPort/PyOutPort.\n_csp_ports is a list of CSP Ports which are used to send/receive data by\nconnected PyIOPorts.\n\ncsp_ports(list) – A list of CSP Ports used by this IO Port.\n\nprocess_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – The process model used by the process of the Port.\n\nshape(tuple) – The shape of the Port.\n\nd_type(type) – The data type of the Port.\n\nA list of CSP Ports used by this IO Port.\n\nlist\n\nProperty to get the corresponding CSP Ports of all connected\nPyPorts (csp_ports). The CSP Port is the low level interface of the\nbackend messaging infrastructure which is used to send and receive data.\n\nA list of all CSP Ports connected to the PyPort.\n\nBases:[AbstractPortImplementation](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.interfaces.AbstractPortImplementation)`AbstractPortImplementation`AbstractPortImplementation\n\nAbstract class for Ports implemented in Python.\n\nPorts at the Process level provide an interface to connect\nProcesses with each other. Once two Processes have been connected by Ports,\nthey can exchange data.\nLava provides four types of Ports: InPorts, OutPorts, RefPorts and VarPorts.\nAn OutPort of a Process can be connected to one or multiple InPorts of other\nProcesses to transfer data from the OutPort to the InPorts. A RefPort of a\nProcess can be connected to a VarPort of another Process. The difference to\nIn-/OutPorts is that a VarPort is directly linked to a Var and via a\nRefPort the Var can be directly modified from a different Process.\nTo exchange data, PyPorts provide an interface to send and receive messages\nvia channels implemented by a backend messaging infrastructure, which has\nbeen inspired by the Communicating Sequential Processes (CSP) paradigm.\nThus, a channel denotes a CSP channel of the messaging infrastructure and\nCSP Ports denote the low level ports also used in the messaging\ninfrastructure. PyPorts are the implementation for message exchange in\nPython, using the low level CSP Ports of the backend messaging\ninfrastructure. A PyPort may have one or multiple connection to other\nPyPorts. These connections are represented by csp_ports, which is a list of\nCSP ports corresponding to the connected PyPorts.\n\nAbstract property to get a list of the corresponding CSP Ports of all\nconnected PyPorts. The CSP Port is the low level interface of the\nbackend messaging infrastructure which is used to send and receive data.\n\nA list of all CSP Ports connected to the PyPort.\n\nBases:`ABC`ABC\n\nInterface for Transformers that are used in receiving PyPorts to\ntransform data.\n\nTransforms incoming data in way that is determined by which CSP\nport the data is received.\n\ndata(numpy.ndarray) – data that will be transformed\n\ncsp_port([AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort) – CSP port that the data was received on\n\ntransformed_data– the transformed data\n\nnumpy.ndarray\n\nBases:[AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)`AbstractTransformer`AbstractTransformer\n\nTransformer that does not transform the data but returns it unchanged.\n\nTransforms incoming data in way that is determined by which CSP\nport the data is received.\n\ndata(numpy.ndarray) – data that will be transformed\n\ncsp_port([AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort) – CSP port that the data was received on\n\ntransformed_data– the transformed data\n\nnumpy.ndarray\n\nBases:[AbstractPyIOPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractPyIOPort)`AbstractPyIOPort`AbstractPyIOPort\n\nPython implementation of InPort used within AbstractPyProcessModel.\n\nPyInPort is an input Port that can be used in a Process to receive data sent\nfrom a connected PyOutPort of another Process over a channel. PyInPort can\nreceive (recv()) the data, which removes it from the channel, look (peek())\nat the data which keeps it on the channel or check (probe()) if there is\ndata on the channel. The different class attributes are used to select the\ntype of OutPorts via LavaPyType declarations in PyProcModels, e.g.,\nLavaPyType(PyInPort.VEC_DENSE, np.int32, precision=24) creates a PyInPort.\nA PyOutPort (source) can be connected to one or multiple PyInPorts (target).\n\ncsp_ports(ty.List[[AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort]) – Used to receive data from the referenced PyOutPort.\n\nprocess_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – The process model used by the process of the Port.\n\nshape(tuple,default=tuple()) – The shape of the Port.\n\nd_type(type,default=int) – The data type of the Port.\n\ntransformer([AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer,default: identity function) – Enables transforming the received data in accordance with the\nvirtual ports on the path to the PyInPort.\n\nEnables transforming the received data in accordance with the\nvirtual ports on the path to the PyVarPort.\n\n[AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer\n\nType of PyInPort. CSP Port sends data as dense vector.\n\n[PyInPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortVectorDense)PyInPortVectorDense, default=None\n\nType of PyInPort. CSP Port sends data as sparse vector (data + indices),\nso only entries which have changed in a vector need to be communicated.\n\n[PyInPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortVectorSparse)PyInPortVectorSparse, default=None\n\nType of PyInPort. CSP Port sends data element by element for the whole\ndata structure. So the CSP channel does need less memory to transfer\ndata.\n\n[PyInPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortScalarDense)PyInPortScalarDense, default=None\n\nType of PyInPort. CSP Port sends data element by element, but after each\nelement the index of the data entry is also given. So only entries which\nneed to be changed need to be communicated.\n\n[PyInPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortScalarSparse)PyInPortScalarSparse, default=None\n\nalias of[PyInPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortScalarDense)`PyInPortScalarDense`PyInPortScalarDense\n\nalias of[PyInPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortScalarSparse)`PyInPortScalarSparse`PyInPortScalarSparse\n\nalias of[PyInPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortVectorDense)`PyInPortVectorDense`PyInPortVectorDense\n\nalias of[PyInPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPortVectorSparse)`PyInPortVectorSparse`PyInPortVectorSparse\n\nAbstract method to receive data (vectors/scalars) sent from connected\nOutPorts (source Ports). Keeps the data on the channel.\n\nThe scalar or vector received from a connected OutPort. If the InPort isconnected to several OutPorts, their input is added in a point-wisefashion.\n\nThe scalar or vector received from a connected OutPort. If the InPort is\n\nconnected to several OutPorts, their input is added in a point-wise\n\nfashion.\n\nMethod to check (probe) if there is data (vectors/scalars)\nto receive from connected OutPorts (source Ports).\n\nresult– Returns True only when there is data to receive from all connected\nOutPort channels.\n\nbool\n\nAbstract method to receive data (vectors/scalars) sent from connected\nOutPorts (source Ports). Removes the retrieved data from the channel.\nExpects data on the channel and will block execution if there is no data\nto retrieve on the channel.\n\nThe scalar or vector received from a connected OutPort. If the InPort isconnected to several OutPorts, their input is added in a point-wisefashion.\n\nThe scalar or vector received from a connected OutPort. If the InPort is\n\nconnected to several OutPorts, their input is added in a point-wise\n\nfashion.\n\nBases:[PyInPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPort)`PyInPort`PyInPort\n\nPython implementation of PyInPort for dense scalar data.\n\nTBD\n\n`int`int\n\nTBD\n\n`int`int\n\nBases:[PyInPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPort)`PyInPort`PyInPort\n\nPython implementation of PyInPort for sparse scalar data.\n\nTBD\n\n`Tuple`Tuple[`int`int,`int`int]\n\nTBD\n\n`Tuple`Tuple[`int`int,`int`int]\n\nBases:[PyInPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPort)`PyInPort`PyInPort\n\nPython implementation of PyInPort for dense vector data.\n\nMethod to receive data (vectors) sent from connected\nOutPorts (source Ports). Keeps the data on the channel.\n\nresult– The vector received from a connected OutPort. If the InPort is\nconnected to several OutPorts, their input is added in a point-wise\nfashion.\n\nndarray of shape _shape\n\nMethod to receive data (vectors/scalars) sent from connected\nOutPorts (source Ports). Removes the retrieved data from the channel.\nExpects data on the channel and will block execution if there is no data\nto retrieve on the channel.\n\nresult– The vector received from a connected OutPort. If the InPort is\nconnected to several OutPorts, their input is added in a point-wise\nfashion.\n\nndarray of shape _shape\n\nBases:[PyInPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyInPort)`PyInPort`PyInPort\n\nPython implementation of PyInPort for sparse vector data.\n\nTBD\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray]\n\nTBD\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray]\n\nBases:[AbstractPyIOPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractPyIOPort)`AbstractPyIOPort`AbstractPyIOPort\n\nPython implementation of OutPort used within AbstractPyProcessModels.\n\nPyOutPort is an output Port sending data to a connected input Port\n(PyInPort) over a channel. PyOutPort can send (send()) the data by adding it\nto the channel, or it can clear (flush()) the channel to remove any data\nfrom it. The different class attributes are used to select the type of\nOutPorts via LavaPyType declarations in PyProcModels, e.g., LavaPyType(\nPyOutPort.VEC_DENSE, np.int32, precision=24) creates a PyOutPort.\nA PyOutPort (source) can be connected to one or multiple PyInPorts (target).\n\ncsp_ports(list) – A list of CSP Ports used by this IO Port.\n\nprocess_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – The process model used by the process of the Port.\n\nshape(tuple) – The shape of the Port.\n\nd_type(type) – The data type of the Port.\n\nType of PyInPort. CSP Port sends data as dense vector.\n\n[PyOutPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortVectorDense)PyOutPortVectorDense, default=None\n\nType of PyInPort. CSP Port sends data as sparse vector (data + indices),\nso only entries which have changed in a vector need to be communicated.\n\n[PyOutPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortVectorSparse)PyOutPortVectorSparse, default=None\n\nType of PyInPort. CSP Port sends data element by element for the whole\ndata structure. So the CSP channel does need less memory to transfer\ndata.\n\n[PyOutPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortScalarDense)PyOutPortScalarDense, default=None\n\nType of PyInPort. CSP Port sends data element by element, but after each\nelement the index of the data entry is also given. So only entries which\nneed to be changed need to be communicated.\n\n[PyOutPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortScalarSparse)PyOutPortScalarSparse, default=None\n\nalias of[PyOutPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortScalarDense)`PyOutPortScalarDense`PyOutPortScalarDense\n\nalias of[PyOutPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortScalarSparse)`PyOutPortScalarSparse`PyOutPortScalarSparse\n\nalias of[PyOutPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortVectorDense)`PyOutPortVectorDense`PyOutPortVectorDense\n\nalias of[PyOutPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPortVectorSparse)`PyOutPortVectorSparse`PyOutPortVectorSparse\n\nTBD\n\nAbstract method to send data to the connected Port PyInPort (target).\n\ndata(ndarrayorint) – The data (vector or scalar) to be sent to the InPort (target).\n\nBases:[PyOutPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPort)`PyOutPort`PyOutPort\n\nPython implementation of PyOutPort for dense scalar data.\n\nTBD\n\nBases:[PyOutPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPort)`PyOutPort`PyOutPort\n\nPython implementation of PyOutPort for sparse scalar data.\n\nTBD\n\nBases:[PyOutPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPort)`PyOutPort`PyOutPort\n\nPython implementation of PyOutPort for dense vector data.\n\nAbstract method to send data to the connected in Port (target).\n\nSends data only if the OutPort is connected to at least one InPort.\n\ndata(ndarray) – The data vector to be sent to the in Port (target).\n\nBases:[PyOutPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyOutPort)`PyOutPort`PyOutPort\n\nPython implementation of PyOutPort for sparse vector data.\n\nTBD\n\nBases:[AbstractPyPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractPyPort)`AbstractPyPort`AbstractPyPort\n\nPython implementation of RefPort used within AbstractPyProcessModels.\n\nA PyRefPort is a Port connected to a VarPort of a variable Var of another\nProcess. It is used to get or set the value of the referenced Var across\nProcesses. A PyRefPort is connected via two CSP channels and corresponding\nCSP ports to a PyVarPort. One channel is used to send data from the\nPyRefPort to the PyVarPort and the other channel is used to receive data\nfrom the PyVarPort. PyRefPorts can get the value of a referenced Var\n(read()), set the value of a referenced Var (write()) and block execution\nuntil receipt of prior ‘write’ commands (sent from PyRefPort to PyVarPort)\nhave been acknowledged (wait()).\n\ncsp_send_port([CspSendPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspSendPort)CspSendPortorNone) – Used to send data to the referenced Port PyVarPort (target).\n\ncsp_recv_port([CspRecvPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspRecvPort)CspRecvPortorNone) – Used to receive data from the referenced Port PyVarPort (source).\n\nprocess_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – The process model used by the process of the Port.\n\nshape(tuple,default=tuple()) – The shape of the Port.\n\nd_type(type,default=int) – The data type of the Port.\n\ntransformer([AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer,default: identity function) – Enables transforming the received data in accordance with the\nvirtual ports on the path to the PyRefPort.\n\nUsed to send data to the referenced Port PyVarPort (target).\n\n[CspSendPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspSendPort)CspSendPort\n\nUsed to receive data from the referenced Port PyVarPort (source).\n\n[CspRecvPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspRecvPort)CspRecvPort\n\nEnables transforming the received data in accordance with the\nvirtual ports on the path to the PyRefPort.\n\n[AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer\n\nType of PyInPort. CSP Port sends data as dense vector.\n\n[PyRefPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortVectorDense)PyRefPortVectorDense, default=None\n\nType of PyInPort. CSP Port sends data as sparse vector (data + indices),\nso only entries which have changed in a vector need to be communicated.\n\n[PyRefPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortVectorSparse)PyRefPortVectorSparse, default=None\n\nType of PyInPort. CSP Port sends data element by element for the whole\ndata structure. So the CSP channel does need less memory to transfer\ndata.\n\n[PyRefPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortScalarDense)PyRefPortScalarDense, default=None\n\nType of PyInPort. CSP Port sends data element by element, but after each\nelement the index of the data entry is also given. So only entries which\nneed to be changed need to be communicated.\n\n[PyRefPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortScalarSparse)PyRefPortScalarSparse, default=None\n\nalias of[PyRefPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortScalarDense)`PyRefPortScalarDense`PyRefPortScalarDense\n\nalias of[PyRefPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortScalarSparse)`PyRefPortScalarSparse`PyRefPortScalarSparse\n\nalias of[PyRefPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortVectorDense)`PyRefPortVectorDense`PyRefPortVectorDense\n\nalias of[PyRefPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPortVectorSparse)`PyRefPortVectorSparse`PyRefPortVectorSparse\n\nProperty to get the corresponding CSP Ports of all connected\nPyPorts (csp_ports). The CSP Port is the low level interface of the\nbackend messaging infrastructure which is used to send and receive data.\n\nA list of all CSP Ports connected to the PyPort.\n\nAbstract method to request and return data from a VarPort.\n:rtype: The value of the referenced var.\n\nBlocks execution until receipt of prior ‘write’ commands (sent from\nRefPort to VarPort) have been acknowledged. Calling wait() ensures that\nthe value written by the RefPort can be received (and set) by the\nVarPort at the same time step. If wait() is not called, it is possible\nthat the value is received only at the next time step\n(non-deterministic).\n\nPreliminary implementation. Currently, a simple read() ensures the\nwrites have been acknowledged. This is inefficient and will be\noptimized later at the CspChannel level\n\nAbstract method to write data to a VarPort to set its Var.\n\ndata(ndarray,tupleofndarray,int,tupleofint) – The new value of the referenced Var.\n\nBases:[PyRefPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPort)`PyRefPort`PyRefPort\n\nPython implementation of RefPort for dense scalar data.\n\nTBD\n\n`int`int\n\nTBD\n\nBases:[PyRefPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPort)`PyRefPort`PyRefPort\n\nPython implementation of RefPort for sparse scalar data.\n\nTBD\n\n`Tuple`Tuple[`int`int,`int`int]\n\nTBD\n\nBases:[PyRefPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPort)`PyRefPort`PyRefPort\n\nPython implementation of RefPort for dense vector data.\n\nMethod to request and return data from a referenced Var using a\nPyVarPort.\n\nresult– The value of the referenced Var.\n\nndarray of shape _shape\n\nAbstract method to write data to a VarPort to set the value of the\nreferenced Var.\n\ndata(ndarray) – The data to send via _csp_send_port.\n\nBases:[PyRefPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPort)`PyRefPort`PyRefPort\n\nPython implementation of RefPort for sparse vector data.\n\nTBD\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray]\n\nTBD\n\nBases:[AbstractPyPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractPyPort)`AbstractPyPort`AbstractPyPort\n\nPython implementation of VarPort used within AbstractPyProcessModel.\n\nA PyVarPort is a Port linked to a variable Var of a Process and might be\nconnected to a RefPort of another process. It is used to get or set the\nvalue of the referenced Var across Processes. A PyVarPort is connected via\ntwo channels to a PyRefPort. One channel is used to send data from the\nPyRefPort to the PyVarPort and the other is used to receive data from the\nPyVarPort. PyVarPorts set or send the value of the linked Var (service())\ngiven the command VarPortCmd received by a connected PyRefPort.\n\nvar_name(str) – The name of the Var linked to this VarPort.\n\ncsp_send_port([CspSendPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspSendPort)CspSendPortorNone) – Csp Port used to send data to the referenced in Port (target).\n\ncsp_recv_port([CspRecvPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspRecvPort)CspRecvPortorNone) – Csp Port used to receive data from the referenced Port (source).\n\nprocess_model([AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)AbstractProcessModel) – The process model used by the process of the Port.\n\nshape(tuple,default=tuple()) – The shape of the Port.\n\nd_type(type,default=int) – The data type of the Port.\n\ntransformer([AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer,default: identity function) – Enables transforming the received data in accordance with the\nvirtual ports on the path to the PyVarPort.\n\nThe name of the Var linked to this VarPort.\n\nstr\n\nUsed to send data to the referenced Port PyRefPort (target).\n\n[CspSendPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspSendPort)CspSendPort\n\nUsed to receive data from the referenced Port PyRefPort (source).\n\n[CspRecvPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.pypychannel.CspRecvPort)CspRecvPort\n\nEnables transforming the received data in accordance with the\nvirtual ports on the path to the PyVarPort.\n\n[AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)AbstractTransformer\n\nType of PyInPort. CSP Port sends data as dense vector.\n\n[PyVarPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortVectorDense)PyVarPortVectorDense, default=None\n\nType of PyInPort. CSP Port sends data as sparse vector (data + indices),\nso only entries which have changed in a vector need to be communicated.\n\n[PyVarPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortVectorSparse)PyVarPortVectorSparse, default=None\n\nType of PyInPort. CSP Port sends data element by element for the whole\ndata structure. So the CSP channel does need less memory to transfer\ndata.\n\n[PyVarPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortScalarDense)PyVarPortScalarDense, default=None\n\nType of PyInPort. CSP Port sends data element by element, but after each\nelement the index of the data entry is also given. So only entries which\nneed to be changed need to be communicated.\n\n[PyVarPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortScalarSparse)PyVarPortScalarSparse, default=None\n\nalias of[PyVarPortScalarDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortScalarDense)`PyVarPortScalarDense`PyVarPortScalarDense\n\nalias of[PyVarPortScalarSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortScalarSparse)`PyVarPortScalarSparse`PyVarPortScalarSparse\n\nalias of[PyVarPortVectorDense](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortVectorDense)`PyVarPortVectorDense`PyVarPortVectorDense\n\nalias of[PyVarPortVectorSparse](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPortVectorSparse)`PyVarPortVectorSparse`PyVarPortVectorSparse\n\nProperty to get the corresponding CSP Ports of all connected\nPyPorts (csp_ports). The CSP Port is the low level interface of the\nbackend messaging infrastructure which is used to send and receive data.\n\nA list of all CSP Ports connected to the PyPort.\n\nAbstract method to set the value of the linked Var of the VarPort,\nreceived from the connected RefPort, or to send the value of the linked\nVar of the VarPort to the connected RefPort. The connected RefPort\ndetermines whether it will perform a read() or write() operation by\nsending a command VarPortCmd.\n\nBases:[PyVarPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPort)`PyVarPort`PyVarPort\n\nPython implementation of VarPort for dense scalar data.\n\nTBD\n\n`int`int\n\nTBD\n\n`int`int\n\nTBD\n\nBases:[PyVarPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPort)`PyVarPort`PyVarPort\n\nPython implementation of VarPort for sparse scalar data.\n\nTBD\n\n`Tuple`Tuple[`int`int,`int`int]\n\nTBD\n\n`Tuple`Tuple[`int`int,`int`int]\n\nTBD\n\nBases:[PyVarPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPort)`PyVarPort`PyVarPort\n\nPython implementation of VarPort for dense vector data.\n\nMethod to set the value of the linked Var of the VarPort,\nreceived from the connected RefPort, or to send the value of the linked\nVar of the VarPort to the connected RefPort. The connected RefPort\ndetermines whether it will perform a read() or write() operation by\nsending a command VarPortCmd.\n\nBases:[PyVarPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPort)`PyVarPort`PyVarPort\n\nPython implementation of VarPort for sparse vector data.\n\nTBD\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray]\n\nTBD\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray]\n\nTBD\n\nBases:`object`object\n\nClass to get the mapping of PyRefPort types to PyVarPort types.\n\nPyRefPorts and PyVarPorts can be implemented as different subtypes, defining\nthe format of the data to process. To connect PyRefPorts and PyVarPorts they\nneed to have a compatible data format.\nThis class maps the fitting data format between PyRefPorts and PyVarPorts.\n\nDictionary containing the mapping of compatible PyRefPort types to\nPyVarPort types.\n\ndict\n\nClass method to return the compatible PyVarPort type given the\nPyRefPort type.\n\nref_port(ty.Type[[PyRefPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyRefPort)PyRefPort]) – PyRefPort type to be mapped to a PyVarPort type.\n\nresult– PyVarPort type compatible to given PyRefPort type.\n\nty.Type[[PyVarPort](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.PyVarPort)PyVarPort]\n\nBases:`object`object\n\nHelper class to specify constants. Used for communication between\nPyRefPorts and PyVarPorts.\n\nBases:[AbstractTransformer](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.ports.AbstractTransformer)`AbstractTransformer`AbstractTransformer\n\nTransforms incoming data in way that is determined by which CSP\nport the data is received.\n\ndata(numpy.ndarray) – data that will be transformed\n\ncsp_port([AbstractCspPort](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.AbstractCspPort)AbstractCspPort) – CSP port that the data was received on\n\ntransformed_data– the transformed data\n\nnumpy.ndarray\n\nBases:`object`object\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>@implements(proc=SimpleProcess,protocol=AsyncProtocol)>>>@requires(CPU)>>>classSimpleProcessModel(PyAsyncProcessModel):>>>u=LavaPyType(int,int)>>>v=LavaPyType(int,int)\n``````\n\n``````\n>>>defrun_async(self):>>>whileTrue:>>>self.u=self.u+10>>>self.v=self.v+1000>>>ifself.check_for_stop_cmd():>>>return\n``````\n\n``````\n>>>@implements(proc=RingBuffer,protocol=LoihiProtocol)>>>@requires(CPU)>>>@tag('floating_pt')>>>classPySendModel(AbstractPyRingBuffer):>>># Ring buffer send process model.>>>s_out:PyOutPort=LavaPyType(PyOutPort.VEC_DENSE,float)>>>data:np.ndarray=LavaPyType(np.ndarray,float)\n``````\n\n``````\n>>>defrun_spk(self)->None:>>>buffer=self.data.shape[-1]>>>self.s_out.send(self.data[...,(self.time_step-1)%buffer])\n``````\n\n``````\n>>>port=PyRefPort()>>>port.write(5)>>># potentially do other stuff>>>port.wait()# waits until (all) previous writes have finished\n``````\n\n``````\nPyLearningConnection\n``````\n\n``````\nstr\n``````\n\n``````\nPyInPort\n``````\n\n``````\nndarray\n``````\n\n``````\nAbstractLearningConnection\n``````\n\n``````\nAbstractProcessModel\n``````\n\n``````\nAbstractPyProcessModel\n``````\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nLearningNeuronModel\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nAbstractPyPort\n``````\n\n``````\nAbstractPortImplementation\n``````\n\n``````\nAbstractTransformer\n``````\n\n``````\nAbstractPyIOPort\n``````\n\n``````\nPyInPortScalarDense\n``````\n\n``````\nPyInPortScalarSparse\n``````\n\n``````\nPyInPortVectorDense\n``````\n\n``````\nPyInPortVectorSparse\n``````\n\n``````\nPyOutPortScalarDense\n``````\n\n``````\nPyOutPortScalarSparse\n``````\n\n``````\nPyOutPortVectorDense\n``````\n\n``````\nPyOutPortVectorSparse\n``````\n\n``````\nPyRefPortScalarDense\n``````\n\n``````\nPyRefPortScalarSparse\n``````\n\n``````\nPyRefPortVectorDense\n``````\n\n``````\nPyRefPortVectorSparse\n``````\n\n``````\nPyRefPort\n``````\n\n``````\nPyVarPortScalarDense\n``````\n\n``````\nPyVarPortScalarSparse\n``````\n\n``````\nPyVarPortVectorDense\n``````\n\n``````\nPyVarPortVectorSparse\n``````\n\n``````\nPyVarPort\n``````\n\n``````\nDict\n``````\n\n``````\nType\n``````\n\n``````\nUnion\n``````\n\n``````\ntype\n``````\n\n``````\nint\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.model.sub.html",
    "title": "lava.magma.core.model.sub — Lava  documentation",
    "content": "Bases:[AbstractProcessModel](https://lava-nc.org/lava/lava.magma.core.model.html#lava.magma.core.model.model.AbstractProcessModel)`AbstractProcessModel`AbstractProcessModel\n\nAbstract base class for any ProcessModel that derives the behavior of\nthe Process it implements from other sub processes.\n\nSub classes must implement the __init__ method which must accept the\nProcess, that the SubProcessModel implements, as an argument. This allows\nSubProcessModel to access all process attributes such as Vars, Ports or\ninitialization arguments passed to the Process constructor via\nproc.init_args.\n\nWithin the ProcessModel constructor, other sub processes can be\ninstantiated and connected to each other or the the ports of the\nparent process.\nIn addition, Vars of sub processes can be exposed as Vars of the parent\nprocess by defining an ‘alias’ relationship between parent process and\nsub process Vars.\n\nExample:\n\nFinds and returns all sub processes of ProcessModel.\n\n`Dict`Dict[`str`str,[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess]\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>classSubProcessModel(AbstractSubProcessModel):>>>def__init__(self,proc:AbstractProcess):>>># Create one or more sub processes>>>self.proc1=Proc1(**proc.init_args)>>>self.proc2=Proc2(**proc.init_args)\n``````\n\n``````\n>>># Connect one or more ports of sub processes>>>self.proc1.out_ports.out1.connect(self.proc2.in_ports.input1)\n``````\n\n``````\n>>># Connect one or more ports of parent port with ports of sub>>># processes>>>proc.in_ports.input1.connect(self.proc1.in_ports.input1)>>>self.proc2.out_ports.output1.connect(proc.out_ports.output1)>>>self.proc1.ref_ports.ref1.connect(proc.ref_ports.ref1)\n``````\n\n``````\n>>># Define one or more alias relationships between Vars of parent>>># and sub processes>>>proc.vars.var1.alias(self.proc2.vars.var3)\n``````\n\n``````\nAbstractProcessModel\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.process.html",
    "title": "lava.magma.core.process — Lava  documentation",
    "content": "Bases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nBase class for connection Processes.\n\nThis base class holds all necessary Vars, Ports and functionality for\nonline learning in fixed and floating point simulations.\n\nInput port to receive back-propagating action potentials (BAP)\n\n[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)InPort\n\nConditional for pre-synaptic spike times (is 1 if pre-synaptic neurons\nspiked in this time-step).\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nWithin-epoch spike times of pre-synaptic neurons.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nFirst pre-synaptic trace.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nSecond pre-synaptic trace.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nConditional for post-synaptic spike times (is 1 if post-synaptic neurons\nspiked in this time-step).\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nWithin-epoch spike times of post-synaptic neurons.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nFirst post-synaptic trace.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nSecond post-synaptic trace.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nThird post-synaptic trace.\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nTag synaptic variable\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nDelay synaptic variable\n\n[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var\n\nshape(tuple,ndarray) – Shape of the connection in format (post, pre) order.\n\nlearning_rule([LoihiLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.LoihiLearningRule)LoihiLearningRule) – Learning rule which determines the parameters for online learning.\n\nBases:`ABC`ABC\n\nA member of a process has a reference to its parent process, a name\nand a shape because it is generally tensor-valued.\n\nReturns name of ProcessMember.\n\nReturns parent process of ProcessMember.\n\nReturns the size of the tensor-valued ProcessMember which is the\nproduct of all elements of its shape.\n\nBases:`ABC`ABC\n\nA singleton class that generates globally unique ids to distinguish\nother unique objects.\n\nReturns next id.\n\n`int`int\n\nResets singleton.\n\nBases:`IntEnum`IntEnum\n\nAn enumeration.\n\nBases:`object`object\n\nBase class for plastic neuron processes.\n\nThis base class holds all necessary Vars, Ports and functionality for\nonline learning in fixed and floating point simulations.\n\nshape(tuple:) – Shape of the neuron process.\n\nlearning_rule([LoihiLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.LoihiLearningRule)LoihiLearningRule) – Learning rule which determines the parameters for online learning.\n\nBases:`object`object\n\nThe notion of a Process is inspired by the Communicating Sequential\nProcess paradigm for distributed, parallel, and asynchronous programming.\n\nA Process represents the fundamental computational unit of the Lava\nframework. Processes are independent from each other in that they primarily\noperate on their own local memory while communication with other\nProcesses happens via message passing through channels at runtime. This\nmakes parallel processing safe against side effects from shared-memory\ninteraction. Nevertheless, shared-memory interaction between Processes is\nalso supported. Lava Processes are built for executing on a distributed and\nheterogeneous hardware platform consisting of different compute resources.\nCompute resources can be conventional CPUs, GPUs, embedded CPUs, or\nneuromorphic cores.\n\nLava Processes consist of the following key components:\n\nState: A Lava Process has internal state that is realized through\nLava Vars (variables). Vars can be initialized by the user\nand are mutable during execution either as a result of Process code or\nuser interaction.\n\nPorts: Lava Processes communicate with their environment or other\nProcesses solely via messages. Messages are sent between ports via\nchannels. Processes may have one or more input, output, or reference\nports.\n\nOutPorts can only be connected to one or more InPorts and\ncommunication is uni-directional.\n\nInPorts can receive inputs from one or more OutPorts.\n\nRefPorts can be connected to Vars of remote Processes and allow the\nProcess with the RefPort to access another Process’ internal state\ndirectly and bi-directionally. This type of shared-memory interaction\nis potentially unsafe and should be used with caution!\n\nAPI: A Lava Process can expose a public API to configure it or interact\nwith it at setup or during execution interactively. In general,\nthe public state Vars can be considered part of a Process’ API.\n\nCrucially, the behavior of a Process is not specified by the Process itself\nbut by separate ProcessModels that implement the behavior\nof a Process at different levels of abstraction, in different\nprogramming languages, and for different compute resources, such as\nCPUs or neuromorphic cores. A single Process can support multiple\nProcessModels of the same type or of different types. Please refer to\nthe documentation of AbstractProcessModel for more details.\n\nDevelopers creating new Processes need to inherit from the\nAbstractProcess interface. Any internal Vars or Ports must be initialized\nwithin its init method:\n\nVars should only be used for dynamic state parameters that play a role in\nthe behavioral model of the Process or for static configuration parameters\nthat affect the behavior of the Process (e.g., the membrane potential of a\nLIF neuron).\nMeta parameters that are only needed for communicating information between\nthe Process and its ProcessModels (e.g., shape) should not become a Var.\nThey should be passed to the Process as keyword arguments and\nthen need to be passed to the __init__ method of AbstractProcess,\nas is done with ‘shape’ and ‘name’ in the example above. All such keyword\narguments are stored in the member ‘proc_params’, which is passed on to all\nProcessModels of the Process.\n\nVars can be initialized with user-provided values and Processes can be\nconnected to other Processes via their ports:``p1=NewProcess(<in_args>)p2=NewProcess(<in_args>)p1.out_port1.connect(p2.in_port1)```p1=NewProcess(<in_args>)p2=NewProcess(<in_args>)p1.out_port1.connect(p2.in_port1)`For more information on connecting Processes, see the documentation of\nInPort, OutPort, and RefPort.\n\nOnce a concrete Process has been created and connected with other\nProcesses it needs to be compiled before it is ready for execution.\nA Process is compiled by the Lava Compiler while execution is controlled\nby the Lava Runtime. While the Lava Compiler and Runtime can be created\nmanually to compile and run a Process, the AbstractProcess interface\nprovides short-hand methods to compile and run a Process without\ninteracting with the compiler or runtime directly. In particular running\nan uncompiled Process will compile a Process automatically.\nSince all Processes created in a session usually form a system, calling\n‘compile(..)’ or ‘run(..)’ on any of them compiles and runs all of them\nautomatically.\n\nAt compile time, the user must provide the Lava Compiler with a\nspecific instance of a RunConfig class. A RunConfig class represents a set\nof rules that allows the compiler to select one and only one ProcessModel\nof a specific Process to be compiled for execution with specific compute\nresources. See the documentation on RunConfigs for more information how\nto create and customize such RunConfigs.\n\nFinally, in order to run a Process, a RunCondition must be provided. A\nRunCondition such as ‘RunSteps’ or ‘RunContinuous’ specifies until when a\nProcess should be executed.\n\nSince Processes generally run asynchronously and in parallel,\nthe execution of a set of Processes can either be paused or stopped by\ncalling the corresponding ‘pause()’ or ‘stop()’ methods.\n\nIn order to save time setting up Processes for future use, Processes\ncan also be saved and reloaded from disk.\n\nproc_params– Any keyword arguments that get passed from child\nProcesses will be stored in the AbstractProcess member\n‘proc_params’ and passed to all ProcessModels. ‘proc_params’ can\nbe further added to or modified in order to pass parameters\nto ProcessModels that are not represented by dynamic state\nvariables (Vars) of the Process.\n\nname(str,optional) – Name of the Process. Default is ‘Process_ID’, where ID\nis an integer value that is determined automatically.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig,optional) – Configuration options for logging.\n\nOn destruction, terminate Runtime automatically to\nfree compute resources.\n\nExecuted when Process enters a “with” block of a context manager.\n\nStop the runtime when exiting “with” block of a context manager.\n\nCompiles this and any process connected to this process and\nreturns the resulting Executable that can either be serialized or\npassed to Runtime.\n\nrun_cfg([RunConfig](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.RunConfig)RunConfig) – Used by the compiler to select a ProcessModel for each Process.\n\ncompile_config(Dict[str,Any],optional) – Configuration options for the Compiler and SubCompilers.\n\n[Executable](https://lava-nc.org/lava/lava.magma.compiler.html#lava.magma.compiler.executable.Executable)`Executable`Executable\n\nCreates a runtime for this process and all connected processes by\ncompiling the process to an executable and assigning that executable to\nthe process and connected processes.\n\nSee Process.run() for information on Process blocking, which must be\nspecified in the run_cfg passed to create_runtime.\n\nrun_cfg([RunConfig](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.RunConfig)RunConfig,optional) – Used by the compiler to select a ProcessModel for each Process.\nMust be provided when Processes have to be compiled, can be\nomitted otherwise.\n\ncompile_config(Dict[str,Any],optional) – Configuration options for the Compiler and SubCompilers.\n\nReturn folded view process\n\nReturns True if process has been compiled.\n\nReturns True, is this Process is a sub process of ‘proc’.\n\nLoads and de-serializes Process from disk.\n\nReturn model\n\nReturn model class\n\nPauses process execution while running in non-blocking mode.\n\nRegisters other processes as sub processes of this process.\n\nExecutes this and any connected Processes that form a Process\nnetwork. If any Process has not been compiled, it is automatically\ncompiled before execution.\n\nWhen condition.blocking is True, the Processes are executed for as long\nas the RunCondition is satisfied. Otherwise, the method run() returns\nimmediately while the Processes are executed. In this case,\nthe methods wait(), pause(), and stop() can be used to\ninteract with the Runtime:\n\nwait() blocks execution for as long as the RunCondition is\nsatisfied.\n\npause() pauses execution as soon as possible and returns\ncontrol back to the user.\n\nstop() terminates execution and releases control of all\ninvolved compute nodes.\n\nIf a run has been suspended by either pause() or a RunCondition\nbeing no longer satisfied, run() can be called again to resume\nexecution from the current state.\n\nNOTE: run_cfg will be ignored when re-running a previously compiled\nprocess.\n\ncondition([AbstractRunCondition](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_conditions.AbstractRunCondition)AbstractRunCondition) – Specifies for how long to run the Process.\n\nrun_cfg([RunConfig](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_configs.RunConfig)RunConfig,optional) – Used by the compiler to select a ProcessModel for each Process.\nMust be provided when Processes have to be compiled, can be\nomitted otherwise.\n\ncompile_config(Dict[str,Any],optional) – Configuration options for the Compiler and SubCompilers.\n\nReturns current Runtime or None if no Runtime exists.\n\nSerializes and saves Process in current stage to disk.\nSerialization at different levels will be possible: After\npartitioning, after mapping, ...\n\nTerminates process execution by releasing all allocated compute\nnodes.\n\nValidates that any aliased Var is a member of a Process that is a\nstrict sub-Process of this Var’s Process.\n\nWaits until end of process execution or for as long as\nRunCondition is met by blocking execution at command line level.\n\nBases:`object`object\n\nRepresents a collection of objects. Member objects can be accessed via\ndot-notation (SomeCollection.some_object_name). Collection also offers an\niterator to iterate all member objects.\n\nprocess([AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess) – Parent Process that holds the Collection\n\nname(str) – Name of the Collection\n\nAdds members to a Collection.\n\nmembers(Dict[str,mem_type]) – Dictionary of Collection members, where the key is\nthe string name of the member and the value is the member.\n\nReturns True if member is in collection.\n\n`bool`bool\n\nReturns True if Collection has no members.\n\n`bool`bool\n\nalias of`Union`Union[[InPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.InPort)`InPort`InPort,[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)`OutPort`OutPort,[RefPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.RefPort)`RefPort`RefPort,[VarPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.VarPort)`VarPort`VarPort,[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)`Var`Var,[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess]\n\nReturns the names of Collection members.\n\nReturns the members of the Collection.\n\nBases:`object`object\n\nConfiguration options for logging that can be passed into a Process.\n\nBases:`object`object\n\nWrapper around a dictionary that is used to pass parameters from a\nProcess to its ProcessModels. The dictionary can be filled with an\ninitial dictionary of parameters. Any further changes via the __setitem__\nmethod may not overwrite existing values. To overwrite a value, use the\nmethod overwrite().\n\ninitial_parameters(Dict[str,Any]) – Initial dictionary of parameters for a Process/ProcessModel.\n\nSets a key-value pair without checking whether the key is already\npresent in ProcessParameters.\n\n`None`None\n\nBases:`type`type\n\nMetaclass for AbstractProcess that overwrites __call__() in order to\ncall _post_init() initializer method after __init__() of any sub class\nis called.\n\nBases:[IdGeneratorSingleton](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.interfaces.IdGeneratorSingleton)`IdGeneratorSingleton`IdGeneratorSingleton\n\nProcessServer singleton keeps track of all existing processes and issues\nnew globally unique process ids.\n\nReturns number of processes created so far.\n\nRegisters a process with ProcessServer.\n\n`int`int\n\nResets the ProcessServer to initial state.\n\nBases:[AbstractProcessMember](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.interfaces.AbstractProcessMember)`AbstractProcessMember`AbstractProcessMember\n\nRepresents a Lava variable. A Var implements the state of a Process and\nis part of its public user interface. Vars have the following properties:\n\nVars are numeric objects: Typically vars represent ints, float data types.\n\nVars are tensor-valued: In general Vars represent multiple numeric\nvalues not just scalar objects with a shape.\n\nVars can be initialized with numeric objects with a dimensionality\nequal or less than specified by its shape. The initial value will be\nbroadcast to the shape of the Var at compile time.\n\nVars have a name: The Variable name will be assigned by the parent\nprocess of a Var.\n\nVars are mutable at runtime.\n\nVars are owned by a Process but shared-memory access by other Process\nis possible though should be used with caution.\n\nHow to enable interactive Var access?\n\nVar can access Runtime via parent Process.\n\nThe compiler could have prepared the Executable with mapping\ninformation where each Var of a Process got mapped to. I.e. these can\njust be the former ExecVars. So the ExecVars are just stored inside the\nExecutable.\n\nAlternatively, the Executable stores a map from var_id -> ExecVar\n\nEstablishes an ‘alias’ relationship between this and ‘other_var’.\nThe other Var must be a member of a strict sub processes of this\nVar’s parent process which might be instantiated within a\nSubProcessModel that implements this Var’s parent process.\nBoth, this and ‘other_var’ must have the same ‘shape’ and be both\n‘shareable’ or not.\n\n:param : param other_var: The other Var that this Var is an alias for.\n:param Calls to Var.set:\n:type Calls to Var.set: ..) or Var.get(\n:param Var.:\n\nGets and returns value of Var. If this Var aliases another Var,\nthen get() is delegated to aliased Var.\n\n`ndarray`ndarray\n\nReturn model.\n\nSets value of Var. If this Var aliases another Var, then set(..) is\ndelegated to aliased Var.\n\nValidates that any aliased Var is a member of a Process that is a\nstrict sub-Process of this Var’s Process.\n\nBases:[IdGeneratorSingleton](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.interfaces.IdGeneratorSingleton)`IdGeneratorSingleton`IdGeneratorSingleton\n\nVarServer singleton keeps track of all existing Vars and issues\nnew globally unique Var ids.\n\nReturns number of vars created so far.\n\nRegisters a Var with VarServer.\n\n`int`int\n\nResets the VarServer to initial state.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>classNewProcess(AbstractProcess):>>>def__init__(self,shape,name):>>>super().__init__(shape=shape,name=name)>>>self.var1=Var(shape=shape)>>>self.in_port1=InPort(shape=shape)>>>self.out_port1=OutPort(shape=shape)>>>...\n``````\n\n``````\n>>>Executable---------->>>|>>>Var->Process->Runtime->RuntimeService->ProcModel->Var\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nExecutable\n``````\n\n``````\nInPort\n``````\n\n``````\nOutPort\n``````\n\n``````\nRefPort\n``````\n\n``````\nVarPort\n``````\n\n``````\nVar\n``````\n\n``````\nstr\n``````\n\n``````\nint\n``````\n\n``````\nbool\n``````\n\n``````\nIdGeneratorSingleton\n``````\n\n``````\nOptional\n``````\n\n``````\nProcessServer\n``````\n\n``````\nAbstractProcessMember\n``````\n\n``````\nVarServer\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.process.ports.html",
    "title": "lava.magma.core.process.ports — Lava  documentation",
    "content": "Bases:`Exception`Exception\n\nRaised when the axis over which ports should be concatenated is out of\nbounds.\n\nBases:`Exception`Exception\n\nRaised when incompatible ports are tried to be concatenated.\n\nBases:`Exception`Exception\n\nRaised when an attempt is made to create more than one connection\nbetween source and destination port.\n\nBases:`Exception`Exception\n\nRaised when new port shape is incompatible with old shape.\n\nBases:`Exception`Exception\n\nRaised when indices in transpose axes are out of bounds for the old\nshape dimension.\n\nBases:`Exception`Exception\n\nRaised when transpose axes is incompatible with old shape dimension.\n\nBases:`Exception`Exception\n\nRaised when an attempt is made to connect a RefPort or VarPort to a\nnon-sharable Var.\n\nBases:`ABC`ABC\n\nInterface for destination ports such as InPorts and VarPorts in which\nconnections terminate.\nThis class needs no implementation and only serves to establish a clear\ntype hierarchy needed for validating connections.\n\nBases:[AbstractPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractPort)`AbstractPort`AbstractPort\n\nAbstract base class for InPorts and OutPorts.\nThis class needs no implementation and only serves to establish a clear\ntype hierarchy needed for validating connections.\n\nBases:[AbstractProcessMember](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.interfaces.AbstractProcessMember)`AbstractProcessMember`AbstractProcessMember\n\nAbstract base class for any type of port of a Lava Process.\n\nPorts of a process can be connected to ports of other processes to enable\nmessage-based communication via channels. Sub classes of AbstractPort\nonly facilitate connecting to other compatible ports. Message-passing\nitself is only handled after compilation at runtime by port\nimplementations within the corresponding ProcessModel.\n\nPorts are tensor-valued, have a name and a parent process. In addition,\na port may have zero or more input and output connections that contain\nreferences to ports that connect to this port or that this port connects\nto. Port to port connections are directional and connecting ports,\neffectively means to associate them with each other as inputs or outputs.\nThese connections, imply an a-cyclic graph structure that allows the\ncompiler to infer connections between processes.\n\nshape(tuple[int,...]) – Determines the number of connections created by this port\n\nConcatenates this port with other ports in given order along given\naxis by deriving and returning a new virtual ConcatPort. This implies\nresulting ConcatPort can only be forward connected to another port.\nAll ports must have the same shape outside of the concatenation\ndimension.\n\nports(ty.Union[\"AbstractPort\",ty.List[\"AbstractPort\"]]) – Port(s) that will be concatenated after this port.\n\naxis(int) – Axis/dimension along which ports are concatenated.\n\n[ConcatPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.ConcatPort)`ConcatPort`ConcatPort\n\nFlattens this port to a (N,)-shaped port by deriving and returning\na new virtual ReshapePort with a N equal to the total number of\nelements of this port.\n\n[ReshapePort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.ReshapePort)`ReshapePort`ReshapePort\n\nReturns the list of all destination ports that this port connects to\neither directly or indirectly (through other ports).\n\n`List`List[[AbstractDstPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractDstPort)`AbstractDstPort`AbstractDstPort]\n\nReturns the incoming transformation functions for all incoming\nconnections.\n\nA dictionary that maps from the ID of an incoming source port to\nthe list of transformation functions implementing the virtual\nports on the way to the current port. The transformation\nfunctions in the list are sorted from source to destination port.\n\ndict(list(functools.partial))\n\nReturns the list of all incoming virtual ports in order from\nsource to the current port.\n\nThe string of the tuple is the ID of the source port, the list\nis the list of all incoming virtual ports, sorted from source to\ndestination port.\n\ntuple(str, list(AbstractVirtualPorts))\n\nReturns the outgoing transformation functions for all outgoing\nconnections.\n\nA dictionary that maps from the ID of a destination port to\nthe list of transformation functions implementing the virtual\nports on the way from the current port. The transformation\nfunctions in the list are sorted from source to destination port.\n\ndict(list(functools.partial))\n\nReturns the list of all outgoing virtual ports in order from\nthe current port to the destination port.\n\nThe string of the tuple is the ID of the destination port, the list\nis the list of all outgoing virtual ports, sorted from source to\ndestination port.\n\ntuple(str, list(AbstractVirtualPorts))\n\nReturns the list of all source ports that connect either directly\nor indirectly (through other ports) to this port.\n\n`List`List[[AbstractSrcPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractSrcPort)`AbstractSrcPort`AbstractSrcPort]\n\nReshapes this port by deriving and returning a new virtual\nReshapePort with the new shape. This implies that the resulting\nReshapePort can only be forward connected to another port.\n\nnew_shape(tuple[int,...]) – New shape of port. Number of total elements must not change.\n\n[ReshapePort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.ReshapePort)`ReshapePort`ReshapePort\n\nPermutes the tensor dimension of this port by deriving and returning\na new virtual TransposePort the new permuted dimension. This implies\nthat the resulting TransposePort can only be forward connected to\nanother port.\n\naxes(ty.Optional[ty.Union[ty.Tuple[int,...],ty.List]]) – Order of permutation. Number of total elements and number of\ndimensions must not change.\n\n[TransposePort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.TransposePort)`TransposePort`TransposePort\n\nBases:[AbstractPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractPort)`AbstractPort`AbstractPort\n\nAbstract base class for RefPorts and VarPorts.\nThis class needs no implementation and only serves to establish a clear\ntype hierarchy needed for validating connections.\n\nBases:`ABC`ABC\n\nInterface for source ports such as OutPorts and RefPorts from which\nconnections originate.\nThis class needs no implementation and only serves to establish a clear\ntype hierarchy needed for validating connections.\n\nBases:[AbstractPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractPort)`AbstractPort`AbstractPort\n\nAbstract base class interface for any type of port that merely serves\nto transform the properties of a user-defined port.\n\nConnects this virtual port to other port(s).\n\nports(ty.Union[\"AbstractPort\",ty.List[\"AbstractPort\"]]) – The port(s) to connect to. Connections from an IOPort to a RVPort\nand vice versa are not allowed.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nReturns a function pointer that implements the backward (bwd)\ntransformation of the virtual port.\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nReturns a function pointer that implements the forward (fwd)\ntransformation of the virtual port.\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nReturns parent process of parent port that this VirtualPort was\nderived from.\n\nBases:[AbstractVirtualPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractVirtualPort)`AbstractVirtualPort`AbstractVirtualPort\n\nA ConcatPort is a virtual port that allows to concatenate multiple\nports along given axis into a new port before connecting to another port.\nThe shape of all concatenated ports outside of the concatenation\ndimension must be the same.\nIt is used by the compiler to map the indices of the underlying\ntensor-valued data array from the derived to the new shape.\n\nReturns a function pointer that implements the backward (bwd)\ntransformation of the virtual port.\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nReturns a function pointer that implements the forward (fwd)\ntransformation of the virtual port.\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nBases:[VarPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.VarPort)`VarPort`VarPort\n\nSub class for VarPort to identify implicitly created VarPorts when\na RefPort connects directly to a Var.\n\nBases:[AbstractIOPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractIOPort)`AbstractIOPort`AbstractIOPort,[AbstractDstPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractDstPort)`AbstractDstPort`AbstractDstPort\n\nInput ports are members of a Lava Process and can be connected to\nother ports to facilitate receiving of messages via channels.\n\nInPorts can receive connections from other OutPorts of peer processes\nor from other InPorts of processes that contain this InPort’s parent\nprocess as a sub process. Similarly, InPorts can connect to other InPorts\nof nested sub processes.\n\nshape(tuple[int,...]) – Determines the number of connections created by this port.\n\nreduce_op(ty.Optional[ty.Type[[AbstractReduceOp](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.reduce_ops.AbstractReduceOp)AbstractReduceOp]]) – Operation to be applied on incoming data, default: None.\n\nConnects this InPort to other InPort(s) of a nested process. InPorts\ncannot connect to other OutPorts.\n\nports(ty.Union[\"InPort\",ty.List[\"InPort\"]]) – The InPort(s) to connect to.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nConnects other OutPort(s) to this InPort or connects other\nInPort(s) of parent process to this InPort.\n\nports(ty.Union[\"AbstractIOPort\",ty.List[\"AbstractIOPort\"]]) – The AbstractIOPort(s) that connect to this InPort.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nBases:[AbstractIOPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractIOPort)`AbstractIOPort`AbstractIOPort,[AbstractSrcPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractSrcPort)`AbstractSrcPort`AbstractSrcPort\n\nOutput ports are members of a Lava Process and can be connected to\nother ports to facilitate sending of messages via channels.\n\nOutPorts connect to other InPorts of peer processes or to other OutPorts of\nprocesses that contain this OutPort’s parent process as a sub process.\nSimilarly, OutPorts can receive connections from other OutPorts of nested\nsub processes.\n\nConnects this OutPort to other InPort(s) of another process\nor to OutPort(s) of its parent process.\n\nports(ty.Union[\"AbstractIOPort\",ty.List[\"AbstractIOPort\"]]) – The AbstractIOPort(s) to connect to.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nConnects other OutPort(s) of a nested process to this OutPort.\nOutPorts cannot receive connections from other InPorts.\n\nports(ty.Union[\"OutPort\",ty.List[\"OutPort\"]]) – The OutPorts(s) that connect to this OutPort.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nBases:[AbstractRVPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractRVPort)`AbstractRVPort`AbstractRVPort,[AbstractSrcPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractSrcPort)`AbstractSrcPort`AbstractSrcPort\n\nRefPorts are members of a Lava Process and can be connected to\ninternal Lava Vars of other processes to facilitate direct shared memory\naccess to those processes.\n\nShared-memory-based communication can have side-effects and should\ntherefore be used with caution.\n\nRefPorts connect to other VarPorts of peer processes or to other RefPorts\nof processes that contain this RefPort’s parent process as a sub process\nvia the connect(..) method..\nSimilarly, RefPorts can receive connections from other RefPorts of nested\nsub processes via the connect_from(..) method.\n\nHere, VarPorts only serve as a wrapper for Vars. VarPorts can be created\nstatically during process definition to explicitly expose a Var for\nremote memory access (which might be safer).\nAlternatively, VarPorts can be created dynamically by connecting a\nRefPort to a Var via the connect_var(..) method.\n\nConnects this RefPort to other VarPort(s) of another process\nor to RefPort(s) of its parent process.\n\nports(ty.Union[\"AbstractRVPort\",ty.List[\"AbstractRVPort\"]]) – The AbstractRVPort(s) to connect to.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nConnects other RefPort(s) of a nested process to this RefPort.\nRefPorts cannot receive connections from other VarPorts.\n\nports(ty.Union[\"RefPort\",ty.List[\"RefPort\"]]) – The RefPort(s) that connect to this RefPort.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nConnects this RefPort to Lava Process Var(s) to facilitate shared\nmemory access.\n\nvariables(ty.Union[[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var,ty.List[[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var]]) – Var or list of Vars to connect to.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nCreates and returns an ImplicitVarPort for the given Var.\n\n[ImplicitVarPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.ImplicitVarPort)`ImplicitVarPort`ImplicitVarPort\n\nReturns destination Vars this RefPort is connected to.\n\n`List`List[[Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)`Var`Var]\n\nBases:[AbstractVirtualPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractVirtualPort)`AbstractVirtualPort`AbstractVirtualPort\n\nA ReshapePort is a virtual port that allows to change the shape of a\nport before connecting to another port.\nIt is used by the compiler to map the indices of the underlying\ntensor-valued data array from the derived to the new shape.\n\nReturns a function pointer that implements the backward (bwd)\ntransformation of the ReshapePort, which reshapes incoming data to\na new shape (the shape of the source Process).\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nReturns a function pointer that implements the forward (fwd)\ntransformation of the ReshapePort, which reshapes incoming data to\na new shape (the shape of the destination Process).\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nBases:[AbstractVirtualPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractVirtualPort)`AbstractVirtualPort`AbstractVirtualPort\n\nA TransposePort is a virtual port that allows to permute the dimensions\nof a port before connecting to another port.\nIt is used by the compiler to map the indices of the underlying\ntensor-valued data array from the derived to the new shape.\n\nExample\n\nout_port = OutPort((2, 4, 3))\nin_port = InPort((3, 2, 4))\nout_port.transpose([3, 1, 2]).connect(in_port)\n\nReturns a function pointer that implements the backward (bwd)\ntransformation of the TransposePort, which transposes (permutes)\nincoming data according to a specific order of axes (to match the\nsource Process).\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nReturns a function pointer that implements the forward (fwd)\ntransformation of the TransposePort, which transposes (permutes)\nincoming data according to a specific order of axes (to match the\ndestination Process).\n\nfunction_pointer– a function pointer that can be applied to incoming data\n\nfunctools.partial\n\nBases:[AbstractRVPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractRVPort)`AbstractRVPort`AbstractRVPort,[AbstractDstPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.AbstractDstPort)`AbstractDstPort`AbstractDstPort\n\nVarPorts are members of a Lava Process and act as a wrapper for\ninternal Lava Vars to facilitate connections between RefPorts and Vars\nfor shared memory access from the parent process of the RefPort to\nthe parent process of the Var.\n\nShared-memory-based communication can have side-effects and should\ntherefore be used with caution.\n\nVarPorts can receive connections from other RefPorts of peer processes\nor from other VarPorts of processes that contain this VarPort’s parent\nprocess as a sub process via the connect(..) method. Similarly, VarPorts\ncan connect to other VarPorts of nested sub processes via the\nconnect_from(..) method.\n\nVarPorts can either be created in the constructor of a Process to\nexplicitly expose a Var for shared memory access (which might be safer).\nAlternatively, VarPorts can be created dynamically by connecting a\nRefPort to a Var via the RefPort.connect_var(..) method.\n\nConnects this VarPort to other VarPort(s) of a nested process.\nVarPorts cannot connect to other RefPorts.\n\nports(ty.Union[\"VarPort\",ty.List[\"VarPort\"]]) – The VarPort(s) to connect to.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nConnects other RefPort(s) to this VarPort or connects other\nVarPort(s) of parent process to this VarPort.\n\nports(ty.Union[\"AbstractRVPort\",ty.List[\"AbstractRVPort\"]]) – The AbstractRVPort(s) that connect to this VarPort.\n\nconnection_configs(ConnectionConfigs) – Configuration for this connection. See “ConnectionConfig” class.\n\nGenerates a string-based ID for a port that makes it identifiable\nwithin a network of Processes.\n\nproc_id(int) – ID of the Process that the Port is associated with\n\nport_name(str) – name of the Port\n\nport_id– ID of a port\n\nstr\n\nChecks that both lists are disjoint.\n\nIf ‘obj’ is not a list, converts ‘obj’ into [obj].\n\n`List`List[`Any`Any]\n\nBases:`ABC`ABC\n\nReduce operations are required by InPorts to specify how date from\nmultiple OutPorts connected to the same InPorts gets integrated.\n\nBases:[AbstractReduceOp](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.reduce_ops.AbstractReduceOp)`AbstractReduceOp`AbstractReduceOp\n\nReduceOp to indicate that multiple inputs to same InPort should be\nadded.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractPort\n``````\n\n``````\nAbstractProcessMember\n``````\n\n``````\nConcatPort\n``````\n\n``````\nReshapePort\n``````\n\n``````\nAbstractDstPort\n``````\n\n``````\nAbstractSrcPort\n``````\n\n``````\nTransposePort\n``````\n\n``````\nAbstractVirtualPort\n``````\n\n``````\nVarPort\n``````\n\n``````\nAbstractIOPort\n``````\n\n``````\nAbstractRVPort\n``````\n\n``````\nImplicitVarPort\n``````\n\n``````\nVar\n``````\n\n``````\nAbstractReduceOp\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.sync.html",
    "title": "lava.magma.core.sync — Lava  documentation",
    "content": "Bases:`object`object\n\nSpecify to run a group ofProcessesProcessesusing a specificSyncProtocolSyncProtocol.\n\nASyncProtocolSyncProtocoldefines how and whenProcessesProcessesare synchronized and\ncommunication is possible. TheSyncDomainSyncDomainmaps a list ofProcessesProcessesto\na givenSyncProtocolSyncProtocol.\n\nname(str) – Name of the SyncDomain.\n\nprotocol([AbstractSyncProtocol](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.protocol.AbstractSyncProtocol)AbstractSyncProtocol) – SyncProtocol theProcessesProcessesare mapped to.\n\nprocesses(ty.List[[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)AbstractProcess]) – List ofProcessesProcessesto run in the given SyncProtocol.\n\nBases:`object`object\n\nBase class forSyncProtocolsSyncProtocols.\n\nASyncProtocolSyncProtocoldefines how and when the Processes in aSyncDomainSyncDomainare\nsynchronized and communication is possible. SyncProtocols need to implement\ntheruntime_service()’ method which returns a map between hardware\nresources and the corresponding `RuntimeServicesruntime_service()’ method which returns a map between hardware\nresources and the corresponding `RuntimeServices.\n\nFor example:\n\nThe phases of execution and synchronizations points are implemented in\nthe specificRuntimeServiceRuntimeService.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>@property>>>defruntime_service(self)->ty.Dict[Resource,AbstractRuntimeService]:>>>return{CPU:LoihiPyRuntimeService,>>>LMT:NxSdkRuntimeService,>>>NeuroCore:NxSdkRuntimeService,>>>Loihi1NeuroCore:NxSdkRuntimeService,>>>Loihi2NeuroCore:NxSdkRuntimeService}\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.core.sync.protocols.html",
    "title": "lava.magma.core.sync.protocols — Lava  documentation",
    "content": "Bases:[AbstractSyncProtocol](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.protocol.AbstractSyncProtocol)`AbstractSyncProtocol`AbstractSyncProtocol\n\nProtocol to run processes asynchronously.\n\nWith the AsyncProtocol, Processes are executed without synchronization\non specific phases. This means that the Processes could run with\na varying speed and message passing is possible at any time.\n\nAsyncProtocolAsyncProtocolis currently only implemented for execution on CPU using\nthePyAsyncProcessModelPyAsyncProcessModelin which therun_async()run_async()function must be\nimplemented to define the behavior of the underlyingProcessProcess.\n\nFor example:\n\nBases:[AbstractSyncProtocol](https://lava-nc.org/lava/lava.magma.core.sync.html#lava.magma.core.sync.protocol.AbstractSyncProtocol)`AbstractSyncProtocol`AbstractSyncProtocol\n\nA ProcessModel implementing this synchronization protocol adheres to the\nphases of execution of the neuromorphic processor Loihi.\n\nEach phase is implemented by a dedicated method. Loihi’s phases of\nexecution are listed below in order of execution. A ProcessModel adhering\nto this protocol must implement each phase in a dedicated method.\nAdditionally, for each phase (except the spiking phase), a “guard” method\nmust be implemented that determines whether its respective phase is\nexecuted in the given time step.\n\nFor example:\nTo execute the post management phase (‘run_post_mgmt()’), the functionpost_guard()post_guard()must return True.\n\nPhasesThe phases are executed in this order:\n\nSynaptic input is served, neuron states are updated and\noutput spikes are generated and delivered. This method is always\nexecuted and does not have a corresponding guard method.\n\n‘run_spk()’ -> None\n\nNone\n\nMemory is consolidated before the learning phase.\nIn order to jump into this phase, ‘pre_guard’ andlearn_guardlearn_guardmust return True.\n\n‘run_pre_mgmt()’ -> None\n\nlearn_guard()learn_guard()-> None, andpre_guard()pre_guard()-> None\n\nActivity traces are calculated, learning rules are applied and\nparameters (weights, thresholds, delays, tags, etc) are updated.\nIn order to jump into this phase, ‘lrn_guard’ must return True.\n\n‘run_lrn()’ -> None\n\nlearn_guard()learn_guard()-> None\n\nMemory is consolidated after learning phase. Read and write access\nto neuron states are safe now.\nIn order to jump into this phase ‘post_guard’ must return True.\n\n‘run_post_mgmt()’ -> None\n\nlearn_guard()learn_guard()-> None\n\nMemory of the host system is consolidated.\nIn order to jump into this phase ‘host_guard’ must return True.\n\n‘run_host_mgmt()’ -> None\n\nhost_guard()host_guard()-> None\n\nReturn RuntimeService.\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`tuple`tuple\n\nAlias for field number 1\n\nAlias for field number 0\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>@implements(proc=SimpleProcess,protocol=AsyncProtocol)>>>@requires(CPU)>>>classSimpleProcessModel(PyAsyncProcessModel):>>>u=LavaPyType(int,int)>>>v=LavaPyType(int,int)\n``````\n\n``````\n>>>defrun_async(self):>>>whileTrue:>>>self.u=self.u+10>>>self.v=self.v+1000>>>ifself.check_for_stop_cmd():>>>return\n``````\n\n``````\n>>>defpost_guard(self)->bool:>>>returnTrue\n``````\n\n``````\n>>>defrun_post_mgmt(self)->None:>>># i.e. read out a variable from a neurocore\n``````\n\n``````\nAbstractSyncProtocol\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.html",
    "title": "Magma — Lava  documentation",
    "content": "Magma is the sub-package of Lava providing the main components of Lava\n\n[Magma core](https://lava-nc.org/lava/lava.magma.core.html#module-lava.magma.core)Magma corebase-classes for[Processes](https://lava-nc.org/lava/lava.magma.core.process.html#lava-magma-core-process-process)Processes,[ProcessModels](https://lava-nc.org/lava/lava.magma.core.model.html#lava-magma-core-model-model)ProcessModelsand[SyncDomains](https://lava-nc.org/lava/lava.magma.core.sync.html#lava-magma-core-sync-domain)SyncDomainsas well as basic behavior and functionality\n\n[Magma compiler](https://lava-nc.org/lava/lava.magma.compiler.html#lava-magma-compiler-compiler)Magma compilerbuilding the network and the communications channels\n\n[Magma runtime](https://lava-nc.org/lava/lava.magma.runtime.html#lava-magma-runtime-runtime)Magma runtimeproviding a control interface for the user\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.runtime.html",
    "title": "lava.magma.runtime — Lava  documentation",
    "content": "Defines message tokens for Actions (Commands) and Responses. Also defines\nhelper functions to convert scalar values to these message tokens.\n\nBases:`object`object\n\nSignifies the Mgmt Command being sent between two actors. These may be\nbetween runtime and runtime_service or the runtime_service\nand process model.\n\nSignifies Read a variable\n\nSignifies a PAUSE command from one actor to another\n\nSignifies a RUN command for 0 timesteps from one actor to another. Any\nnon negative integer signifies a run command\n\nSignifies Write a variable\n\nSignifies a STOP command from one actor to another\n\nBases:`object`object\n\nSignifies the response to a Mgmt command. This response can be sent\nby any actor upon receiving a Mgmt command\n\nSignfies Ack or Finished with the Command\n\nSignifies Error raised\n\nSignifies Execution State to be Paused\n\nSignifies Request of PAUSE\n\nSignifies Request of STOP\n\nSignifies Completion of Set Var\n\nSignifies Termination\n\nHelper function to compare two np arrays created by enum_to_np.\n\na(`array`array) – 1-D array created by enum_to_np\n\nb(`array`array) – 1-D array created by enum_to_np\n\n`bool`bool\n\nTrue if the two arrays are equal\n\nHelper function to convert an int (or EnumInt) or a float to a single value\nnp array so as to pass it via the message passing framework. The dtype of\nthe np array is specified by d_type with the default of np.int32.\n\nvalue(`Union`Union[`int`int,`float`float]) – value to be converted to a 1-D array\n\nd_type(`type`type) – type of the converted np array\n\n`array`array\n\nnp array with the value\n\nBases:`object`object\n\nLava runtime which consumes an executable and run\nrun_condition. Exposes\nthe APIs to start, pause, stop and wait on an execution. Execution could\nbe blocking and non-blocking as specified by the run\nrun_condition.\n\nOn destruction, terminate Runtime automatically to\nfree compute resources.\n\nInitialize the runtime on entering “with” block of a context manager.\n\nStop the runtime when exiting “with” block of a context manager.\n\nGets value of a variable with id ‘var_id’.\n\n`ndarray`ndarray\n\nInitializes the runtime\n\nJoin all ports and processes\n\nReturns the selected NodeCfg.\n\nPauses the execution\n\nSets value of a variable with id ‘var_id’.\n\nGiven a run condition, starts the runtime\n\nrun_condition([AbstractRunCondition](https://lava-nc.org/lava/lava.magma.core.html#lava.magma.core.run_conditions.AbstractRunCondition)`AbstractRunCondition`AbstractRunCondition) – AbstractRunCondition\n\nNone\n\nStops an ongoing or paused run.\n\nWaits for existing run to end. This is helpful if the execution\nwas started in non-blocking mode earlier.\n\nFunction to build and attach a system process to\n\nargs– List Parameters to be passed onto the process\n\nkwargs– Dict Parameters to be passed onto the process\n\nNone\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractRunCondition\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html",
    "title": "lava.magma.runtime.message_infrastructure — Lava  documentation",
    "content": "Bases:`object`object\n\nFactory class to create the messaging infrastructure\n\nCreates the message infrastructure instance based on type\nof actor framework being chosen.\n\nBases:`ABC`ABC\n\nA Message Infrastructure Interface which can create actors which would\nparticipate in message passing/exchange, start and stop them as well as\ndeclare the underlying Channel Infrastructure Class to be used for message\npassing implementation.\n\nReturns a list of actors\n\nGiven a target_fn starts a system process\n\nGiven the Channel Type, Return the Channel Implementation to\nbe used during execution\n\n`Type`Type[[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)`Channel`Channel]\n\nStarts the messaging infrastructure\n\nStops the messaging infrastructure\n\nBases:`object`object\n\nBases:[MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)`MessageInfrastructureInterface`MessageInfrastructureInterface\n\nImplements message passing using shared memory and multiprocessing\n\nReturns a list of actors\n\nGiven a target_fn starts a system (os) process\n\n`Any`Any\n\nGiven a channel type, returns the shared memory based class\nimplementation for the same\n\n`Type`Type[[Channel](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.Channel)`Channel`Channel]\n\nReturns the underlying shared memory manager\n\nStarts the shared memory manager\n\nStops the shared memory manager\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`Process`Process\n\nWraps a process so that the exceptions can be collected if present\n\nException property.\n\nWait until child process terminates\n\nMethod to be run in sub-process; can be overridden in sub-class\n\nBases:[MessageInfrastructureInterface](https://lava-nc.org/lava/lava.magma.runtime.message_infrastructure.html#lava.magma.runtime.message_infrastructure.message_infrastructure_interface.MessageInfrastructureInterface)`MessageInfrastructureInterface`MessageInfrastructureInterface\n\nImplements message passing using nx board\n\nReturns a list of actors\n\nGiven a target_fn starts a system (os) process\n\n`Any`Any\n\nGiven a channel type, returns the shared memory based class\nimplementation for the same.\n\n`Type`Type[[ChannelType](https://lava-nc.org/lava/lava.magma.compiler.channels.html#lava.magma.compiler.channels.interfaces.ChannelType)`ChannelType`ChannelType]\n\nStarts the shared memory manager\n\nStops the shared memory manager\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nChannel\n``````\n\n``````\nMessageInfrastructureInterface\n``````\n\n``````\nChannelType\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.runtime.runtime_services.channel_broker.html",
    "title": "lava.magma.runtime.runtime_services.channel_broker — Lava  documentation",
    "content": "Bases:`object`object\n\nBases:`ABC`ABC\n\nAbstract ChannelBroker.\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\nBases:[AbstractChannelBroker](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.channel_broker.html#lava.magma.runtime.runtime_services.channel_broker.channel_broker.AbstractChannelBroker)`AbstractChannelBroker`AbstractChannelBroker\n\nChannelBroker for NxSdkRuntimeService.\n\nChannelBroker handles communication between NxSdkRuntimeService,\nLava Processes and a Loihi board. It uses the NxCore\nboard object and creates GRPC Channels for each port.\nThe ChannelBroker sends messages over the GRPC\nchannels and services requests by the NxSdkRuntimeService.\nThe NxSdkRuntimeService intercepts CPort and NcPort\nmessages from the Runtime and brokers the communication between\nruntime and Loihi Boards.\n\n`List`List[[Channel](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.channel_broker.html#lava.magma.runtime.runtime_services.channel_broker.channel_broker.Channel)`Channel`Channel]\n\nRetrieves request from processes and brokers communication\nbetween Cproc Models in Python and Cproc Models in C.\nAfter sending requests to the GRPC channel the process\nis informed about completion.\n\nStart the polling threads\n\nBases:`object`object\n\nBases:`object`object\n\nBases:`object`object\n\n`str`str\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractChannelBroker\n``````\n\n``````\nChannel\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.magma.runtime.runtime_services.html",
    "title": "lava.magma.runtime.runtime_services — Lava  documentation",
    "content": "Bases:`object`object\n\nEnumerator of Lava Loihi phases\n\nBases:`IntEnum`IntEnum\n\nEnumerator of different Loihi Versions.\n\nBases:`object`object\n\nEnumerator phases in which snip can run in nxcore.\n\nINIT Phase of Embedded Snip. This executes only once.\n\nManagement Phase of Embedded Snip.\n\nPre-Learn Management Phase of Embedded Snip.\n\nA management phase snip triggered remotely\n\nSPIKING Phase of Embedded Snip.\n\nAny User Command to execute during embedded execution.\n(Internal Use Only)\n\nConcurrent Execution for Host Snip.\n\nHost Post Execution Phase for Host Snip.\n\nHost Pre Execution Phase for Host Snip.\n\nBases:`ABC`ABC\n\nThe RuntimeService interface is responsible for\ncoordinating the execution of a group of process models belonging to a common\nsynchronization domain. The domain will follow a SyncProtocol or will be\nasynchronous. The processes and their corresponding process models are\nselected by the Runtime dependent on the RunConfiguration assigned at the\nstart of execution. For each group of processes which follow the same\nprotocol and execute on the same node, the Runtime creates a RuntimeService.\nEach RuntimeService coordinates all actions and commands from the Runtime,\ntransmitting them to the processes under its management and\nreturning action and command responses back to Runtime.\n\nRuntimeService Types:\n\nthe CPU and written in Python.\nConcrete Implementations:\n\nLoihiPyRuntimeService: Coordinates process models executing on\nthe CPU and written in Python and following the LoihiProtocol.\n\nAsyncPyRuntimeService: Coordinates process models executing on\nthe CPU and written in Python and following the AsyncProtocol.\n\nBases:[PyRuntimeService](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.html#lava.magma.runtime.runtime_services.runtime_service.PyRuntimeService)`PyRuntimeService`PyRuntimeService\n\nRuntimeService that implements Async SyncProtocol in Py.\n\nBases:`object`object\n\nSignifies Request of PAUSE\n\nSignifies Request of STOP\n\nSignfies Ack or Finished with the Command\n\nSignifies Error raised\n\nSignifies Execution State to be Paused\n\nSignifies Termination\n\nRetrieves commands from the runtime and relays them to the process\nmodels. Also send the acknowledgement back to runtime.\n\nBases:[PyRuntimeService](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.html#lava.magma.runtime.runtime_services.runtime_service.PyRuntimeService)`PyRuntimeService`PyRuntimeService\n\nRuntimeService that implements Loihi SyncProtocol in Python.\n\nBases:`object`object\n\nSignifies Request of LEARNING\n\nSignifies Request of PAUSE\n\nSignifies Request of PREMPTION\n\nSignifies Request of PREMPTION\n\nSignifies Request of STOP\n\nSignfies Ack or Finished with the Command\n\nSignifies Error raised\n\nSignifies Execution State to be Paused\n\nSignifies Termination\n\nBases:`object`object\n\nRetrieves commands from the runtime. On STOP or PAUSE commands all\nProcessModels are notified and expected to TERMINATE or PAUSE,\nrespectively. Otherwise the number of time steps is received as command.\nIn this case iterate through the phases of the Loihi protocol until the\nlast time step is reached. The runtime is informed after the last time\nstep. The loop ends when receiving the STOP command from the runtime.\n\nBases:[AbstractRuntimeService](https://lava-nc.org/lava/lava.magma.runtime.runtime_services.html#lava.magma.runtime.runtime_services.interfaces.AbstractRuntimeService)`AbstractRuntimeService`AbstractRuntimeService\n\nAbstract RuntimeService for Python, it provides base methods\nfor start and run. It is not meant to instantiated directly\nbut used by inheritance\n\nStop the necessary channels to coordinate with runtime and group\nof processes this RuntimeService is managing\n\nOverride this method to implement the runtime service. The run\nmethod is invoked upon start which called when the execution is\nstarted by the runtime.\n\nStart the necessary channels to coordinate with runtime and group\nof processes this RuntimeService is managing\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyRuntimeService\n``````\n\n``````\nAbstractRuntimeService\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.conv.html",
    "title": "lava.proc.conv — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract template implementation of PyConvModel.\n\n`ndarray`ndarray\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractPyConvModel](https://lava-nc.org/lava/lava.proc.conv.html#lava.proc.conv.models.AbstractPyConvModel)`AbstractPyConvModel`AbstractPyConvModel\n\nConv with fixed point synapse implementation.\n\n`ndarray`ndarray\n\nalias of[Conv](https://lava-nc.org/lava/lava.proc.conv.html#lava.proc.conv.process.Conv)`Conv`Conv\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyConvModel](https://lava-nc.org/lava/lava.proc.conv.html#lava.proc.conv.models.AbstractPyConvModel)`AbstractPyConvModel`AbstractPyConvModel\n\nConv with float synapse implementation.\n\nalias of[Conv](https://lava-nc.org/lava/lava.proc.conv.html#lava.proc.conv.process.Conv)`Conv`Conv\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nBases:`IntEnum`IntEnum\n\nDefines how images are represented by tensors.\n\nN: number of images in a batch\nH: height of an image\nW: width of an image\nC: number of channels of an image\n\nConvolution implementation\n\ninput(3 dimensional np array) – convolution input.\n\nweight(4 dimensional np array) – convolution kernel weight.\n\nkernel_size(2 element tuple,list, orarray) – convolution kernel size in XY/WH format.\n\nstride(2 element tuple,list, orarray) – convolution stride in XY/WH format.\n\npadding(2 element tuple,list, orarray) – convolution padding in XY/WH format.\n\ndilation(2 element tuple,list, orarray) – dilation of convolution kernel in XY/WH format.\n\ngroups(int) – number of convolution groups.\n\nconvolution output\n\n3 dimensional np array\n\nScipy based implementation of convolution\n\ninput(3 dimensional np array) – convolution input.\n\nweight(4 dimensional np array) – convolution kernel weight.\n\nkernel_size(2 element tuple,list, orarray) – convolution kernel size in XY/WH format.\n\nstride(2 element tuple,list, orarray) – convolution stride in XY/WH format.\n\npadding(2 element tuple,list, orarray) – convolution padding in XY/WH format.\n\ndilation(2 element tuple,list, orarray) – dilation of convolution kernel in XY/WH format.\n\ngroups(int) – number of convolution groups.\n\nconvolution output\n\n3 dimensional np array\n\nTranslate convolution kernel into sparse matrix.\n\ninput_shape(tupleof3 ints) – Shape of input to the convolution.\n\noutput_shape(tupleof3 ints) – Shape of output from the convolution.\n\nkernel(numpy array with 4 dimensions) – Convolution kernel. The kernel should have four dimensions. The order\nof kernel tensor is described by`order`orderargument. See Notes for the\nsupported orders.\n\nstride(tupleof2 ints) – Convolution stride.\n\npadding(tupleof2 ints) – Convolution padding.\n\ndilation(tupleof2 ints) – Convolution dilation.\n\ngroup(int) – Convolution groups.\n\norder([TensorOrder](https://lava-nc.org/lava/lava.proc.conv.html#lava.proc.conv.utils.TensorOrder)TensorOrder,optional) – The order of convolution kernel tensor. The default is lava convolution\norder i.e.`TensorOrder.NWHC`TensorOrder.NWHC\n\n`Tuple`Tuple[`ndarray`ndarray,`ndarray`ndarray,`ndarray`ndarray]\n\nnp.ndarray– Destination indices of sparse matrix. It is a linear array of ints.np.ndarray– Source indices of sparse matrix. It is a linear array of ints.np.ndarray– Weight value at non-zero location.\n\nnp.ndarray– Destination indices of sparse matrix. It is a linear array of ints.\n\nnp.ndarray– Source indices of sparse matrix. It is a linear array of ints.\n\nnp.ndarray– Weight value at non-zero location.\n\nValueError– If tensor order is not supported.\n\nAssertionError– if output channels is not divisible by group.\n\nAssertionError– if input channels is not divisible by group.\n\nNotes\n\nInput/Output order\n\nKernel order\n\nOperation type\n\nWHC\n\nNWHC\n\nDefault Lava order. The operation is convolution.\n\nCHW\n\nNCHW\n\nDefault PyTorch order. The operation in correlation.\n\nHWC\n\nHWCN\n\nDefault Tensorflow order. The operation in correlation.\n\nCreate a tuple of two integers from the given input.\n\nvalue(intortuple(int) ortuple(int,int)) –\n\ntuple value of input\n\ntuple(int, int)\n\nException– if argument value is not 1/2 dimensional.\n\nCalculates the output shape of convolution operation.\n\ninput_shape(3 element tuple,list, orarray) – shape of input to convolution in XYZ/WHC format.\n\nout_channels(int) – number of output channels.\n\nkernel_size(2 element tuple,list, orarray) – convolution kernel size in XY/WH format.\n\nstride(2 element tuple,list, orarray) – convolution stride in XY/WH format.\n\npadding(2 element tuple,list, orarray) – convolution padding in XY/WH format.\n\ndilation(2 element tuple,list, orarray) – dilation of convolution kernel in XY/WH format.\n\nshape of convolution output in XYZ/WHC format.\n\ntuple of 3 ints\n\nException– for invalid x convolution dimension.\n\nException– for invalid y convolution dimension.\n\nclamps as if input is a signed value within the precision of bits.\n\nx(int,float,np array) – input\n\nbits(int) – number of bits for the variable\n\nclamped value\n\nsame type as x\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nndarray\n``````\n\n``````\nAbstractPyConvModel\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nConv\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyInPort\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.dense.html",
    "title": "lava.proc.dense — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract Conn Process with Dense synaptic connections which incorporates\ndelays into the Conn Process.\n\nCalculate the activation matrix based on s_in by performing\ndelay_wgts * s_in.\n\n`ndarray`ndarray\n\nCreate a matrix where the synaptic weights are separated\nby their corresponding delays. The first matrix contains all the\nweights, where the delay is equal to zero. The second matrix\ncontains all the weights, where the delay is equal to one and so on.\nThese matrices are then stacked together vertically.\n\nReturns 2D matrix of form\n:rtype:`ndarray`ndarray\n\n(num_flat_output_neurons * max_delay + 1, num_flat_input_neurons) where\ndelay_wgts[\n\nk * num_flat_output_neurons : (k + 1) * num_flat_output_neurons, :\n\n]\ncontains the weights for all connections with a delay equal to k.\nThis allows for the updating of the activation buffer and updating\nweights.\n\nUpdates the activations for the connection.\nClears first column of a_buff and rolls them to the last column.\nFinally, calculates the activations for the current time step and adds\nthem to a_buff.\nThis order of operations ensures that delays of 0 correspond to\nthe next time step.\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nImplementation of Conn Process with Dense synaptic connections that is\nbit-accurate with Loihi’s hardware implementation of Dense, which means,\nit mimics Loihi behavior bit-by-bit.\n\nFunction that runs in Spiking Phase\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nImplementation of Conn Process with Dense synaptic connections in\nfloating point precision. This short and simple ProcessModel can be used\nfor quick algorithmic prototyping, without engaging with the nuances of a\nfixed point implementation.\n\nFunction that runs in Spiking Phase\n\nBases:[AbstractPyDelayDenseModel](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDelayDenseModel)`AbstractPyDelayDenseModel`AbstractPyDelayDenseModel\n\nImplementation of Conn Process with Dense synaptic connections that is\nbit-accurate with Loihi’s hardware implementation of Dense, which means,\nit mimics Loihi behaviour bit-by-bit. DelayDense incorporates delays into\nthe Conn Process. Loihi 2 has a maximum of 6 bits for delays, meaning a\nspike can be delayed by 0 to 63 time steps.\n\nalias of[DelayDense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.DelayDense)`DelayDense`DelayDense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[AbstractPyDelayDenseModel](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDelayDenseModel)`AbstractPyDelayDenseModel`AbstractPyDelayDenseModel\n\nImplementation of Conn Process with Dense synaptic connections in\nfloating point precision. This short and simple ProcessModel can be used\nfor quick algorithmic prototyping, without engaging with the nuances of a\nfixed point implementation. DelayDense incorporates delays into the Conn\nProcess.\n\nalias of[DelayDense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.DelayDense)`DelayDense`DelayDense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[AbstractPyDenseModelBitAcc](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDenseModelBitAcc)`AbstractPyDenseModelBitAcc`AbstractPyDenseModelBitAcc\n\nalias of[Dense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.Dense)`Dense`Dense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyDenseModelFloat](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDenseModelFloat)`AbstractPyDenseModelFloat`AbstractPyDenseModelFloat\n\nalias of[Dense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.Dense)`Dense`Dense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[LearningConnectionModelBitApproximate](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.connection.LearningConnectionModelBitApproximate)`LearningConnectionModelBitApproximate`LearningConnectionModelBitApproximate,[AbstractPyDenseModelBitAcc](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDenseModelBitAcc)`AbstractPyDenseModelBitAcc`AbstractPyDenseModelBitAcc\n\nImplementation of Conn Process with Dense synaptic connections that\nuses similar constraints as Loihi’s hardware implementation of dense\nconnectivity but does not reproduce Loihi bit-by-bit.\n\nalias of[LearningDense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.LearningDense)`LearningDense`LearningDense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[LearningConnectionModelFloat](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.connection.LearningConnectionModelFloat)`LearningConnectionModelFloat`LearningConnectionModelFloat,[AbstractPyDenseModelFloat](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.models.AbstractPyDenseModelFloat)`AbstractPyDenseModelFloat`AbstractPyDenseModelFloat\n\nImplementation of Conn Process with Dense synaptic connections in\nfloating point precision. This short and simple ProcessModel can be used\nfor quick algorithmic prototyping, without engaging with the nuances of a\nfixed point implementation.\n\nalias of[LearningDense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.LearningDense)`LearningDense`LearningDense\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[Dense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.Dense)`Dense`Dense\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nDense connections between neurons. Realizes the following abstract\nbehavior: a_out = weights * s_in\n\nweights(numpy.ndarray) – 2D connection weight matrix of form (num_flat_output_neurons,\nnum_flat_input_neurons) in C-order (row major).\n\nweight_exp(int,optional) – Shared weight exponent of base 2 used to scale magnitude of\nweights, if needed. Mostly for fixed point implementations.\nUnnecessary for floating point implementations.\nDefault value is 0.\n\nnum_weight_bits(int,optional) – Shared weight width/precision used by weight. Mostly for fixed\npoint implementations. Unnecessary for floating point\nimplementations.\nDefault is for weights to use full 8 bit precision.\n\nsign_mode([SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode,optional) –Shared indicator whether synapse is of type SignMode.NULL,\nSignMode.MIXED, SignMode.EXCITATORY, or SignMode.INHIBITORY. If\nSignMode.MIXED, the sign of the weight is\nincluded in the weight bits and the fixed point weight used for\ninference is scaled by 2.\nUnnecessary for floating point implementations.In the fixed point implementation, weights are scaled according to\nthe following equations:\nw_scale = 8 - num_weight_bits + weight_exp + isMixed()\nweights = weights * (2 ** w_scale)\n\nShared indicator whether synapse is of type SignMode.NULL,\nSignMode.MIXED, SignMode.EXCITATORY, or SignMode.INHIBITORY. If\nSignMode.MIXED, the sign of the weight is\nincluded in the weight bits and the fixed point weight used for\ninference is scaled by 2.\nUnnecessary for floating point implementations.\n\nIn the fixed point implementation, weights are scaled according to\nthe following equations:\nw_scale = 8 - num_weight_bits + weight_exp + isMixed()\nweights = weights * (2 ** w_scale)\n\nnum_message_bits(int,optional) – Determines whether the Dense Process deals with the incoming\nspikes as binary spikes (num_message_bits = 0) or as graded\nspikes (num_message_bits > 0). Default is 0.\n\nBases:[LearningConnectionProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.connection.LearningConnectionProcess)`LearningConnectionProcess`LearningConnectionProcess,[Dense](https://lava-nc.org/lava/lava.proc.dense.html#lava.proc.dense.process.Dense)`Dense`Dense\n\nDense connections between neurons. Realizes the following abstract\nbehavior: a_out = weights * s_in ‘\n\nweights(numpy.ndarray) – 2D connection weight matrix of form (num_flat_output_neurons,\nnum_flat_input_neurons) in C-order (row major).\n\nweight_exp(int,optional) – Shared weight exponent of base 2 used to scale magnitude of\nweights, if needed. Mostly for fixed point implementations.\nUnnecessary for floating point implementations.\nDefault value is 0.\n\nnum_weight_bits(int,optional) – Shared weight width/precision used by weight. Mostly for fixed\npoint implementations. Unnecessary for floating point\nimplementations.\nDefault is for weights to use full 8 bit precision.\n\nsign_mode([SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode,optional) –Shared indicator whether synapse is of type SignMode.NULL,\nSignMode.MIXED, SignMode.EXCITATORY, or SignMode.INHIBITORY. If\nSignMode.MIXED, the sign of the weight is\nincluded in the weight bits and the fixed point weight used for\ninference is scaled by 2.\nUnnecessary for floating point implementations.In the fixed point implementation, weights are scaled according to\nthe following equations:\nw_scale = 8 - num_weight_bits + weight_exp + isMixed()\nweights = weights * (2 ** w_scale)\n\nShared indicator whether synapse is of type SignMode.NULL,\nSignMode.MIXED, SignMode.EXCITATORY, or SignMode.INHIBITORY. If\nSignMode.MIXED, the sign of the weight is\nincluded in the weight bits and the fixed point weight used for\ninference is scaled by 2.\nUnnecessary for floating point implementations.\n\nIn the fixed point implementation, weights are scaled according to\nthe following equations:\nw_scale = 8 - num_weight_bits + weight_exp + isMixed()\nweights = weights * (2 ** w_scale)\n\nnum_message_bits(int,optional) – Determines whether the LearningDense Process deals with the incoming\nspikes as binary spikes (num_message_bits = 0) or as graded\nspikes (num_message_bits > 0). Default is 0.\n\nlearning_rule([LoihiLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.LoihiLearningRule)LoihiLearningRule) – Learning rule which determines the parameters for online learning.\n\ngraded_spike_cfg([GradedSpikeCfg](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.constants.GradedSpikeCfg)GradedSpikeCfg) –Indicates how to use incoming graded spike to update pre-synaptic traces(0) GradedSpikeCfg.USE_REGULAR_IMPULSE interprets the spike as a\nbinary spike, adds regular impulses to pre-synaptic traces, at the end\nof the epoch.\n(1) GradedSpikeCfg.OVERWRITE interprets the spike as a graded spike,\noverwrites the value of the pre-synaptic trace x1 by payload/2,\nupon spiking.\n(2) GradedSpikeCfg.ADD_WITH_SATURATION interprets the spike as a graded\nspike, adds payload/2 to the pre-synaptic trace x1, upon spiking,\nsaturates x1 to 127 (fixed-pt/hw only).\n(3) GradedSpikeCfg.ADD_WITHOUT_SATURATION interprets the spike as a\ngraded spike, adds payload/2 to the pre-synaptic trace x1, upon spiking,\nkeeps only overflow above 127 in x1 (fixed-pt/hw only), adds regular\nimpulse to x2 on overflow.\nIn addition, only pre-synaptic graded spikes that trigger overflow in\nx1 and regular impulse addition to x2 will be considered by the\nlearning rule Products conditioned on x0.\n\nIndicates how to use incoming graded spike to update pre-synaptic traces\n\n(0) GradedSpikeCfg.USE_REGULAR_IMPULSE interprets the spike as a\nbinary spike, adds regular impulses to pre-synaptic traces, at the end\nof the epoch.\n(1) GradedSpikeCfg.OVERWRITE interprets the spike as a graded spike,\noverwrites the value of the pre-synaptic trace x1 by payload/2,\nupon spiking.\n(2) GradedSpikeCfg.ADD_WITH_SATURATION interprets the spike as a graded\nspike, adds payload/2 to the pre-synaptic trace x1, upon spiking,\nsaturates x1 to 127 (fixed-pt/hw only).\n(3) GradedSpikeCfg.ADD_WITHOUT_SATURATION interprets the spike as a\ngraded spike, adds payload/2 to the pre-synaptic trace x1, upon spiking,\nkeeps only overflow above 127 in x1 (fixed-pt/hw only), adds regular\nimpulse to x2 on overflow.\nIn addition, only pre-synaptic graded spikes that trigger overflow in\nx1 and regular impulse addition to x2 will be considered by the\nlearning rule Products conditioned on x0.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nndarray\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nPyInPort\n``````\n\n``````\nAbstractPyDelayDenseModel\n``````\n\n``````\nDelayDense\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nAbstractPyDenseModelBitAcc\n``````\n\n``````\nDense\n``````\n\n``````\nAbstractPyDenseModelFloat\n``````\n\n``````\nLearningConnectionModelBitApproximate\n``````\n\n``````\nLearningDense\n``````\n\n``````\nLearningConnectionModelFloat\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nLearningConnectionProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.html",
    "title": "Lava process library — Lava  documentation",
    "content": "The Lava process library contains a collection of commonly used Processes and ProcessModels from which higher-level applications can be built.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.io.html",
    "title": "lava.proc.io — Lava  documentation",
    "content": "Bases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nAbstract dataloader object.\n\ndataset(Iterable) – The actual dataset object. Dataset is expected to return`(input,label/ground_truth)`(input,label/ground_truth)when indexed.\n\ninterval(int,optional) – Interval between each data load, by default 1\n\noffset(int,optional) – Offset (phase) for each data load, by default 0\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\n`ndarray`ndarray\n\nBases:[AbstractPyDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPyDataloaderModel)`AbstractPyDataloaderModel`AbstractPyDataloaderModel\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\n`None`None\n\nFunction that runs in Post Lrn Mgmt Phase\n\n`None`None\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractPyDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPyDataloaderModel)`AbstractPyDataloaderModel`AbstractPyDataloaderModel\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\n`None`None\n\nFunction that runs in Post Lrn Mgmt Phase\n\n`None`None\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractPySpikeDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPySpikeDataloaderModel)`AbstractPySpikeDataloaderModel`AbstractPySpikeDataloaderModel\n\nalias of[SpikeDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.SpikeDataloader)`SpikeDataloader`SpikeDataloader\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPySpikeDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPySpikeDataloaderModel)`AbstractPySpikeDataloaderModel`AbstractPySpikeDataloaderModel\n\nalias of[SpikeDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.SpikeDataloader)`SpikeDataloader`SpikeDataloader\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyStateDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPyStateDataloaderModel)`AbstractPyStateDataloaderModel`AbstractPyStateDataloaderModel\n\nalias of[StateDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.StateDataloader)`StateDataloader`StateDataloader\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyStateDataloaderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractPyStateDataloaderModel)`AbstractPyStateDataloaderModel`AbstractPyStateDataloaderModel\n\nalias of[StateDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.StateDataloader)`StateDataloader`StateDataloader\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractDataloader)`AbstractDataloader`AbstractDataloader\n\nDataloader object that sends spike for a input sample at a\nset interval and offset (phase).\n\ndataset(Iterable) – The actual dataset object. Dataset is expected to return`(spike,label/ground_truth)`(spike,label/ground_truth)when indexed.\n\ninterval(int,optional) – Interval between each data load, by default 1\n\noffset(int,optional) – Offset (phase) for each data load, by default 0\n\nBases:[AbstractDataloader](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.dataloader.AbstractDataloader)`AbstractDataloader`AbstractDataloader\n\nDataloader object that loads new data sample to internal state at a\nset interval and offset (phase).\n\ndataset(Iterable) – The actual dataset object. Dataset is expected to return`(input,label/ground_truth)`(input,label/ground_truth)when indexed.\n\ninterval(int,optional) – Interval between each data load, by default 1\n\noffset(int,optional) – Offset (phase) for each data load, by default 0\n\nConnects the internal state (ref-port) to the variable. The variable\ncan be internal state of some other process which needs to reflect the\ndataset input.\n\nvar([Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Var) – Variable that is connected to this object’s state (ref-port).\n\n`None`None\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nImplementation of Delta encoder.\n\nBases:`Enum`Enum\n\nEnumeration of message compression mode.\n\nNo compression. Raw 32 bit data is communicated as it is.\n\nSparse 32 bit data and index is communicated.\n\nFour 8 bit data packed into 32 bit message. NOTE: only works for 8 bit\ndata.\n\n8 bit data and 8 bit delta encoded index. NOTE: only works for 8 bit\ndata.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nDelta encoding with threshold.\n\nDelta encoding looks at the difference of new input and sends only the\ndifference (delta) when it is more than a positive threshold.\n\nDelta dynamics:\ndelta   = act_new - act + residue           # delta encoding\ns_out   = delta if abs(delta) > vth else 0  # spike mechanism\nresidue = delta - s_out                     # residue accumulation\nact     = act_new\n\nshape(Tuple) – Shape of the sigma process.\n\nvth(intorfloat) – Threshold of the delta encoder.\n\nspike_exp(Optional[int]) – Scaling exponent with base 2 for the spike message.\nNote: This should only be used for fixed point models.\nDefault is 0.\n\nnum_bits(Optional[int]) – Precision for spike output. It is applied before spike_exp. If None,\nprecision is not enforced, i.e. the spike output is unbounded.\nDefault is None.\n\ncompression([Compression](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.encoder.Compression)Compression) – Data compression mode, by default DENSE compression.\n\nBases:[AbstractPyDeltaEncoderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.encoder.AbstractPyDeltaEncoderModel)`AbstractPyDeltaEncoderModel`AbstractPyDeltaEncoderModel\n\nDense (No) compression Model of PyDeltaEncoder.\n\nalias of[DeltaEncoder](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.encoder.DeltaEncoder)`DeltaEncoder`DeltaEncoder\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[AbstractPyDeltaEncoderModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.encoder.AbstractPyDeltaEncoderModel)`AbstractPyDeltaEncoderModel`AbstractPyDeltaEncoderModel\n\nSparse compression Model of PyDeltaEncoder.\n\nBased on compression mode, it can be\n* SPARSE: 32 bit data and 32 bit index used for messaging sparse data.\n* PACKED_4: Four 8 bit data packed into one 32 bit data for messaging.\n* DELTA_SPARSE_8: 8 bit index and 8 bit data messaging.\n\nPython decoding script for delta_sparse_8 encoding. It is useful for\ndebug and verify the encoding.\n\n8 bit compressed data and index encoding.\n\n4x 8bit data encodig into one 32 bit data.\n\nBasic sparse encoding.\n\nalias of[DeltaEncoder](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.encoder.DeltaEncoder)`DeltaEncoder`DeltaEncoder\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract Reset process implementation.\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\n`None`None\n\nFunction that runs in Post Lrn Mgmt Phase\n\n`None`None\n\nBases:[AbstractPyReset](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.reset.AbstractPyReset)`AbstractPyReset`AbstractPyReset\n\nReset process implementation for int type.\n\nalias of[Reset](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.reset.Reset)`Reset`Reset\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyReset](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.reset.AbstractPyReset)`AbstractPyReset`AbstractPyReset\n\nReset process implementation for float type.\n\nalias of[Reset](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.reset.Reset)`Reset`Reset\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nResets it’s internal state at a set interval and offset (phase).\n\nreset_value(intorfloat) – reset value, by default 0\n\ninterval(int,optional) – reset interval, by default 1\n\noffset(int,optional) – reset offset (phase), by default 0\n\n`None`None\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract Read Var process implementation.\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\n`None`None\n\nFunction that runs in Post Lrn Mgmt Phase\n\n`None`None\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract ring buffer receive process model.\n\nReceive spikes and store in an internal variable\n\n`None`None\n\nBases:[AbstractPyRead](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.AbstractPyRead)`AbstractPyRead`AbstractPyRead\n\nRead Var process implementation for int type.\n\nalias of[Read](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.Read)`Read`Read\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyRead](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.AbstractPyRead)`AbstractPyRead`AbstractPyRead\n\nRead Var process implementation for float type.\n\nalias of[Read](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.Read)`Read`Read\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyReceiveModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.AbstractPyReceiveModel)`AbstractPyReceiveModel`AbstractPyReceiveModel\n\nFixed point ring buffer receive process model.\n\nalias of[RingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.RingBuffer)`RingBuffer`RingBuffer\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyReceiveModel](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.AbstractPyReceiveModel)`AbstractPyReceiveModel`AbstractPyReceiveModel\n\nFloat ring buffer receive process model.\n\nalias of[RingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.sink.RingBuffer)`RingBuffer`RingBuffer\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nReads and logs the data of it’s internal state at a\nset interval and offset (phase).\n\nbuffer(int) – number of samples to buffer\n\ninterval(int,optional) – reset interval, by default 1\n\noffset(int,optional) – reset offset (phase), by default 0\n\n`None`None\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess for receiving arbitrarily shaped data into a ring buffer\nmemory. Works as a substitute for probing.\n\nshape(tuple) – shape of the process\n\nbuffer(int) – size of data sink buffer\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract ring buffer process model.\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractPyRingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.source.AbstractPyRingBuffer)`AbstractPyRingBuffer`AbstractPyRingBuffer\n\nFixed point ring buffer send process model.\n\nalias of[RingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.source.RingBuffer)`RingBuffer`RingBuffer\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractPyRingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.source.AbstractPyRingBuffer)`AbstractPyRingBuffer`AbstractPyRingBuffer\n\nFloat ring buffer send process model.\n\nalias of[RingBuffer](https://lava-nc.org/lava/lava.proc.io.html#lava.proc.io.source.RingBuffer)`RingBuffer`RingBuffer\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nSpike generator process from circular data buffer.\n\ndata(np array) – data to generate spike from. Last dimension is assumed as time.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nAbstractProcess\n``````\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nndarray\n``````\n\n``````\nAbstractPyDataloaderModel\n``````\n\n``````\nOptional\n``````\n\n``````\nPyRefPort\n``````\n\n``````\nAbstractPySpikeDataloaderModel\n``````\n\n``````\nSpikeDataloader\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nAbstractPyStateDataloaderModel\n``````\n\n``````\nStateDataloader\n``````\n\n``````\nAbstractDataloader\n``````\n\n``````\nAbstractPyDeltaEncoderModel\n``````\n\n``````\nDeltaEncoder\n``````\n\n``````\nAbstractPyReset\n``````\n\n``````\nReset\n``````\n\n``````\nAbstractPyRead\n``````\n\n``````\nRead\n``````\n\n``````\nAbstractPyReceiveModel\n``````\n\n``````\nPyInPort\n``````\n\n``````\nRingBuffer\n``````\n\n``````\nAbstractPyRingBuffer\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.learning_rules.html",
    "title": "lava.proc.learning_rules — Lava  documentation",
    "content": "Bases:[Loihi3FLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.Loihi3FLearningRule)`Loihi3FLearningRule`Loihi3FLearningRule\n\nBases:[Loihi2FLearningRule](https://lava-nc.org/lava/lava.magma.core.learning.html#lava.magma.core.learning.learning_rule.Loihi2FLearningRule)`Loihi2FLearningRule`Loihi2FLearningRule\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nLoihi3FLearningRule\n``````\n\n``````\nLoihi2FLearningRule\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.lif.html",
    "title": "lava.proc.lif — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract implementation of fixed point precision\nleaky-integrate-and-fire neuron model. Implementations like those\nbit-accurate with Loihi hardware inherit from here.\n\nVoltage reset behaviour. This can differ for different neuron\nmodels.\n\nThe run function that performs the actual computation during\nexecution orchestrated by a PyLoihiProcessModel using the\nLoihiProtocol.\n\nScale bias with bias exponent by taking into account sign of the\nexponent.\n\nPlaceholder method for scaling threshold(s).\n\nPlaceholder method to specify spiking behaviour of a LIF neuron.\n\nCommon sub-threshold dynamics of current and voltage variables for\nall LIF models. This is where the ‘leaky integration’ happens.\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nAbstract implementation of floating point precision\nleaky-integrate-and-fire neuron model.\n\nSpecific implementations inherit from here.\n\nVoltage reset behaviour. This can differ for different neuron\nmodels.\n\nThe run function that performs the actual computation during\nexecution orchestrated by a PyLoihiProcessModel using the\nLoihiProtocol.\n\nAbstract method to define the activation function that determines\nhow spikes are generated.\n\nCommon sub-threshold dynamics of current and voltage variables for\nall LIF models. This is where the ‘leaky integration’ happens.\n\nBases:[LearningNeuronModelFixed](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.neuron.LearningNeuronModelFixed)`LearningNeuronModelFixed`LearningNeuronModelFixed,[AbstractPyLifModelFixed](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFixed)`AbstractPyLifModelFixed`AbstractPyLifModelFixed\n\nImplementation of Leaky-Integrate-and-Fire neural\nprocess in fixed point precision with learning enabled.\n\nalias of[LearningLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LearningLIF)`LearningLIF`LearningLIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nCalculates the third factor trace and sends it to the\nDense process for learning.\n\n`None`None\n\nScale threshold according to the way Loihi hardware scales it. In\nLoihi hardware, threshold is left-shifted by 6-bits to MSB-align it\nwith other state variables of higher precision.\n\nSpike when voltage exceeds threshold.\n\nBases:[LearningNeuronModelFloat](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.neuron.LearningNeuronModelFloat)`LearningNeuronModelFloat`LearningNeuronModelFloat,[AbstractPyLifModelFloat](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFloat)`AbstractPyLifModelFloat`AbstractPyLifModelFloat\n\nImplementation of Leaky-Integrate-and-Fire neural process in floating\npoint precision with learning enabled.\n\nalias of[LearningLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LearningLIF)`LearningLIF`LearningLIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nCalculates the third factor trace and sends it to the\nDense process for learning.\n\n`None`None\n\nSpiking activation function for LIF.\n\nBases:[AbstractPyLifModelFixed](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFixed)`AbstractPyLifModelFixed`AbstractPyLifModelFixed\n\nImplementation of Leaky-Integrate-and-Fire neural process bit-accurate\nwith Loihi’s hardware LIF dynamics, which means, it mimics Loihi\nbehaviour bit-by-bit.\n\nCurrently missing features (compared to Loihi 1 hardware):\n\nrefractory period after spiking\n\naxonal delays\n\nPrecisions of state variables\n\ndu: unsigned 12-bit integer (0 to 4095)\n\ndv: unsigned 12-bit integer (0 to 4095)\n\nbias_mant: signed 13-bit integer (-4096 to 4095). Mantissa part of neuron\nbias.\n\nbias_exp: unsigned 3-bit integer (0 to 7). Exponent part of neuron bias.\n\nvth: unsigned 17-bit integer (0 to 131071).\n\nalias of[LIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIF)`LIF`LIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nScale threshold according to the way Loihi hardware scales it. In\nLoihi hardware, threshold is left-shifted by 6-bits to MSB-align it\nwith other state variables of higher precision.\n\nSpike when voltage exceeds threshold.\n\nBases:[AbstractPyLifModelFloat](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFloat)`AbstractPyLifModelFloat`AbstractPyLifModelFloat\n\nImplementation of Leaky-Integrate-and-Fire neural process in floating\npoint precision. This short and simple ProcessModel can be used for quick\nalgorithmic prototyping, without engaging with the nuances of a fixed\npoint implementation.\n\nalias of[LIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIF)`LIF`LIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nSpiking activation function for LIF.\n\nBases:[AbstractPyLifModelFloat](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFloat)`AbstractPyLifModelFloat`AbstractPyLifModelFloat\n\nImplementation of Leaky-Integrate-and-Fire neural process with\nrefractory period in floating point precision.\n\nalias of[LIFRefractory](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIFRefractory)`LIFRefractory`LIFRefractory\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nThe run function that performs the actual computation during\nexecution orchestrated by a PyLoihiProcessModel using the\nLoihiProtocol.\n\nSpiking activation function for LIF Refractory.\n\nSub-threshold dynamics of current and voltage variables for\nall refractory LIF models. This is where the ‘leaky integration’\nhappens.\n\nBases:[AbstractPyLifModelFixed](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFixed)`AbstractPyLifModelFixed`AbstractPyLifModelFixed\n\nImplementation of Leaky-Integrate-and-Fire neural process with reset\nbit-accurate with Loihi’s hardware LIF dynamics, which means, it mimics\nLoihi behaviour.\n\nPrecisions of state variables\n\ndu: unsigned 12-bit integer (0 to 4095)\n\ndv: unsigned 12-bit integer (0 to 4095)\n\nbias_mant: signed 13-bit integer (-4096 to 4095). Mantissa part of neuron\nbias.\n\nbias_exp: unsigned 3-bit integer (0 to 7). Exponent part of neuron bias.\n\nvth: unsigned 17-bit integer (0 to 131071).\n\nalias of[LIFReset](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIFReset)`LIFReset`LIFReset\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nThe run function that performs the actual computation during\nexecution orchestrated by a PyLoihiProcessModel using the\nLoihiProtocol.\n\nScale threshold according to the way Loihi hardware scales it. In\nLoihi hardware, threshold is left-shifted by 6-bits to MSB-align it\nwith other state variables of higher precision.\n\nSpike when voltage exceeds threshold.\n\nBases:[AbstractPyLifModelFloat](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFloat)`AbstractPyLifModelFloat`AbstractPyLifModelFloat\n\nImplementation of Leaky-Integrate-and-Fire neural process with reset\nin floating point precision.\n\nalias of[LIFReset](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIFReset)`LIFReset`LIFReset\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nThe run function that performs the actual computation during\nexecution orchestrated by a PyLoihiProcessModel using the\nLoihiProtocol.\n\nSpiking activation function for LIF.\n\nBases:[AbstractPyLifModelFixed](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFixed)`AbstractPyLifModelFixed`AbstractPyLifModelFixed\n\nImplementation of Ternary Leaky-Integrate-and-Fire neural process\nwith fixed point precision.\n\nSee also\n\nBit-Accurate LIF neuron model\n\nalias of[TernaryLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.TernaryLIF)`TernaryLIF`TernaryLIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nReset voltage of all spiking neurons to 0.\n\nPlaceholder method for scaling threshold(s).\n\nPlaceholder method to specify spiking behaviour of a LIF neuron.\n\nBases:[AbstractPyLifModelFloat](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.models.AbstractPyLifModelFloat)`AbstractPyLifModelFloat`AbstractPyLifModelFloat\n\nImplementation of Ternary Leaky-Integrate-and-Fire neural process in\nfloating point precision. This ProcessModel builds upon the floating\npoint ProcessModel for LIF by adding upper and lower threshold voltages.\n\nalias of[TernaryLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.TernaryLIF)`TernaryLIF`TernaryLIF\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nReset voltage of all spiking neurons to 0.\n\nSpiking activation for T-LIF: -1 spikes below lower threshold,\n+1 spikes above upper threshold.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nAbstract class for variables common to all neurons with leaky\nintegrator dynamics.\n\nBases:[AbstractLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.AbstractLIF)`AbstractLIF`AbstractLIF\n\nLeaky-Integrate-and-Fire (LIF) neural Process.\n\nLIF dynamics abstracts to:\nu[t] = u[t-1] * (1-du) + a_in         # neuron current\nv[t] = v[t-1] * (1-dv) + u[t] + bias  # neuron voltage\ns_out = v[t] > vth                    # spike if threshold is exceeded\nv[t] = 0                              # reset at spike\n\nshape(tuple(int)) – Number and topology of LIF neurons.\n\nu(float,list,numpy.ndarray,optional) – Initial value of the neurons’ current.\n\nv(float,list,numpy.ndarray,optional) – Initial value of the neurons’ voltage (membrane potential).\n\ndu(float,optional) – Inverse of decay time-constant for current decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\ndv(float,optional) – Inverse of decay time-constant for voltage decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\nbias_mant(float,list,numpy.ndarray,optional) – Mantissa part of neuron bias.\n\nbias_exp(float,list,numpy.ndarray,optional) – Exponent part of neuron bias, if needed. Mostly for fixed point\nimplementations. Ignored for floating point implementations.\n\nvth(float,optional) – Neuron threshold voltage, exceeding which, the neuron will spike.\nCurrently, only a single threshold can be set for the entire\npopulation of neurons.\n\nExample\n\nBases:[LIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIF)`LIF`LIF\n\nLeaky-Integrate-and-Fire (LIF) process with refractory period.\n\nshape(tuple(int)) – Number and topology of LIF neurons.\n\nu(float,list,numpy.ndarray,optional) – Initial value of the neurons’ current.\n\nv(float,list,numpy.ndarray,optional) – Initial value of the neurons’ voltage (membrane potential).\n\ndu(float,optional) – Inverse of decay time-constant for current decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\ndv(float,optional) – Inverse of decay time-constant for voltage decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\nbias_mant(float,list,numpy.ndarray,optional) – Mantissa part of neuron bias.\n\nbias_exp(float,list,numpy.ndarray,optional) – Exponent part of neuron bias, if needed. Mostly for fixed point\nimplementations. Ignored for floating point implementations.\n\nvth(float,optional) – Neuron threshold voltage, exceeding which, the neuron will spike.\nCurrently, only a single threshold can be set for the entire\npopulation of neurons.\n\nrefractory_period(int,optional) – The interval of the refractory period. 1 timestep by default.\n\nSee also\n\n‘Regular’ leaky-integrate-and-fire neuron for\n\n`documentation`documentation\n\nBases:[LIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.LIF)`LIF`LIF\n\nLeaky-Integrate-and-Fire (LIF) neural Process that resets its internal\nstates in regular intervals.\n\nshape(tuple(int)) – Number and topology of LIF neurons.\n\nu(float,list,numpy.ndarray,optional) – Initial value of the neurons’ current.\n\nv(float,list,numpy.ndarray,optional) – Initial value of the neurons’ voltage (membrane potential).\n\ndu(float,optional) – Inverse of decay time-constant for current decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\ndv(float,optional) – Inverse of decay time-constant for voltage decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\nbias_mant(float,list,numpy.ndarray,optional) – Mantissa part of neuron bias.\n\nbias_exp(float,list,numpy.ndarray,optional) – Exponent part of neuron bias, if needed. Mostly for fixed point\nimplementations. Ignored for floating point implementations.\n\nvth(float,optional) – Neuron threshold voltage, exceeding which, the neuron will spike.\nCurrently, only a single threshold can be set for the entire\npopulation of neurons.\n\nreset_interval(int,optional) – The interval of neuron state reset. By default 1 timestep.\n\nreset_offset(int,optional) – The phase/offset of neuron reset. By defalt at 0th timestep.\n\nSee also\n\n‘Regular’ leaky-integrate-and-fire neuron for\n\n`documentation`documentation\n\nBases:[LearningNeuronProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.neuron.LearningNeuronProcess)`LearningNeuronProcess`LearningNeuronProcess,[AbstractLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.AbstractLIF)`AbstractLIF`AbstractLIF\n\nLeaky-Integrate-and-Fire (LIF) neural Process with learning enabled.\n\nshape(tuple(int)) – Number and topology of LIF neurons.\n\nu(float,list,numpy.ndarray,optional) – Initial value of the neurons’ current.\n\nv(float,list,numpy.ndarray,optional) – Initial value of the neurons’ voltage (membrane potential).\n\ndu(float,optional) – Inverse of decay time-constant for current decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\ndv(float,optional) – Inverse of decay time-constant for voltage decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\nbias_mant(float,list,numpy.ndarray,optional) – Mantissa part of neuron bias.\n\nbias_exp(float,list,numpy.ndarray,optional) – Exponent part of neuron bias, if needed. Mostly for fixed point\nimplementations. Ignored for floating point implementations.\n\nvth(float,optional) – Neuron threshold voltage, exceeding which, the neuron will spike.\nCurrently, only a single threshold can be set for the entire\npopulation of neurons.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig,optional) – Configure the amount of debugging output.\n\nlearning_rule(LearningRule) – Defines the learning parameters and equation.\n\nBases:[AbstractLIF](https://lava-nc.org/lava/lava.proc.lif.html#lava.proc.lif.process.AbstractLIF)`AbstractLIF`AbstractLIF\n\nLeaky-Integrate-and-Fire (LIF) neural Process withternaryspiking\noutput, i.e., +1, 0, and -1 spikes. When the voltage of a TernaryLIF neuron\nexceeds its upper threshold (vth_hi), it issues a positive spike and when\nthe voltage drops below its lower threshold (vth_lo), it issues a negative\nspike. Between the two thresholds, the neuron follows leaky linear\ndynamics.\n\nThis class inherits the state variables and ports from AbstractLIF and\nadds two new threshold variables for upper and lower thresholds.\n\nshape(tuple(int)) – Number and topology of LIF neurons.\n\nu(float,list,numpy.ndarray,optional) – Initial value of the neurons’ current.\n\nv(float,list,numpy.ndarray,optional) – Initial value of the neurons’ voltage (membrane potential).\n\ndu(float,optional) – Inverse of decay time-constant for current decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\ndv(float,optional) – Inverse of decay time-constant for voltage decay. Currently, only a\nsingle decay can be set for the entire population of neurons.\n\nbias_mant(float,list,numpy.ndarray,optional) – Mantissa part of neuron bias.\n\nbias_exp(float,list,numpy.ndarray,optional) – Exponent part of neuron bias, if needed. Mostly for fixed point\nimplementations. Ignored for floating point implementations.\n\nvth_hi(float,optional) – Upper threshold voltage, exceeding which the neuron spikes +1.\nCurrently, only a single higher threshold can be set for the entire\npopulation of neurons.\n\nvth_lo(float,optional) – Lower threshold voltage, below which the neuron spikes -1.\nCurrently, only a single lower threshold can be set for the entire\npopulation of neurons.\n\nSee also\n\n‘Regular’ leaky-integrate-and-fire neuron for\n\n`documentation`documentation\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n>>>lif=LIF(shape=(200,15),du=10,dv=5)This will create 200x15 LIF neurons that all have the same current decayof 10 and voltage decay of 5.\n``````\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyInPort\n``````\n\n``````\nndarray\n``````\n\n``````\nint\n``````\n\n``````\nNone\n``````\n\n``````\nfloat\n``````\n\n``````\nLearningNeuronModelFixed\n``````\n\n``````\nAbstractPyLifModelFixed\n``````\n\n``````\nLearningLIF\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nLearningNeuronModelFloat\n``````\n\n``````\nAbstractPyLifModelFloat\n``````\n\n``````\nLIF\n``````\n\n``````\nLIFRefractory\n``````\n\n``````\nLIFReset\n``````\n\n``````\nlava.proc.lif.models.PyLifModelBitAcc\n``````\n\n``````\nTernaryLIF\n``````\n\n``````\nAbstractProcess\n``````\n\n``````\nAbstractLIF\n``````\n\n``````\nlava.proc.lif.process.LIF\n``````\n\n``````\nLearningNeuronProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.monitor.html",
    "title": "lava.proc.monitor — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nThis process model contains prototypical Ports and Vars to have\none-to-one correspondes with Monitor process.\n\nalias of[Monitor](https://lava-nc.org/lava/lava.proc.monitor.html#lava.proc.monitor.process.Monitor)`Monitor`Monitor\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nGuard function that determines if post lrn mgmt phase will get\nexecuted or not for the current timestep.\n\nDuring this phase, RefPorts of Monitor process collects data from\nmonitored Vars\n\nDuring this phase, InPorts of Monitor process collects data from\nmonitored OutPorts\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nMonitor process to probe/monitor a given variable of a target process.\n\nMonitor process is initialized without any Ports and Vars. The InPorts,\nRefPorts and Vars are created dynamically, as the Monitor process is\nused to probe OutPorts and Vars of other processes. For this purpose,\nMonitor process has theprobe()probe()method, which as arguments takes the\ntarget Var or OutPorts and number of time steps we want to monitor given\nprocess.\n\nNote: Monitor currently only supports to record from a singe Var or Port.\n\ndata(dict) – Dictionary that is populated by monitoring data once get_data(..)\nmethod is called, has the following structure:\ndata\n__monitored_process_name\n__monitored_var_or_out_port_name\n\nproc_params(dict) – Process parameters that will be transferred to the corresponding\nProcessModel. It is populated with the names of dynamically\ncreated port and var names of Monitor process, to be carried to its\nProcessModel. It is a dictionary of the following structure:\n“RefPorts”: names of RefPorts created to monitor target Vars\n“VarsData1”: names of Vars created to store data from target Vars\n“InPorts”: names of InPorts created to monitor target OutPorts\n“VarsData2”: names of Vars created to store data from target OutPorts\n“n_ref_ports”: number of created RefPorts, also monitored Vars\n“n_in_ports”: number of created InPorts, also monitored OutPorts\n\ntarget_names(dict) – The names of the targeted Processes and Vars/OutPorts to be monitored.\nThis is used in get_data(..) method to access the target names\ncorresponding to data-storing Vars of Monitor process during readout\nphase. This dict has the follwoing sturcture:\nkey: name of the data-storing Vars, i.e. VarsData1 and VarsData2\nvalue: [monitored_process_name, monitored_var_or_out_port_name]\n\nFetch and return the recorded data.\n\nThe recorded data is fetched, presented in a readable\ndict format and returned.\n\ndata– Data dictionary collected by Monitor Process.\n\ndict\n\nPlot the recorded data into subplots.\n\nCan handle recordings of multiple processes and multiple variables\nper process.\nEach process will create a separate column in the subplots, each\nvariable will be plotted in a separate row.\n\nax(matplotlib.Axes) – Axes to plot the data into\n\ntarget([Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Varor[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – The target which should be plotted\n\n*args– Passed to the matplotlib.plot function to customize the plot.\n\n**kwargs– Passed to the matplotlib.plot function to customize the plot.\n\nRun after __init__.\n\nCreates one prototypical RefPort, InPort and two Vars.\nThis ensures coherence and one-to-one correspondence between the\nMonitor Process and ProcessModel in terms og LavaPyTypes and\nPorts/Vars. These prototypical ports can later be updated inside theprobe()probe()method.\n\nNote: This is separated from constructor, because once multi-variable\nmonitoring is enabled, this method will be deprecated.\n\nProbe a Var or OutPort to record data from.\n\nRecord the target for num_step time steps, where target can be\na Var or OutPort of a process.\n\ntarget([Var](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.variable.Var)Varor[OutPort](https://lava-nc.org/lava/lava.magma.core.process.ports.html#lava.magma.core.process.ports.ports.OutPort)OutPort) – a Var or OutPort of some process to be monitored.\n\nnum_steps(int) – The number of steps the target Var/OutPort should be monitored.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nMonitor\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyInPort\n``````\n\n``````\nndarray\n``````\n\n``````\nPyRefPort\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.receiver.html",
    "title": "lava.proc.receiver — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nCPU model for the Receiver process.\n\nThe process saves any accumulated input messages as a payload variable.\n\nalias of[Receiver](https://lava-nc.org/lava/lava.proc.receiver.html#lava.proc.receiver.process.Receiver)`Receiver`Receiver\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nExecute spiking phase, integrate incomming input and update\npayload.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess saving input messages as a payload variable.\n\nshape(tuple(int)) – Shape of the population of process units.\n\nname(str) – Name of the Process. Default is ‘Process_ID’, where ID is an\ninteger value that is determined automatically.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig) – Configuration options for logging.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess saving input messages as a payload variable. For up to 32bit\npayload.\n\nshape(tuple(int)) – Shape of the population of process units.\n\nname(str) – Name of the Process. Default is ‘Process_ID’, where ID is an\ninteger value that is determined automatically.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig) – Configuration options for logging.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nPyInPort\n``````\n\n``````\nReceiver\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nndarray\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.sdn.html",
    "title": "lava.proc.sdn — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nDelta encodind dynamics method\n\nact_data(np.ndarray) – data to be encoded\n\ndelta encoded data\n\nnp.ndarray\n\nBases:[AbstractSigmaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaModel)`AbstractSigmaModel`AbstractSigmaModel,[AbstractDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractDeltaModel)`AbstractDeltaModel`AbstractDeltaModel\n\nSigma Delta activation dynamics. UNIT and RELU activations are\nsupported.\n\nsigma_data(np.ndarray) – sigma decoded data\n\nactivation output\n\nnp.ndarray\n\nNotImplementedError– if activation mode other than UNIT or RELU is encountered.\n\n`ndarray`ndarray\n\nBases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nSigma decoding dynamics method\n\na_in_data(np.ndarray) – Input data\n\ndecoded data\n\nnp.ndarray\n\nBases:[AbstractDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractDeltaModel)`AbstractDeltaModel`AbstractDeltaModel\n\nFixed point implementation of Delta encoding.\n\nalias of[Delta](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.Delta)`Delta`Delta\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractDeltaModel)`AbstractDeltaModel`AbstractDeltaModel\n\nFloating point implementation of Delta encoding.\n\nalias of[Delta](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.Delta)`Delta`Delta\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractSigmaDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaDeltaModel)`AbstractSigmaDeltaModel`AbstractSigmaDeltaModel\n\nFixed point implementation of Sigma Delta neuron.\n\nalias of[SigmaDelta](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.SigmaDelta)`SigmaDelta`SigmaDelta\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractSigmaDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaDeltaModel)`AbstractSigmaDeltaModel`AbstractSigmaDeltaModel\n\nFixed point implementation of Sigma Delta neuron.\n\nalias of[SigmaDelta](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.SigmaDelta)`SigmaDelta`SigmaDelta\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractSigmaDeltaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaDeltaModel)`AbstractSigmaDeltaModel`AbstractSigmaDeltaModel\n\nFloating point implementation of Sigma Delta neuron.\n\nalias of[SigmaDelta](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.SigmaDelta)`SigmaDelta`SigmaDelta\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nFunction that runs in Spiking Phase\n\n`None`None\n\nBases:[AbstractSigmaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaModel)`AbstractSigmaModel`AbstractSigmaModel\n\nFixed point implementation of Sigma decoding\n\nalias of[Sigma](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.Sigma)`Sigma`Sigma\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nBases:[AbstractSigmaModel](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.models.AbstractSigmaModel)`AbstractSigmaModel`AbstractSigmaModel\n\nFloating point implementation of Sigma decoding\n\nalias of[Sigma](https://lava-nc.org/lava/lava.proc.sdn.html#lava.proc.sdn.process.Sigma)`Sigma`Sigma\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nReLU activation implementation\n\nx(np.ndarray) – input array\n\noutput array\n\nnp.ndarray\n\nBases:`IntEnum`IntEnum\n\nEnum for synapse sigma delta activation mode. Options are\nUNIT: 0\nRELU: 1\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nReturn shape of the Process.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nReturn shape of the Process.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nReturn shape of the Process.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nAbstractSigmaModel\n``````\n\n``````\nAbstractDeltaModel\n``````\n\n``````\nndarray\n``````\n\n``````\nDelta\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nAbstractSigmaDeltaModel\n``````\n\n``````\nSigmaDelta\n``````\n\n``````\nSigma\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.proc.spiker.html",
    "title": "lava.proc.spiker — Lava  documentation",
    "content": "Bases:[PyLoihiProcessModel](https://lava-nc.org/lava/lava.magma.core.model.py.html#lava.magma.core.model.py.model.PyLoihiProcessModel)`PyLoihiProcessModel`PyLoihiProcessModel\n\nCPU model for the Spiker process.\n\nThe process sends messages at the specified rate with a specified payload.\n\nalias of[Spiker](https://lava-nc.org/lava/lava.proc.spiker.html#lava.proc.spiker.process.Spiker)`Spiker`Spiker\n\nalias of[LoihiProtocol](https://lava-nc.org/lava/lava.magma.core.sync.protocols.html#lava.magma.core.sync.protocols.loihi_protocol.LoihiProtocol)`LoihiProtocol`LoihiProtocol\n\nExecute spiking phase, send a payload at the pre-determined rate.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess emitting a specified payload at a given rate.\n\nshape(tuple(int)) – Shape of the population of process units.\n\nperiod(int) – Number of timesteps between subsequent emissions of payload.\nNote that the first spike is emitted at time step period + 1.\n\npayload(int) – A value to be send with every output message.\n\nname(str) – Name of the Process. Default is ‘Process_ID’, where ID is an\ninteger value that is determined automatically.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig) – Configuration options for logging.\n\nBases:[AbstractProcess](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.AbstractProcess)`AbstractProcess`AbstractProcess\n\nProcess emitting a specified payload at a given rate.\nProvides 32bit payloads, and separate payloads for each neuron.\nOther than the default Spiker process, this process actually starts spiking\nat timestep = period.\n\nshape(tuple(int)) – Shape of the population of process units.\n\nperiod(int) – Number of timesteps between subsequent emissions of payload.\n\npayload(int) – A value to be send with every output message.\nCan be in [0, 2**32 - 1] if signed==False,\nor in [-2**31, 2**31 - 1] if signed==True.\n\nsigned(bool) – True if signed payload, False otherwise.\n\nname(str) – Name of the Process. Default is ‘Process_ID’, where ID is an\ninteger value that is determined automatically.\n\nlog_config([LogConfig](https://lava-nc.org/lava/lava.magma.core.process.html#lava.magma.core.process.process.LogConfig)LogConfig) – Configuration options for logging.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nPyLoihiProcessModel\n``````\n\n``````\nndarray\n``````\n\n``````\nSpiker\n``````\n\n``````\nLoihiProtocol\n``````\n\n``````\nPyOutPort\n``````\n\n``````\nAbstractProcess\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/lava.utils.dataloader.html",
    "title": "lava.utils.dataloader — Lava  documentation",
    "content": "Bases:`object`object\n\ndownload_path (str): path of downloaded raw MNIST dataset in IDX\nformat\nsave_path (str): path at which processed npy file will be saved\n\nAfter loading data = np.load(), data is a np.array of np.arrays.\ntrain_imgs = data[0][0]; shape = 60000 x 28 x 28\ntest_imgs = data[1][0]; shape = 10000 x 28 x 28\ntrain_labels = data[0][1]; shape = 60000 x 1\ntest_labels = data[1][1]; shape = 10000 x 1\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/lava.utils.html",
    "title": "Lava Utils — Lava  documentation",
    "content": "Bases:`object`object\n\nBase class for profiling execution time, energy and other\nmetrics on different resources. Depending on the computing\nressource an appropriate profiler needs to be chosen. The run\nconfiguration is used to choose the related profiler, if there\nis one.\n\nDecide which profiler is needed based on the run\nconfiguration.\n\n[Profiler](https://lava-nc.org/lava/lava.utils.html#lava.utils.profiler.Profiler)`Profiler`Profiler\n\nBases:`object`object\n\nSets the os environment for execution on Loihi.\n:type partititon:`Optional`Optional[`str`str]\n:param partititon: Loihi partition name, by default None.\n:type partititon: str, optional\n\n`None`None\n\n`Callable`Callable[[`TypeVar`TypeVar(`_T`_T)],`TypeVar`TypeVar(`_T`_T)]\n\nBases:`property`property\n\nWraps static member function of a class as a static property of that\nclass.\n\nBases:`object`object\n\nBases:`Enum`Enum\n\nEnumeration of sign mode of weights.\n\nTruncate the least significant bits of the weight matrix given the\nsign mode and number of weight bits.\n\nweights(numpy.ndarray,spmatrix) – Weight matrix that is to be truncated.\n\nsign_mode([SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode) – Sign mode to use for truncation.\n\nnum_bits(int) – Number of bits to use to clip the weights to.\n\nlearning_simulation(bool,optional) – Boolean flag, specifying if this method is used in context of learning\n(in simulation).\n\nTruncated weight matrix.\n\nnumpy.ndarray\n\nDetermines the sign mode that describes the values in the given\nweight matrix.\n\nweights(numpy.ndarray) – Weight matrix\n\nThe sign mode that best describes the values in the given weight\nmatrix.\n\n[SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode\n\nOptimizes the weight matrix to best fit in Loihi’s synapse.\n\nweights(np.ndarray,spmatrix) – Standard 8-bit signed weight matrix.\n\nsign_mode([SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode) – Determines whether the weights are purely excitatory, inhibitory,\nor mixed sign.\n\nloihi2(bool,optional) – Flag to optimize for Loihi 2. By default False.\n\nAn object that wraps the optimized weight matrix and weight parameters.\n\n[OptimizedWeights](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.OptimizedWeights)OptimizedWeights\n\nTruncate the least significant bits of the weight matrix given the\nsign mode and number of weight bits.\n\nweights(numpy.ndarray,spmatrix) – Weight matrix that is to be truncated.\n\nsign_mode([SignMode](https://lava-nc.org/lava/lava.utils.html#lava.utils.weightutils.SignMode)SignMode) – Sign mode to use for truncation. See SignMode class for the\ncorrect values.\n\nnum_weight_bits(int) – Number of bits to use for the weight matrix.\n\nmax_num_weight_bits(int,optional) – Maximum number of bits that can be used to represent weights. Default\nis 8.\n\nTruncated weight matrix.\n\nnumpy.ndarray\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nProfiler\n``````\n\n``````\nstr\n``````\n\n``````\nint\n``````\n\n``````\nUnion\n``````\n\n``````\nndarray\n``````\n\n``````\nspmatrix\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/end_to_end/tutorial00_tour_through_lava.html",
    "title": "Walk through Lava — Lava  documentation",
    "content": "Copyright (C) 2022 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nLava is an open-source software library dedicated to the development of algorithms for neuromorphic computation. To that end, Lava provides an easy-to-use Python interface for creating the bits and pieces required for such a neuromorphic algorithm. For easy development, Lava allows to run and test all neuromorphic algorithms on standard von-Neumann hardware like CPU, before they can be deployed on neuromorphic processors such as the Intel Loihi 1/2 processor to leverage their speed and power\nadvantages. Furthermore, Lava is designed to be extensible to custom implementations of neuromorphic behavior and to support new hardware backends.\n\nLava can fundamentally be used at two different levels: Either by using existing resources which can be used to create complex algorithms while requiring almost no deep neuromorphic knowledge. Or, for custom behavior, Lava can be easily extended with new behavior defined in Python and C.\n\nThis tutorial gives an high-level overview over the key components of Lava. For illustration, we will use a simple working example: a feed-forward multi-layer LIF network executed locally on CPU. In the first section of the tutorial, we use the internal resources of Lava to construct such a network and in the second section, we demonstrate how to extend Lava with a custom process using the example of an input generator.\n\nIn addition to the core Lava library described in the present tutorial, the following tutorials guide you to use high level functionalities: -[lava-dl](https://github.com/lava-nc/lava-dl)lava-dlfor deep learning applications -[lava-optimization](https://github.com/lava-nc/lava-optimization)lava-optimizationfor constraint optimization -[lava-dnf](https://github.com/lava-nc/lava-dnf)lava-dnffor Dynamic Neural Fields\n\nIn this section, we will use a simple 2-layered feed-forward network of LIF neurons executed on CPU as canonical example.\n\nThe fundamental building block in the Lava architecture is the`Process`Process. A`Process`Processdescribes a functional group, such as a population of`LIF`LIFneurons, which runs asynchronously and parallel and communicates via`Channels`Channels. A`Process`Processcan take different forms and does not necessarily be a population of neurons, for example it could be a complete network, program code or the interface to a sensor (see figure below).\n\nFor convenience, Lava provides a growing Process Library in which many commonly used`Processes`Processesare publicly available. In the first section of this tutorial, we will use the`Processes`Processesof the Process Library to create and execute a multi-layer LIF network. Take a look at the[documentation](https://lava-nc.org)documentationto find out what other`Processes`Processesare implemented in the Process Library.\n\nLet’s start by importing the classes`LIF`LIFand`Dense`Denseand take a brief look at the docstring.\n\nThe docstring gives insights about the parameters and internal dynamics of the`LIF`LIFneuron.`Dense`Denseis used to connect to a neuron population in an all-to-all fashion, often implemented as a matrix-vector product.\n\nIn the next box, we will create the`Processes`Processeswe need to implement a multi-layer LIF (LIF-Dense-LIF) network.\n\nAs you can see, we can either specify parameters with scalars, then all units share the same initial value for this parameter, or with a tuple (or list, or numpy array) to set the parameter individually per unit.\n\nLet’s investigate the objects we just created. As mentioned before, both,`LIF`LIFand`Dense`Denseare examples of`Processes`Processes, the main building block in Lava.\n\nA`Process`Processholds three key components (see figure below):\n\nInput ports\n\nVariables\n\nOutput ports\n\nThe`Vars`Varsare used to store internal states of the`Process`Processwhile the`Ports`Portsare used to define the connectivity between the`Processes`Processes. Note that a`Process`Processonly defines the`Vars`Varsand`Ports`Portsbut not the behavior. This is done separately in a`ProcessModel`ProcessModel. To separate the interface from the behavioral implementation has the advantage that we can define the behavior of a`Process`Processfor multiple hardware backends using multiple`ProcessModels`ProcessModelswithout changing the\ninterface. We will get into more detail about`ProcessModels`ProcessModelsin the second part of this tutorial.\n\nLet’s take a look at the`Ports`Portsof the`LIF`LIFand`Dense`Denseprocesses we just created. The output`Port`Portof the`LIF`LIFneuron is called`s_out`s_out, which stands for ‘spiking’ output. The input`Port`Portis called`a_in`a_inwhich stands for ‘activation’ input.\n\nFor example, we can see the size of the`Port`Portwhich is in particular important because`Ports`Portscan only connect if their shape matches.\n\nNow that we know about the input and output`Ports`Portsof the`LIF`LIFand`Dense`Dense`Processes`Processes, we can`connect`connectthe network to complete the LIF-Dense-LIF structure.\n\nAs can be seen in the figure above, by`connecting`connectingtwo processes, a`Channel`Channelbetween them is created which means that messages between those two`Processes`Processescan be exchanged.\n\nSimilar to the`Ports`Ports, we can investigate the`Vars`Varsof a`Process`Process.\n\n`Vars`Varsare also accessible as member variables. We can print details of a specific`Var`Varto see the shape, initial value and current value. The`shareable`shareableattribute controls whether a`Var`Varcan be manipulated via remote memory access. Learn more about about this topic in the[remote memory access tutorial](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial07_remote_memory_access.ipynb)remote memory access tutorial.\n\nWe can take a look at the random weights of`Dense`Denseby calling the`get`getfunction.\n\nNote: There is also a`set`setfunction available to change the value of a`Var`Varafter the network was executed.\n\nIn order to record the evolution of the internal`Vars`Varsover time, we need a`Monitor`Monitor. For this example, we want to record the membrane potential of both`LIF`LIFProcesses, hence we need two`Monitors`Monitors.\n\nWe can define the`Var`Varthat a`Monitor`Monitorshould record, as well as the recording duration, using the`probe`probefunction.\n\nNow, that we finished to set up the network and recording`Processes`Processes, we can execute the network by simply calling the`run`runfunction of one of the`Processes`Processes.\n\nThe`run`runfunction requires two parameters, a`RunCondition`RunConditionand a`RunConfig`RunConfig. The`RunCondition`RunConditiondefineshowthe network runs (i.e. for how long) while the`RunConfig`RunConfigdefines on which hardware backend the`Processes`Processesshould be mapped and executed.\n\nLet’s investigate the different possibilities for`RunConditions`RunConditions. One option is`RunContinuous`RunContinuouswhich executes the network continuously and non-blocking until`pause`pauseor`stop`stopis called.\n\nThe second option is`RunSteps`RunSteps, which allows you to define an exact amount of time steps the network should run.\n\nFor this example. we will use`RunSteps`RunStepsand let the network run exactly`num_steps`num_stepstime steps.\n\nNext, we need to provide a`RunConfig`RunConfig. As mentioned above, The`RunConfig`RunConfigdefines on which hardware backend the network is executed.\n\nFor example, we could run the network on the Loihi1 processor using the`Loihi1HwCfg`Loihi1HwCfg, on Loihi2 using the`Loihi2HwCfg`Loihi2HwCfg, or on CPU using the`Loihi1SimCfg`Loihi1SimCfg. The compiler and runtime then automatically select the correct`ProcessModels`ProcessModelssuch that the`RunConfig`RunConfigcan be fulfilled.\n\nFor this section of the tutorial, we will run our network on CPU, later we will show how to run the same network on the Loihi2 processor.\n\nFinally, we can simply call the`run`runfunction of the second`LIF`LIFprocess and provide the`RunConfig`RunConfigand`RunCondition`RunCondition.\n\nAfter the simulation has stopped, we can call`get_data`get_dataon the two monitors to retrieve the recorded membrane potentials.\n\nAlternatively, we can also use the provided`plot`plotfunctionality of the`Monitor`Monitor, to plot the recorded data. As we can see, the bias of the first`LIF`LIFpopulation drives the membrane potential to the threshold which generates output spikes. Those output spikes are passed through the`Dense`Denselayer as input to the second`LIF`LIFpopulation.\n\nAs a last step we must stop the runtime by calling the`stop`stopfunction.`Stop`Stopwill terminate the`Runtime`Runtimeand all states will be lost.\n\nThere are many tools available in the Process Library to construct basic networks\n\nThe fundamental building block in Lava is the`Process`Process\n\nEach`Process`Processconsists of`Vars`Varsand`Ports`Ports\n\nA`Process`Processdefines a common interface across hardware backends, but not the behavior\n\nThe`ProcessModel`ProcessModeldefines the behavior of a`Process`Processfor a specific hardware backend\n\n`Vars`Varsstore internal states,`Ports`Portsare used to implement communication channels between processes\n\nThe`RunConfig`RunConfigdefines on which hardware backend the network runs\n\n[Processes](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial02_processes.ipynb)Processesand[hierarchical Processes](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial06_hierarchical_processes.ipynb)hierarchical Processes\n\n[Possible connectivity patterns](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial05_connect_processes.ipynb)Possible connectivity patterns\n\n[Remote memory access](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial07_remote_memory_access.ipynb)Remote memory access\n\n[Execution](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial04_execution.ipynb)Execution\n\nIn the previous section of this tutorial, we used`Processes`Processeswhich were available in the Process Library. In many cases, this might be sufficient and the Process Library is constantly growing. Nevertheless, the Process Library can not cover all use-cases. Hence, Lava makes it very easy to create new`Processes`Processes. In the following section, we want to show how to implement a custom`Process`Processand the corresponding behavior using a`ProcessModel`ProcessModel.\n\nThe example of the previous section implemented a bias driven LIF-Dense-LIF network. One crucial aspect which is missing this example, is the input/output interaction with sensors and actuators. Commonly used sensors would be Dynamic Vision Sensors or artificial cochleas, but for demonstration purposes we will implement a simple`SpikeGenerator`SpikeGenerator. The purpose of the`SpikeGenerator`SpikeGeneratoris to output random spikes to drive the LIF-Dense-LIF network.\n\nLet’s start by importing the necessary classes from Lava.\n\nAll`Processes`Processesin Lava inherit from a common base class called`AbstractProcess`AbstractProcess. Additionally, we need`Var`Varfor storing the spike probability and`OutPort`OutPortto define the output connections for our`SpikeGenerator`SpikeGenerator.\n\nThe constructor of`Var`Varrequires the shape of the data to be stored and some initial value. We use this functionality to store the spike data. Similarly, we define an`OutPort`OutPortfor our`SpikeGenerator`SpikeGenerator.\n\nAs mentioned earlier, the`Process`Processonly defines the interface but not the behavior of the`SpikeGenerator`SpikeGenerator. We will do that in a separate`ProcessModel`ProcessModelwhich has the advantage that we can define the behavior of a`Process`Processon different hardware backends without changing the interface (see figure below). More details about the different kinds of`ProcessModels`ProcessModelscan be found in the dedicated in-depth tutorials\n([here](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial03_process_models.ipynb)hereand[here](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial06_hierarchical_processes.ipynb)here). Lava automatically selects the correct`ProcessModel`ProcessModelfor each`Process`Processgiven the`RunConfig`RunConfig.\n\nSo, let’s go ahead and define the behavior of the`SpikeGenerator`SpikeGeneratoron a CPU in Python. Later in this tutorial we will show how to implement the same behavior on an embedded CPU in C and how to implement the behavior of a`LIF`LIFprocess on a Loihi2 neuro-core.\n\nWe first import all necessary classes from Lava.\n\nAll`ProcessModels`ProcessModelsdefined to run on CPU are written in Python and inherit from the common class called`PyLoihiProcessModel`PyLoihiProcessModel. Further, we use the decorators`requires`requiresand`implements`implementsto define which computational resources (i.e. CPU, GPU, Loihi1NeuroCore, Loihi2NeuroCore) are required to execute this`ProcessModel`ProcessModeland which`Process`Processit implements. Finally, we need to specify the types of`Vars`Varsand`Ports`Portsin our`SpikeGenerator`SpikeGeneratorusing`LavaPyType`LavaPyTypeand`PyOutPort`PyOutPort.\n\nNote: It is important to mention that the`ProcessModel`ProcessModelneeds to implement the exact same Vars and Ports of the parent process using the same class attribute names.\n\nAdditionally, we define that our`PySpikeGeneratorModel`PySpikeGeneratorModelfollows the`LoihiProtocol`LoihiProtocol. The`LoihiProtocol`LoihiProtocoldefines that the execution of a model follows a specific sequence of phases. For example, there is thespiking phase(`run_spk`run_spk) in which input spikes are received, internal`Vars`Varsare updated and output spikes are sent. There are other phases such as thelearning phase(`run_lrn`run_lrn) in which online learning takes place, or thepost management phase(`run_post_mgmt`run_post_mgmt) in\nwhich memory content is updated. As the`SpikeGenerator`SpikeGeneratorbasically just sends out spikes, the correct place to implement its behavior is the`run_spk`run_spkphase.\n\nTo implement the behavior, we need to have access to the global simulation time. We can easily access the simulation time with`self.time_step`self.time_stepand use that to index the`spike_data`spike_dataand send out the corresponding spikes through the`OutPort`OutPort.\n\nNote: For the`SpikeGenerator`SpikeGeneratorwe only needed an`OutPort`OutPortwhich provides the`send`sendfunction to send data. For the`InPort`InPortthe corresponding function to receive data is called`recv`recv.\n\nNext, we want to redefine our network as in the example before with the exception that we turn off all biases.\n\nWe instantiate the`SpikeGenerator`SpikeGeneratoras usual with the shape of the fist`LIF`LIFpopulation.\n\nTo define the connectivity between the`SpikeGenerator`SpikeGeneratorand the first`LIF`LIFpopulation, we us another`Dense`DenseLayer. Now, we can connect its`OutPort`OutPortto the`InPort`InPortof the`Dense`Denselayer and the`OutPort`OutPortof the`Dense`Denselayer to the`InPort`InPortof the first`LIF`LIFpopulation.\n\nNow that our network is complete, we can execute it the same way as before using the`RunCondition`RunConditionand`RunConfig`RunConfigwe created in the previous example.\n\nAnd now, we can retrieve the recorded data and plot the membrane potentials of the two`LIF`LIFpopulations.\n\nAs we can see, the spikes provided by the`SpikeGenerator`SpikeGeneratorare sucessfully sent to the first`LIF`LIFpopulation. which in turn sends its output spikes to the second`LIF`LIFpopulation.\n\nDefine custom a`Process`Processby inheritance from`AbstractProcess`AbstractProcess\n\n`Vars`Varsare used to store internal data in the`Process`Process\n\n`Ports`Portsare used to connect to other`Processes`Processes\n\nThe behavior of a`Process`Processis defined in a`ProcessModel`ProcessModel\n\n`ProcessModels`ProcessModelsare aware of their hardware backend, they get selected automatically by the compiler/runtime\n\n`PyProcessModels`PyProcessModelsrun on CPU\n\nNumber and names of`Vars`Varsand`Ports`Portsof a`ProcessModel`ProcessModelmust match those of the`Process`Processit implements\n\n[ProcessModels](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial03_process_models.ipynb)ProcessModels\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive deeper with the[in-depth tutorials](https://github.com/lava-nc/lava/tree/main/tutorials/in_depth)in-depth tutorials.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to[our newsletter](http://eepurl.com/hJCyhb)our newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.dense.process import Dense\n\nLIF?\n``````\n\n``````\nInit signature:LIF(*args,**kwargs)Docstring:Leaky-Integrate-and-Fire (LIF) neural Process.\n\nLIF dynamics abstracts to:\nu[t] = u[t-1] * (1-du) + a_in         # neuron current\nv[t] = v[t-1] * (1-dv) + u[t] + bias  # neuron voltage\ns_out = v[t] > vth                    # spike if threshold is exceeded\nv[t] = 0                              # reset at spike\n\nParameters\n----------\nshape : tuple(int)\n    Number and topology of LIF neurons.\nu : float, list, numpy.ndarray, optional\n    Initial value of the neurons' current.\nv : float, list, numpy.ndarray, optional\n    Initial value of the neurons' voltage (membrane potential).\ndu : float, optional\n    Inverse of decay time-constant for current decay. Currently, only a\n    single decay can be set for the entire population of neurons.\ndv : float, optional\n    Inverse of decay time-constant for voltage decay. Currently, only a\n    single decay can be set for the entire population of neurons.\nbias_mant : float, list, numpy.ndarray, optional\n    Mantissa part of neuron bias.\nbias_exp : float, list, numpy.ndarray, optional\n    Exponent part of neuron bias, if needed. Mostly for fixed point\n    implementations. Ignored for floating point implementations.\nvth : float, optional\n    Neuron threshold voltage, exceeding which, the neuron will spike.\n    Currently, only a single threshold can be set for the entire\n    population of neurons.\n\nExample\n-------\n>>> lif = LIF(shape=(200, 15), du=10, dv=5)\nThis will create 200x15 LIF neurons that all have the same current decay\nof 10 and voltage decay of 5.Init docstring:Initializes a new Process.File:~/Documents/lava/src/lava/proc/lif/process.pyType:ProcessPostInitCallerSubclasses:LIFReset\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\n\n# Create processes\nlif1 = LIF(shape=(3, ),                         # Number and topological layout of units in the process\n           vth=10.,                             # Membrane threshold\n           dv=0.1,                              # Inverse membrane time-constant\n           du=0.1,                              # Inverse synaptic time-constant\n           bias_mant=(1.1, 1.2, 1.3),           # Bias added to the membrane voltage in every timestep\n           name=\"lif1\")\n\ndense = Dense(weights=np.random.rand(2, 3),     # Initial value of the weights, chosen randomly\n              name='dense')\n\nlif2 = LIF(shape=(2, ),                         # Number and topological layout of units in the process\n           vth=10.,                             # Membrane threshold\n           dv=0.1,                              # Inverse membrane time-constant\n           du=0.1,                              # Inverse synaptic time-constant\n           bias_mant=0.,                        # Bias added to the membrane voltage in every timestep\n           name='lif2')\n``````\n\n``````\n[3]:\n``````\n\n``````\nfor proc in [lif1, lif2, dense]:\n    for port in proc.in_ports:\n        print(f'Proc: {proc.name:<5}  Name: {port.name:<5}  Size: {port.size}')\n    for port in proc.out_ports:\n        print(f'Proc: {proc.name:<5}  Name: {port.name:<5}  Size: {port.size}')\n``````\n\n``````\nProc: lif1   Name: a_in   Size: 3\nProc: lif1   Name: s_out  Size: 3\nProc: lif2   Name: a_in   Size: 2\nProc: lif2   Name: s_out  Size: 2\nProc: dense  Name: s_in   Size: 3\nProc: dense  Name: a_out  Size: 2\n``````\n\n``````\n[4]:\n``````\n\n``````\nlif1.s_out.connect(dense.s_in)\ndense.a_out.connect(lif2.a_in)\n``````\n\n``````\n[5]:\n``````\n\n``````\nfor var in lif1.vars:\n    print(f'Var: {var.name:<9}  Shape: {var.shape}  Init: {var.init}')\n``````\n\n``````\nVar: bias_exp   Shape: (3,)  Init: 0\nVar: bias_mant  Shape: (3,)  Init: (1.1, 1.2, 1.3)\nVar: du         Shape: (1,)  Init: 0.1\nVar: dv         Shape: (1,)  Init: 0.1\nVar: u          Shape: (3,)  Init: 0\nVar: v          Shape: (3,)  Init: 0\nVar: vth        Shape: (1,)  Init: 10.0\n``````\n\n``````\n[6]:\n``````\n\n``````\ndense.weights.get()\n``````\n\n``````\n[6]:\n``````\n\n``````\narray([[0.39348157, 0.11634913, 0.17421253],\n       [0.51148951, 0.9046173 , 0.04863679]])\n``````\n\n``````\n[7]:\n``````\n\n``````\nfrom lava.proc.monitor.process import Monitor\n\nmonitor_lif1 = Monitor()\nmonitor_lif2 = Monitor()\n\nnum_steps = 100\n\nmonitor_lif1.probe(lif1.v, num_steps)\nmonitor_lif2.probe(lif2.v, num_steps)\n``````\n\n``````\n[8]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunContinuous\n\nrun_condition = RunContinuous()\n``````\n\n``````\n[9]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\n\nrun_condition = RunSteps(num_steps=num_steps)\n``````\n\n``````\n[10]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\nrun_cfg = Loihi1SimCfg(select_tag=\"floating_pt\")\n``````\n\n``````\n[11]:\n``````\n\n``````\nlif2.run(condition=run_condition, run_cfg=run_cfg)\n``````\n\n``````\n[12]:\n``````\n\n``````\ndata_lif1 = monitor_lif1.get_data()\ndata_lif2 = monitor_lif2.get_data()\n``````\n\n``````\n[13]:\n``````\n\n``````\nimport matplotlib\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\n# Create a subplot for each monitor\nfig = plt.figure(figsize=(16, 5))\nax0 = fig.add_subplot(121)\nax1 = fig.add_subplot(122)\n\n# Plot the recorded data\nmonitor_lif1.plot(ax0, lif1.v)\nmonitor_lif2.plot(ax1, lif2.v)\n``````\n\n``````\n[14]:\n``````\n\n``````\nlif2.stop()\n``````\n\n``````\n[15]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import OutPort\n``````\n\n``````\n[16]:\n``````\n\n``````\nclass SpikeGenerator(AbstractProcess):\n    \"\"\"Spike generator process provides spikes to subsequent Processes.\n\n    Parameters\n    ----------\n    shape: tuple\n        defines the dimensionality of the generated spikes per timestep\n    spike_prob: int\n        spike probability in percent\n    \"\"\"\n    def __init__(self, shape: tuple, spike_prob: int) -> None:\n        super().__init__()\n        self.spike_prob = Var(shape=(1, ), init=spike_prob)\n        self.s_out = OutPort(shape=shape)\n``````\n\n``````\n[17]:\n``````\n\n``````\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.model.py.ports import PyOutPort\n``````\n\n``````\n[18]:\n``````\n\n``````\n@implements(proc=SpikeGenerator, protocol=LoihiProtocol)\n@requires(CPU)\nclass PySpikeGeneratorModel(PyLoihiProcessModel):\n    \"\"\"Spike Generator process model.\"\"\"\n    spike_prob: int = LavaPyType(int, int)\n    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)\n\n    def run_spk(self) -> None:\n        # Generate random spike data\n        spike_data = np.random.choice([0, 1], p=[1 - self.spike_prob/100, self.spike_prob/100], size=self.s_out.shape[0])\n\n        # Send spikes\n        self.s_out.send(spike_data)\n``````\n\n``````\n[19]:\n``````\n\n``````\n# Create processes\nlif1 = LIF(shape=(3, ),                         # Number of units in this process\n           vth=10.,                             # Membrane threshold\n           dv=0.1,                              # Inverse membrane time-constant\n           du=0.1,                              # Inverse synaptic time-constant\n           bias_mant=0.,                        # Bias added to the membrane voltage in every timestep\n           name=\"lif1\")\n\ndense = Dense(weights=np.random.rand(2, 3),     # Initial value of the weights, chosen randomly\n              name='dense')\n\nlif2 = LIF(shape=(2, ),                         # Number of units in this process\n           vth=10.,                             # Membrane threshold\n           dv=0.1,                              # Inverse membrane time-constant\n           du=0.1,                              # Inverse synaptic time-constant\n           bias_mant=0.,                        # Bias added to the membrane voltage in every timestep\n           name='lif2')\n\n# Connect the OutPort of lif1 to the InPort of dense\nlif1.s_out.connect(dense.s_in)\n\n# Connect the OutPort of dense to the InPort of lif2\ndense.a_out.connect(lif2.a_in)\n\n# Create Monitors to record membrane potentials\nmonitor_lif1 = Monitor()\nmonitor_lif2 = Monitor()\n\n# Probe membrane potentials from the two LIF populations\nmonitor_lif1.probe(lif1.v, num_steps)\nmonitor_lif2.probe(lif2.v, num_steps)\n``````\n\n``````\n[20]:\n``````\n\n``````\n# Instantiate SpikeGenerator\nspike_gen = SpikeGenerator(shape=(lif1.a_in.shape[0], ), spike_prob=7)\n\n# Instantiate Dense\ndense_input = Dense(weights=np.eye(lif1.a_in.shape[0])) # one-to-one connectivity\n\n# Connect spike_gen to dense_input\nspike_gen.s_out.connect(dense_input.s_in)\n\n# Connect dense_input to LIF1 population\ndense_input.a_out.connect(lif1.a_in)\n``````\n\n``````\n[21]:\n``````\n\n``````\nlif2.run(condition=run_condition, run_cfg=run_cfg)\n``````\n\n``````\n[22]:\n``````\n\n``````\n# Create a subplot for each monitor\nfig = plt.figure(figsize=(16, 5))\nax0 = fig.add_subplot(121)\nax1 = fig.add_subplot(122)\n\n# Plot the recorded data\nmonitor_lif1.plot(ax0, lif1.v)\nmonitor_lif2.plot(ax1, lif2.v)\n``````\n\n``````\n[23]:\n``````\n\n``````\nlif2.stop()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/end_to_end/tutorial01_mnist_digit_classification.html",
    "title": "MNIST Digit Classification with Lava — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nMotivation: In this tutorial, we will build a Lava Process for an MNIST classifier, using the Lava Processes for LIF neurons and Dense connectivity. The tutorial is useful to get started with Lava in a few minutes.\n\nhave the[Lava framework installed](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Lava framework installed\n\nare familiar with the[Process concept in Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Process concept in Lava\n\nhow Lava Process(es) can perform the MNIST digit classification task using[Leaky Integrate-and-Fire (LIF)](https://github.com/lava-nc/lava/tree/main/src/lava/proc/lif)Leaky Integrate-and-Fire (LIF)neurons and[Dense (fully connected)](https://github.com/lava-nc/lava/tree/main/src/lava/proc/dense)Dense (fully connected)connectivity.\n\nhow to create a Process\n\nhow to create Python ProcessModels\n\nhow to connect Processes\n\nhow to execute them\n\nIn this tutorial, we will build a multi-layer feed-forward classifier without any convolutional layers. The architecture is shown below.\n\nImportant Note:\n\nThe classifier is a simple feed-forward model using pre-trained network parameters (weights and biases). It illustrates how to build, compile and run a functional model in Lava. Please refer to[Lava-DL](https://github.com/lava-nc/lava-dl)Lava-DLto understand how to train deep networks with Lava.\n\nThe 3 Processes shown above are: -SpikeInputProcess - generates spikes via integrate and fire dynamics, using the image input -ImageClassifierProcess - encapsulates feed-forward architecture of Dense connectivity and LIF neurons -OutputProcess - accumulates output spikes from the feed-forward process and infers the class label\n\nBelow we create the Lava Process classes. We need to define only the structure of the process here. The details about how the Process will be executed are specified in the[ProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModelsbelow.\n\nAs mentioned above, we define Processes for - converting input images to binary spikes from those biases (SpikeInput), - the 3-layer fully connected feed-forward network (MnistClassifier) - accumulating the output spikes and inferring the class for an input image (OutputProcess)\n\nDecorators for ProcessModels: -`@implements`@implements: associates a ProcessModel with a Process through the argument`proc`proc. Using`protocol`protocolargument, we will specify the synchronization protocol used by the ProcessModel. In this tutorial, all ProcessModels execute according to the`LoihiProtocol`LoihiProtocol. Which means, similar to the Loihi chip, each time-step is divided intospiking,pre-management,post-management, andlearningphases. It is necessary to specify behaviour of a\nProcessModel during the spiking phase using`run_spk`run_spkfunction. Other phases are optional. -`@requires`@requires: specifies the hardware resource on which a ProcessModel will be executed. In this tutorial, we will execute all ProcessModels on a CPU.\n\nSpikingInput ProcessModel\n\nImageClassifier ProcessModel\n\nNotice that the following process model is further decomposed into sub-Processes, which implement LIF neural dynamics and Dense connectivity. We will not go into the details of how these are implemented in this tutorial.\n\nAlso notice that aSubProcessModeldoes not actually contain any concrete execution. This is handled by the ProcessModels of the constituent Processes.\n\nOutputProcess ProcessModel\n\nIf you receive an`UnpicklingError`UnpicklingErrorwhen instantiating the`ImageClassifier`ImageClassifier, make sure to download the pretrained weights from GitHub LFS in the current directory using:\n\nBelow, we will run the classifier process in a loop of`num_images`num_imagesnumber of iterations. Each iteration will run the Process for`num_steps_per_image`num_steps_per_imagenumber of time-steps.\n\nWe take this approach to clear the neural states of all three LIF layers inside the classifier after every image. We need to clear the neural states, because the network parameters were trained assuming clear neural states for each inference.\n\nNote: Below we have used`Var.set()`Var.set()function to set the values of internal state variables. The same behaviour can be achieved by using`RefPorts`RefPorts. See the[RefPorts tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial07_remote_memory_access.html)RefPorts tutorialto learn more about how to use`RefPorts`RefPortsto access internal state variables of Lava Processes.\n\nImportant Note:\n\nThe classifier is a simple feed-forward model using pre-trained network parameters (weights and biases). It illustrates how to build, compile and run a functional model in Lava. Please refer to[Lava-DL](https://github.com/lava-nc/lava-dl)Lava-DLto understand how to train deep networks with Lava.\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\n[SubProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)SubProcessModelsor[Hierarchical Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)Hierarchical Processes\n\n[RefPorts](https://lava-nc.org/lava/notebooks/in_depth/tutorial07_remote_memory_access.html)RefPorts\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport os\nimport numpy as np\nimport typing as ty\n``````\n\n``````\n[2]:\n``````\n\n``````\n# Import Process level primitives\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n``````\n\n``````\n[3]:\n``````\n\n``````\nclass SpikeInput(AbstractProcess):\n    \"\"\"Reads image data from the MNIST dataset and converts it to spikes.\n    The resulting spike rate is proportional to the pixel value.\"\"\"\n\n    def __init__(self,\n                 vth: int,\n                 num_images: ty.Optional[int] = 25,\n                 num_steps_per_image: ty.Optional[int] = 128):\n        super().__init__()\n        shape = (784,)\n        self.spikes_out = OutPort(shape=shape)  # Input spikes to the classifier\n        self.label_out = OutPort(shape=(1,))  # Ground truth labels to OutputProc\n        self.num_images = Var(shape=(1,), init=num_images)\n        self.num_steps_per_image = Var(shape=(1,), init=num_steps_per_image)\n        self.input_img = Var(shape=shape)\n        self.ground_truth_label = Var(shape=(1,))\n        self.v = Var(shape=shape, init=0)\n        self.vth = Var(shape=(1,), init=vth)\n\n\nclass ImageClassifier(AbstractProcess):\n    \"\"\"A 3 layer feed-forward network with LIF and Dense Processes.\"\"\"\n\n    def __init__(self, trained_weights_path: str):\n        super().__init__()\n\n        # Using pre-trained weights and biases\n        real_path_trained_wgts = os.path.realpath(trained_weights_path)\n\n        wb_list = np.load(real_path_trained_wgts, encoding='latin1', allow_pickle=True)\n        w0 = wb_list[0].transpose().astype(np.int32)\n        w1 = wb_list[2].transpose().astype(np.int32)\n        w2 = wb_list[4].transpose().astype(np.int32)\n        b1 = wb_list[1].astype(np.int32)\n        b2 = wb_list[3].astype(np.int32)\n        b3 = wb_list[5].astype(np.int32)\n\n        self.spikes_in = InPort(shape=(w0.shape[1],))\n        self.spikes_out = OutPort(shape=(w2.shape[0],))\n        self.w_dense0 = Var(shape=w0.shape, init=w0)\n        self.b_lif1 = Var(shape=(w0.shape[0],), init=b1)\n        self.w_dense1 = Var(shape=w1.shape, init=w1)\n        self.b_lif2 = Var(shape=(w1.shape[0],), init=b2)\n        self.w_dense2 = Var(shape=w2.shape, init=w2)\n        self.b_output_lif = Var(shape=(w2.shape[0],), init=b3)\n\n        # Up-level currents and voltages of LIF Processes\n        # for resetting (see at the end of the tutorial)\n        self.lif1_u = Var(shape=(w0.shape[0],), init=0)\n        self.lif1_v = Var(shape=(w0.shape[0],), init=0)\n        self.lif2_u = Var(shape=(w1.shape[0],), init=0)\n        self.lif2_v = Var(shape=(w1.shape[0],), init=0)\n        self.oplif_u = Var(shape=(w2.shape[0],), init=0)\n        self.oplif_v = Var(shape=(w2.shape[0],), init=0)\n\n\nclass OutputProcess(AbstractProcess):\n    \"\"\"Process to gather spikes from 10 output LIF neurons and interpret the\n    highest spiking rate as the classifier output\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        shape = (10,)\n        n_img = kwargs.pop('num_images', 25)\n        self.num_images = Var(shape=(1,), init=n_img)\n        self.spikes_in = InPort(shape=shape)\n        self.label_in = InPort(shape=(1,))\n        self.spikes_accum = Var(shape=shape)  # Accumulated spikes for classification\n        self.num_steps_per_image = Var(shape=(1,), init=128)\n        self.pred_labels = Var(shape=(n_img,))\n        self.gt_labels = Var(shape=(n_img,))\n``````\n\n``````\n[4]:\n``````\n\n``````\n# Import parent classes for ProcessModels\nfrom lava.magma.core.model.sub.model import AbstractSubProcessModel\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n# Import ProcessModel ports, data-types\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\n\n# Import execution protocol and hardware resources\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.resources import CPU\n\n# Import decorators\nfrom lava.magma.core.decorator import implements, requires\n\n# Import MNIST dataset\nfrom lava.utils.dataloader.mnist import MnistDataset\nnp.set_printoptions(linewidth=np.inf)\n``````\n\n``````\n[5]:\n``````\n\n``````\n@implements(proc=SpikeInput, protocol=LoihiProtocol)\n@requires(CPU)\nclass PySpikeInputModel(PyLoihiProcessModel):\n    num_images: int = LavaPyType(int, int, precision=32)\n    spikes_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\n    label_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, np.int32,\n                                      precision=32)\n    num_steps_per_image: int = LavaPyType(int, int, precision=32)\n    input_img: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n    ground_truth_label: int = LavaPyType(int, int, precision=32)\n    v: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n    vth: int = LavaPyType(int, int, precision=32)\n\n    def __init__(self, proc_params):\n        super().__init__(proc_params=proc_params)\n        self.mnist_dataset = MnistDataset()\n        self.curr_img_id = 0\n\n    def post_guard(self):\n        \"\"\"Guard function for PostManagement phase.\n        \"\"\"\n        if self.time_step % self.num_steps_per_image == 1:\n            return True\n        return False\n\n    def run_post_mgmt(self):\n        \"\"\"Post-Management phase: executed only when guard function above\n        returns True.\n        \"\"\"\n        img = self.mnist_dataset.images[self.curr_img_id]\n        self.ground_truth_label = self.mnist_dataset.labels[self.curr_img_id]\n        self.input_img = img.astype(np.int32) - 127\n        self.v = np.zeros(self.v.shape)\n        self.label_out.send(np.array([self.ground_truth_label]))\n        self.curr_img_id += 1\n\n    def run_spk(self):\n        \"\"\"Spiking phase: executed unconditionally at every time-step\n        \"\"\"\n        self.v[:] = self.v + self.input_img\n        s_out = self.v > self.vth\n        self.v[s_out] = 0  # reset voltage to 0 after a spike\n        self.spikes_out.send(s_out)\n``````\n\n``````\n[6]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.dense.process import Dense\n\n@implements(ImageClassifier)\n@requires(CPU)\nclass PyImageClassifierModel(AbstractSubProcessModel):\n    def __init__(self, proc):\n        self.dense0 = Dense(weights=proc.w_dense0.init)\n        self.lif1 = LIF(shape=(64,), bias_mant=proc.b_lif1.init, vth=400,\n                        dv=0, du=4095)\n        self.dense1 = Dense(weights=proc.w_dense1.init)\n        self.lif2 = LIF(shape=(64,), bias_mant=proc.b_lif2.init, vth=350,\n                        dv=0, du=4095)\n        self.dense2 = Dense(weights=proc.w_dense2.init)\n        self.output_lif = LIF(shape=(10,), bias_mant=proc.b_output_lif.init,\n                              vth=1, dv=0, du=4095)\n\n        proc.spikes_in.connect(self.dense0.s_in)\n        self.dense0.a_out.connect(self.lif1.a_in)\n        self.lif1.s_out.connect(self.dense1.s_in)\n        self.dense1.a_out.connect(self.lif2.a_in)\n        self.lif2.s_out.connect(self.dense2.s_in)\n        self.dense2.a_out.connect(self.output_lif.a_in)\n        self.output_lif.s_out.connect(proc.spikes_out)\n\n        # Create aliases of SubProcess variables\n        proc.lif1_u.alias(self.lif1.u)\n        proc.lif1_v.alias(self.lif1.v)\n        proc.lif2_u.alias(self.lif2.u)\n        proc.lif2_v.alias(self.lif2.v)\n        proc.oplif_u.alias(self.output_lif.u)\n        proc.oplif_v.alias(self.output_lif.v)\n``````\n\n``````\n[7]:\n``````\n\n``````\n@implements(proc=OutputProcess, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyOutputProcessModel(PyLoihiProcessModel):\n    label_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, int, precision=32)\n    spikes_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, bool, precision=1)\n    num_images: int = LavaPyType(int, int, precision=32)\n    spikes_accum: np.ndarray = LavaPyType(np.ndarray, np.int32, precision=32)\n    num_steps_per_image: int = LavaPyType(int, int, precision=32)\n    pred_labels: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n    gt_labels: np.ndarray = LavaPyType(np.ndarray, int, precision=32)\n\n    def __init__(self, proc_params):\n        super().__init__(proc_params=proc_params)\n        self.current_img_id = 0\n\n    def post_guard(self):\n        \"\"\"Guard function for PostManagement phase.\n        \"\"\"\n        if self.time_step % self.num_steps_per_image == 0 and \\\n                self.time_step > 1:\n            return True\n        return False\n\n    def run_post_mgmt(self):\n        \"\"\"Post-Management phase: executed only when guard function above\n        returns True.\n        \"\"\"\n        gt_label = self.label_in.recv()\n        pred_label = np.argmax(self.spikes_accum)\n        self.gt_labels[self.current_img_id] = gt_label\n        self.pred_labels[self.current_img_id] = pred_label\n        self.current_img_id += 1\n        self.spikes_accum = np.zeros_like(self.spikes_accum)\n\n    def run_spk(self):\n        \"\"\"Spiking phase: executed unconditionally at every time-step\n        \"\"\"\n        spk_in = self.spikes_in.recv()\n        self.spikes_accum = self.spikes_accum + spk_in\n``````\n\n``````\n[8]:\n``````\n\n``````\nnum_images = 25\nnum_steps_per_image = 128\n\n# Create Process instances\nspike_input = SpikeInput(vth=1,\n                         num_images=num_images,\n                         num_steps_per_image=num_steps_per_image)\nmnist_clf = ImageClassifier(\n    trained_weights_path=os.path.join('.', 'mnist_pretrained.npy'))\noutput_proc = OutputProcess(num_images=num_images)\n\n# Connect Processes\nspike_input.spikes_out.connect(mnist_clf.spikes_in)\nmnist_clf.spikes_out.connect(output_proc.spikes_in)\n# Connect Input directly to Output for ground truth labels\nspike_input.label_out.connect(output_proc.label_in)\n``````\n\n``````\ngitlfsfetch\ngitlfspull\n``````\n\n``````\n[9]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\n# Loop over all images\nfor img_id in range(num_images):\n    print(f\"\\rCurrent image: {img_id+1}\", end=\"\")\n\n    # Run each image-inference for fixed number of steps\n    mnist_clf.run(\n        condition=RunSteps(num_steps=num_steps_per_image),\n        run_cfg=Loihi1SimCfg(select_sub_proc_model=True,\n                             select_tag='fixed_pt'))\n\n    # Reset internal neural state of LIF neurons\n    mnist_clf.lif1_u.set(np.zeros((64,), dtype=np.int32))\n    mnist_clf.lif1_v.set(np.zeros((64,), dtype=np.int32))\n    mnist_clf.lif2_u.set(np.zeros((64,), dtype=np.int32))\n    mnist_clf.lif2_v.set(np.zeros((64,), dtype=np.int32))\n    mnist_clf.oplif_u.set(np.zeros((10,), dtype=np.int32))\n    mnist_clf.oplif_v.set(np.zeros((10,), dtype=np.int32))\n\n# Gather ground truth and predictions before stopping exec\nground_truth = output_proc.gt_labels.get().astype(np.int32)\npredictions = output_proc.pred_labels.get().astype(np.int32)\n\n# Stop the execution\nmnist_clf.stop()\n\naccuracy = np.sum(ground_truth==predictions)/ground_truth.size * 100\n\nprint(f\"\\nGround truth: {ground_truth}\\n\"\n      f\"Predictions : {predictions}\\n\"\n      f\"Accuracy    : {accuracy}\")\n``````\n\n``````\nCurrent image: 25\nGround truth: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1]\nPredictions : [3 0 4 1 4 2 1 3 1 4 3 5 3 6 1 7 2 8 5 9 4 0 9 1 1]\nAccuracy    : 88.0\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/end_to_end/tutorial02_excitatory_inhibitory_network.html",
    "title": "Excitatory-Inhibitory Neural Network with Lava — Lava  documentation",
    "content": "Copyright (C) 2022-23 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nMotivation: In this tutorial, we will build a Lava Process for a neural networks of excitatory and inhibitory neurons (E/I network). E/I networks are a fundamental example of neural networks mimicking the structure of the brain and exhibiting rich dynamical behavior.\n\nhave the[Lava framework installed](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Lava framework installed\n\nare familiar with the[Process concept in Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Process concept in Lava\n\nhow to implement simple E/I Network Lava Process\n\nhow to define and select multiple ProcessModels for the E/I Network, based on Rate and[Leaky Integrate-and-Fire (LIF)](https://github.com/lava-nc/lava/tree/main/src/lava/proc/lif)Leaky Integrate-and-Fire (LIF)neurons\n\nhow to use tags to chose between different ProcessModels when running the Process\n\nthe principle adjustments needed to run bit-accurate ProcessModels\n\nFrom bird’s-eye view, an E/I network is a recurrently coupled network of neurons. Since positive couplings (excitatory synapses) alone lead to a positive feedback loop ultimately causing a divergence in the activity of the network, appropriate negative couplings (inhibitory synapses) need to be introduced to counterbalance this effect. We here require a separation of the neurons into two populations: Neurons can either be inhibitory or excitatory. Such networks exhibit different dynamical\nstates. By introducing a control parameter, we can switch between these states and simultaneously alter the response properties of the network. In the notebook below, we introduce two incarnations of E/I networks with different single neuron models: Rate and LIF neurons. By providing a utility function that maps the weights from rate to LIF networks, we can retain hallmark properties of the dynamic in both networks. Technically, the abstract E/I network is implemented via a LavaProcess, the\nconcrete behavior - Rate and LIF dynamics - is realized with different ProcessModels.\n\nWe define the structure of the E/I Network Lava Process class.\n\ntaudot{r} =  -r + W phi(r) + I_{mathrm{bias}}.\n\nr(i + 1) = (1 - dr) odot r(i) + W phi(r(i)) odot dr + I_{mathrm{bias}} odot dr\n\nend{equation}` Potentially different time scales in the neuron dynamics of excitatory and inhibitory neurons as well as different bias currents for these subpopulations are encoded in the vectorsdrandI_{\\mathrm{bias}}. We use the error function as non-linearity\\phi.\n\nNext, we need to constrain the network with the needed parameters. First, we define the dimensionality of the network which we identify with the total number of neurons as well as the single neuron parameters. We here follow the common choice that the ratio between the number of excitatory and inhibitory neurons equals4and that the connection probability between two arbitrary neurons is identical. The recurrent weights mustbalancethe network, i.e. the average recurrent input to a\nneuron must be less or equal than0. This implies that we need to increase the strength of the inhibitory weights, the`g_factor`g_factor, to at least4. We choose4.5to unambiguously place the network in the inhibition dominated regime. Finally, we set a parameter that controls the response properties of the network by scaling up the recurrent weights, the`q_factor`q_factor.\n\nFinally, we have to set the weights given the above constraints. To this end, we sample the weights randomly from a Gaussian distribution with zero-mean and a standard deviation that scales with the`q_factor`q_factor.\n\nWe first have a look at the activity of the network by plotting the numerical value of the state of the first50neurons.\n\nWe observe that after an initial period the network settles in a fixed point. As it turns out, this is a global stable fixed point of the network dynamics: If we applied a small perturbation, the network would return to the stable state. Such a network is unfit for performing meaningful computations, the dynamics is low-dimensional and rather poor. To better understand this, we apply an additional analysis.\n\nc(tau) = mathrm{Cov}(a(t), a(t+tau))\n\nend{equation}` This means for positive\\tauthe value of the auto-covariance function gives a measure for the similarity of the network statea(t)anda(t+\\tau). By comparingc(\\tau)withc(0), we may assess thememorya network has of its previous states after\\tautime steps. Note that the auto-covariance function is not normalised! Due to this, we may derive further information about the network state: Ifc(0)is small (in our case<< 1), the network activity is not rich and does not exhibit a large temporal variety across neurons. Thus the networks is unable to perform meaningful computations.\n\nAs expected, there is covariance has its maximum at a time lag of0. Examining the covariance function, we first note its values are small (<<1) implying low dimensional dynamics of the network. This fits our observation made above on the grounds of the display of the time-resolved activity.\n\nWe saw that the states of the neurons quickly converged to a globally stable fixed point. The reason for this fixed point is, that the dampening part dominates the dynamical behavior - we need to increase the weights! This we can achieve by increasing the`q_factor`q_factor.\n\nWe find that after increasing the`q_factor`q_factor, the network shows a very different behavior. The stable fixed point is gone, instead we observe chaotic network dynamics: The single neuron trajectories behave unpredictably and fluctuate widely, a small perturbation would lead to completely different state.\n\nWe moreover see that for positive time lags the auto-covariance function still is large. This means that the network has memory of its previous states: The state at a given point in time influences strongly the subsequent path of the trajectories of the neurons. Such a network can perform meaningful computations.\n\nWe now turn to a E/I networks implementing its dynamic behavior with leaky integrate-and-fire neurons. For this, we harness the concepts of Hierarchical Lava Processes and SubProcessModels. These allow us to avoid implementing everything ourselves, but rather to use already defined Processes and their ProcessModels to build more complicated programs. We here use the behavior defined for the[LIF](https://github.com/lava-nc/lava/tree/main/src/lava/proc/lif)LIFand[Dense](https://github.com/lava-nc/lava/tree/main/src/lava/proc/dense)DenseProcesses, we define the behavior of the E/I Network Process. Moreover, we would like to place the LIF E/I network in a similar dynamical regime as the rate network. This is a difficult task since the underlying single neurons dynamics are quite different. We here provide an approximate conversion function that allows for a parameter mapping and especially qualitatively retains properties of the auto-covariance function.\nWith the implementation below, we may either pass LIF specific parameters directlyoruse the same parameters needed for instantiating the rate E/I network and then convert them automatically.\n\nIn order to execute the LIF E/I network and the infrastructure to monitor the activity, we introduce a`CustomRunConfig`CustomRunConfigwhere we specify which ProcessModel we select for execution.\n\nFirst, we visually inspect to spiking activity of the neurons in the network. To this end, we display neurons on the vertical axis and mark the time step when a neuron spiked.\n\nAfter an initial synchronous burst (all neurons are simultaneously driven to the threshold by the external current), we observe an immediate decoupling of the single neuron activities due to the recurrent connectivity. Overall, we see a heterogeneous network state with asynchronous as well as synchronous spiking across neurons. This network state resembles qualitatively the fixed point observed above for the rate network. Before we turn to the study of the auto-covariance we need to address a\nsubtlety in the comparison of spiking and rate network. Comparing spike trains and rates directly is difficult due dynamics of single spiking neurons: Most of the time, a neuron does not spike! To overcome this problem and meaningfully compare quantities like the auto-covariance function, we follow the usual approach and bin the spikes. This means, we apply a sliding box-car window of a given length and count at each time step the spikes in that window to obtain an estimate of the rate.\n\nAfter having an estimate of the rate, we compare the temporally-averaged mean rate of both networks in the first state. To avoid boundary effects of the binning, we disregard time steps at the beginning and the end.\n\nBoth networks behave similarly inasmuch the rates are stationary with only very small fluctuations around the baseline in the LIF case. Next, we turn to the auto-covariance function.\n\nExamining the auto-covariance function, we first note that again the overall values are small. Moreover, we see that for non-vanishing time lags the auto-covariance function quickly decays. This means that the network has no memory of its previous states: Already after few time step we lost almost all information of the previous network state, former states leave little trace in the overall network activity. Such a network is unfit to perform meaningful computation.\n\nNext, we pass the rate network parameters for which we increased the`q_factor`q_factorto the spiking E/I network. Dynamically, this increase again should result in a fundamentally different network state.\n\nHere we see a qualitatively different network activity where the recurrent connections play a more dominant role: At seemingly random times, single neurons enter an active states of variable length. Next, we have a look at the auto-covariance function of the network, especially in direct comparison with the respective function of the rate network.\n\nWe again compare the rate of both networks in the same state.\n\nAgain, we observe a similar behavior on the rate level: In both networks the mean rate fluctuates on a longer time scale with larger values around the baseline in a similar range. Next we compare the auto-covariance functions:\n\nWe observe in the auto-covariance function of the LIF network a slowly decay, akin to the rate network. Even though both auto-covariance functions are not identical, they qualitatively match in that both networks exhibit long-lasting temporal correlations and an activity at the edge of chaos. This implies that both network are in a suitable regime for computation, e.g. in the context of reservoir computing.\n\nAfter having observed these two radically different dynamical states also in the LIF network, we next turn to the question how they come about. The difference between both version of LIF E/I networks is in the recurrently provided activations. We now examine these activations by having look at the excitatory, inhibitory as well as total activation provided to each neuron in both networks.\n\nSince the network needs some time to settle in it’s dynamical state, we discard the first200time steps.\n\nFirst, we look at the distribution of activation of a random neuron in both network states.\n\nNext, we plot the distribution of the temporal average:\n\nWe first note that the the total activation is close to zero with a slight shift to negative values, this prevents the divergence of activity. Secondly, we observe that the width of the distributions is orders of magnitude larger in the high weight case as compared to the low weight network. Finally, we look at the evolution of the mean activation over time. To this end we plot three random sample:\n\nWe see that the temporal evolution of the total activation in the low weights case is much narrower than in the high weights network. Moreover, we see that in the high weights network, the fluctuations of the activations evolve on a very long time scale as compared to the other network. This implies that a neuron can sustain it’s active, bursting state over longer periods of time leading to memory in the network as well as activity at the edge of chaos.\n\nSo far, we have used neuron models and weights that are internally represented as floating point numbers. Next, we turn to bit-accurate implementations of the LIF and Dense process where only a fixed precision for the numerical values is allowed. Here, the parameters need to be mapped to retain the dynamical behavior of the network. First, we define a method for mapping the parameters. It consists of finding an optimal scaling function that consistently maps all appearing floating-point numbers\nto fixed-point numbers.\n\nAfter having defined some primitive conversion functionality we next convert the parameters for the critical network. To constrain the values that we need to represent in the bit-accurate model, we have to find the dynamical range of the state parameters of the network, namely`u`uand`v`vof the LIF neurons.\n\nWe note that for both variables the distributions attain large (small) values with low probability. We hence will remove them in the dynamical range to increase the precision of the overall representation. We do so by choosing0.2and0.8quantiles as minimal resp. maximal values for the dynamic ranges. We finally also need to pass some information about the concrete implementation, e.g. the precision and the bit shifts performed.\n\nUsing the mapped parameters, we construct the fully-fledged parameter dictionary for the E/I network Process using the LIF SubProcessModel.\n\nComparing the spike times after the parameter conversion, we find that after the first initial time steps, the spike times start diverging, even though certain structural similarities remain. This, however, is expected: Since the systems is in a chaotic state, slight differences in the variables lead to a completely different output after some time steps. This is generally the behavior in spiking neural network.Butthe network stays in avery similar dynamical statewithsimilar\nactivity, as can be seen when examining the overall behavior of the rate as well as auto-covariance function.\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Connections](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)Connections\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\n[SubProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)SubProcessModelsor[Hierarchical Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)Hierarchical Processes\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\nfrom matplotlib import pyplot as plt\n``````\n\n``````\n[2]:\n``````\n\n``````\n# Import Process level primitives.\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n``````\n\n``````\n[3]:\n``````\n\n``````\nclass EINetwork(AbstractProcess):\n    \"\"\"Network of recurrently connected neurons.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        shape_exc = kwargs.pop(\"shape_exc\", (1,))\n        bias_exc = kwargs.pop(\"bias_exc\", 1)\n        shape_inh = kwargs.pop(\"shape_inh\", (1,))\n        bias_inh = kwargs.pop(\"bias_inh\", 1)\n        # Factor controlling strength of inhibitory synapses relative to excitatory synapses.\n        self.g_factor = kwargs.pop(\"g_factor\", 4)\n        # Factor controlling response properties of network.\n        # Larger q_factor implies longer lasting effect of provided input.\n        self.q_factor = kwargs.pop(\"q_factor\", 1)\n        weights = kwargs.pop(\"weights\")\n\n        full_shape = shape_exc + shape_inh\n\n        self.state = Var(shape=(full_shape,), init=0)\n        # Variable for possible alternative state.\n        self.state_alt = Var(shape=(full_shape,), init=0)\n        # Biases provided to neurons.\n        self.bias_exc = Var(shape=(shape_exc,), init=bias_exc)\n        self.bias_inh = Var(shape=(shape_inh,), init=bias_inh)\n        self.weights = Var(shape=(full_shape, full_shape), init=weights)\n\n        # Ports for receiving input or sending output.\n        self.inport = InPort(shape=(full_shape,))\n        self.outport = OutPort(shape=(full_shape,))\n``````\n\n``````\n[4]:\n``````\n\n``````\n# Import parent classes for ProcessModels for Hierarchical Processes.\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\nfrom lava.magma.core.model.sub.model import AbstractSubProcessModel\n\n# Import execution protocol.\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n\n# Import decorators.\nfrom lava.magma.core.decorator import implements, tag, requires\n``````\n\n``````\n[5]:\n``````\n\n``````\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.model.model import AbstractProcessModel\n\nfrom scipy.special import erf\n\n@implements(proc=EINetwork, protocol=LoihiProtocol)\n@tag('rate_neurons') # Tag allows for easy selection of ProcessModel in case multiple are defined.\n@requires(CPU)\nclass RateEINetworkModel(PyLoihiProcessModel):\n\n    outport: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)\n    inport: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float)\n    state : np.ndarray = LavaPyType(np.ndarray, float)\n    state_alt : np.ndarray = LavaPyType(np.ndarray, float)\n    bias_exc : np.ndarray = LavaPyType(np.ndarray, float)\n    bias_inh : np.ndarray = LavaPyType(np.ndarray, float)\n    weights : np.ndarray = LavaPyType(np.ndarray, float)\n\n    def __init__(self, proc_params):\n        super().__init__(proc_params=proc_params)\n\n        self.dr_exc = proc_params.get('dr_exc')\n        self.dr_inh = proc_params.get('dr_inh')\n\n        self.shape_exc = proc_params.get('shape_exc')\n        self.shape_inh = proc_params.get('shape_inh')\n\n        self.proc_params = proc_params\n\n        self.got_decay = False\n        self.got_bias = False\n        self.weights_scaled = False\n\n    def get_decay(self):\n        '''Construct decay factor.\n        '''\n        dr_full = np.array([self.dr_exc] * self.shape_exc + [self.dr_inh] * self.shape_inh)\n        self.decay = 1 - dr_full\n\n        self.got_decay= True\n\n    def get_bias(self):\n        '''Construce biases.\n        '''\n        self.bias_full = np.hstack([self.bias_exc, self.bias_inh])\n        self.got_bias = False\n\n    def scale_weights(self):\n        '''Scale the weights with integration time step.\n        '''\n\n        self.weights[:, self.shape_exc:] *= self.dr_exc\n        self.weights[:, :self.shape_exc] *= self.dr_inh\n        self.proc_params.overwrite('weights', self.weights)\n\n        self.weights_scaled = True\n\n    def state_update(self, state):\n        \"\"\"Update network state according to:\n            r[i + 1] = (1 - dr)r[i] + Wr[i]*r*dr + bias*dr\n        \"\"\"\n        state_new = self.decay * state # Decay the state.\n        state_new += self.bias_full # Add the bias.\n        state_new += self.weights @ erf(state) # Add the recurrent input.\n        return state_new\n\n    def run_spk(self):\n        \"\"\"The run function that performs the actual computation during\n        execution orchestrated by a PyLoihiProcessModel using the\n        LoihiProtocol.\n        \"\"\"\n\n        if not self.got_decay:\n            self.get_decay()\n\n        if not self.got_bias:\n            self.get_bias()\n\n        if not self.weights_scaled:\n            self.scale_weights()\n\n        a_in = self.inport.recv()\n        self.state = self.state_update(self.state) + a_in\n        self.outport.send(self.state)\n``````\n\n``````\n[6]:\n``````\n\n``````\n# Fix the randomness.\nnp.random.seed(1234)\n\n# Define dimensionality of the network.\ndim = 400\nshape = (dim,)\n\n# We represent the dimensionality by 400 neurons. As stated above 80% of the neurons will be excitatory.\nnum_neurons_exc = int(dim * 0.8)\nnum_neurons_inh = dim - num_neurons_exc\n\n# Single neuron paramters.\nparams_exc = {\n    \"shape_exc\": num_neurons_exc,\n    \"dr_exc\": 0.01,\n    \"bias_exc\": 0.1}\n\nparams_inh = {\n    \"shape_inh\": num_neurons_inh,\n    \"dr_inh\": 0.01,\n    \"bias_inh\": 0.1}\n\n# Inhibition-exciation balance for scaling inhibitory weights to maintain balance (4 times as many excitatory neurons).\ng_factor = 4.5\n\n# Factor controlling the response properties.\nq_factor = 1\n\n# Parameters Paramters for E/I network.\nnetwork_params_balanced = {}\n\nnetwork_params_balanced.update(params_exc)\nnetwork_params_balanced.update(params_inh)\nnetwork_params_balanced['g_factor'] = g_factor\nnetwork_params_balanced['q_factor'] = q_factor\n``````\n\n``````\n[7]:\n``````\n\n``````\ndef generate_gaussian_weights(dim, num_neurons_exc, q_factor, g_factor):\n    '''Generate connectivity drawn from a Gaussian distribution with mean 0\n    and std of (2 * q_factor) ** 2  / dim.\n    If a excitatory neuron has a negative weight, we set it to 0 and similarly adapt\n    positive weights for inhibitory neurons.\n    W[i, j] is connection weight from pre-synaptic neuron j to post-synaptic neuron i.\n\n    Paramerters\n    -----------\n    dim : int\n        Dimensionality of network\n    num_neurons_exc : int\n        Number of excitatory neurons\n    q_factor : float\n        Factor determining response properties of network\n    g_factor : float\n        Factor determining inhibition-excitation balance\n\n    Returns\n    -------\n    weights : np.ndarray\n        E/I weight matrix\n    '''\n    # Set scaled standard deviation of recurrent weights, J = q_factor**2 * 6 / full_shape.\n    J = (2 * q_factor)**2 / dim\n    weights = np.random.normal(0, J,\n                                   (dim, dim))\n\n    # Impose constraint that neurons can **either** be excitatory (positive weight)\n    # **or** inhibitory (negative weight).\n    exc_conns = np.full(weights.shape, True)\n    exc_conns[:, num_neurons_exc:] = False # Set entries for inhibitory neurons to False.\n    inh_conns = np.invert(exc_conns)\n\n    mask_pos_weights = (weights > 0)\n    mask_neg_weights = (weights < 0)\n\n    # Set negative weights of exciatory neurons to zero and similarly for inhibitory neurons.\n    # This induce sparsity in the connectivity.\n    weights[mask_neg_weights * exc_conns] = 0\n    weights[mask_pos_weights * inh_conns] = 0\n\n    # We finally need to increase the inhibitory weights by a factor to control balance.\n    weights[inh_conns] *= g_factor\n\n    return weights\n\n# Generate weights and store them in parameter dictionary.\nnetwork_params_balanced['weights'] = generate_gaussian_weights(dim,\n                                                               num_neurons_exc,\n                                                               network_params_balanced['q_factor'],\n                                                               network_params_balanced['g_factor'])\n``````\n\n``````\n[8]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n# Import monitoring Process.\nfrom lava.proc.monitor.process import Monitor\n\n# Configurations for execution.\nnum_steps = 1000\nrcfg = Loihi1SimCfg(select_tag='rate_neurons')\nrun_cond = RunSteps(num_steps=num_steps)\n\n# Instantiating network and IO processes.\nnetwork_balanced = EINetwork(**network_params_balanced)\nstate_monitor = Monitor()\n\nstate_monitor.probe(target=network_balanced.state,  num_steps=num_steps)\n\n# Run the network.\nnetwork_balanced.run(run_cfg=rcfg, condition=run_cond)\nstates_balanced = state_monitor.get_data()[network_balanced.name][network_balanced.state.name]\nnetwork_balanced.stop()\n``````\n\n``````\n[9]:\n``````\n\n``````\nplt.figure(figsize=(7,5))\nplt.xlabel('Time Step')\nplt.ylabel('State value')\nplt.plot(states_balanced[:, :50])\nplt.show()\n``````\n\n``````\n[10]:\n``````\n\n``````\ndef auto_cov_fct(acts, max_lag=100, offset=200):\n    \"\"\"Auto-correlation function of parallel spike trains.\n\n    Parameters\n    ----------\n\n    acts : np.ndarray shape (timesteps, num_neurons)\n        Activity of neurons, a spike is indicated by a one\n    max_lag : int\n        Maximal lag for compuation of auto-correlation function\n\n    Returns:\n\n    lags : np.ndarray\n        lags for auto-correlation function\n    auto_corr_fct : np.ndarray\n        auto-correlation function\n    \"\"\"\n    acts_local = acts.copy()[offset:-offset] # Disregard time steps at beginning and end.\n    assert max_lag < acts.shape[0], 'Maximal lag must be smaller then total number of time points'\n    num_neurons = acts_local.shape[1]\n    acts_local -= np.mean(acts_local, axis=0) # Perform temporal averaging.\n    auto_corr_fct = np.zeros(2 * max_lag + 1)\n    lags = np.linspace(-1 * max_lag, max_lag, 2 * max_lag + 1, dtype=int)\n\n    for i, lag in enumerate(lags):\n        shifted_acts_local = np.roll(acts_local, shift=lag, axis=0)\n        auto_corrs = np.zeros(acts_local.shape[0])\n        for j, act in enumerate(acts_local):\n            auto_corrs[j] = np.dot(act - np.mean(act),\n                                   shifted_acts_local[j] - np.mean(shifted_acts_local[j]))/num_neurons\n        auto_corr_fct[i] = np.mean(auto_corrs)\n\n    return lags, auto_corr_fct\n``````\n\n``````\n[11]:\n``````\n\n``````\nlags, ac_fct_balanced = auto_cov_fct(acts=states_balanced)\n\n# Plotting the auto-correlation function.\nplt.figure(figsize=(7,5))\nplt.xlabel('Lag')\nplt.ylabel('Covariance')\nplt.plot(lags, ac_fct_balanced)\nplt.show()\n``````\n\n``````\n[12]:\n``````\n\n``````\n# Defining new, larger q_factor.\nq_factor = np.sqrt(dim / 6)\n\n# Changing the strenghts of the recurrent connections.\nnetwork_params_critical = network_params_balanced.copy()\nnetwork_params_critical['q_factor'] = q_factor\nnetwork_params_critical['weights'] = generate_gaussian_weights(dim,\n                                                               num_neurons_exc,\n                                                               network_params_critical['q_factor'],\n                                                               network_params_critical['g_factor'])\n\n\n# Configurations for execution.\nnum_steps = 1000\nrcfg = Loihi1SimCfg(select_tag='rate_neurons')\nrun_cond = RunSteps(num_steps=num_steps)\n\n# Instantiating network and IO processes.\nnetwork_critical = EINetwork(**network_params_critical)\nstate_monitor = Monitor()\n\nstate_monitor.probe(target=network_critical.state,  num_steps=num_steps)\n\n# Run the network.\nnetwork_critical.run(run_cfg=rcfg, condition=run_cond)\nstates_critical = state_monitor.get_data()[network_critical.name][network_critical.state.name]\nnetwork_critical.stop()\n``````\n\n``````\n[13]:\n``````\n\n``````\nplt.figure(figsize=(7,5))\nplt.xlabel('Time Step')\nplt.ylabel('State value')\nplt.plot(states_critical[:, :50])\nplt.show()\n``````\n\n``````\n[14]:\n``````\n\n``````\nlags, ac_fct_critical = auto_cov_fct(acts=states_critical)\n\n# Plotting the auto-correlation function.\nplt.figure(figsize=(7,5))\nplt.xlabel('Lag')\nplt.ylabel('Correlation')\nplt.plot(lags, ac_fct_critical)\nplt.show()\n``````\n\n``````\n[15]:\n``````\n\n``````\nfrom lava.proc.dense.process import Dense\nfrom lava.proc.lif.process import LIF\nfrom convert_params import convert_rate_to_lif_params\n\n@implements(proc=EINetwork, protocol=LoihiProtocol)\n@tag('lif_neurons')\nclass SubEINetworkModel(AbstractSubProcessModel):\n    def __init__(self, proc):\n\n        convert = proc.proc_params.get('convert', False)\n\n        if convert:\n            proc_params = proc.proc_params._parameters\n            # Convert rate parameters to LIF parameters.\n            # The mapping is based on:\n            # A unified view on weakly correlated recurrent network, Grytskyy et al., 2013.\n            lif_params = convert_rate_to_lif_params(**proc_params)\n\n            for key, val in lif_params.items():\n                try:\n                    proc.proc_params.__setitem__(key, val)\n                except KeyError:\n                    if key == 'weights':\n                        # Weights need to be updated.\n                        proc.proc_params._parameters[key] = val\n                    else:\n                        continue\n\n        # Fetch values for excitatory neurons or set default.\n        shape_exc = proc.proc_params.get('shape_exc')\n        shape_inh = proc.proc_params.get('shape_inh')\n        du_exc = proc.proc_params.get('du_exc')\n        dv_exc = proc.proc_params.get('dv_exc')\n        vth_exc = proc.proc_params.get('vth_exc')\n        bias_mant_exc = proc.proc_params.get('bias_mant_exc')\n        bias_exp_exc = proc.proc_params.get('bias_exp_exc', 0)\n\n\n        # Fetch values for inhibitory neurons or set default.\n        du_inh = proc.proc_params.get('du_inh')\n        dv_inh = proc.proc_params.get('dv_inh')\n        vth_inh = proc.proc_params.get('vth_inh')\n        bias_mant_inh = proc.proc_params.get('bias_mant_inh')\n        bias_exp_inh = proc.proc_params.get('bias_exp_inh', 0)\n\n        # Create parameters for full network.\n        du_full = np.array([du_exc] * shape_exc\n                           + [du_inh] * shape_inh)\n        dv_full = np.array([dv_exc] * shape_exc\n                           + [dv_inh] * shape_inh)\n        vth_full = np.array([vth_exc] * shape_exc\n                            + [vth_inh] * shape_inh)\n        bias_mant_full = np.array([bias_mant_exc] * shape_exc\n                                  + [bias_mant_inh] * shape_inh)\n        bias_exp_full = np.array([bias_exp_exc] * shape_exc +\n                                 [bias_exp_inh] * shape_inh)\n        weights = proc.proc_params.get('weights')\n        weight_exp = proc.proc_params.get('weight_exp', 0)\n\n        full_shape = shape_exc + shape_inh\n\n        # Instantiate LIF and Dense Lava Processes.\n        self.lif = LIF(shape=(full_shape,),\n                       du=du_full,\n                       dv=dv_full,\n                       vth=vth_full,\n                       bias_mant=bias_mant_full,\n                       bias_exp=bias_exp_full)\n\n        self.dense = Dense(weights=weights,\n                           weight_exp=weight_exp)\n\n\n        # Recurrently connect neurons to E/I Network.\n        self.lif.s_out.connect(self.dense.s_in)\n        self.dense.a_out.connect(self.lif.a_in)\n\n        # Connect incoming activation to neurons and elicited spikes to ouport.\n        proc.inport.connect(self.lif.a_in)\n        self.lif.s_out.connect(proc.outport)\n\n        # Alias v with state and u with state_alt.\n        proc.vars.state.alias(self.lif.vars.v)\n        proc.vars.state_alt.alias(self.lif.vars.u)\n``````\n\n``````\n[16]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n# Import io processes.\nfrom lava.proc import io\n\nfrom lava.proc.dense.models import PyDenseModelFloat\nfrom lava.proc.lif.models import PyLifModelFloat\n\n\n# Configurations for execution.\nnum_steps = 1000\nrun_cond = RunSteps(num_steps=num_steps)\n\nclass CustomRunConfigFloat(Loihi1SimCfg):\n    def select(self, proc, proc_models):\n        # Customize run config to always use float model for io.sink.RingBuffer.\n        if isinstance(proc, io.sink.RingBuffer):\n            return io.sink.PyReceiveModelFloat\n        if isinstance(proc, LIF):\n            return PyLifModelFloat\n        elif isinstance(proc, Dense):\n            return PyDenseModelFloat\n        else:\n            return super().select(proc, proc_models)\n\nrcfg = CustomRunConfigFloat(select_tag='lif_neurons', select_sub_proc_model=True)\n\n# Instantiating network and IO processes.\nlif_network_balanced = EINetwork( **network_params_balanced, convert=True)\noutport_plug = io.sink.RingBuffer(shape=shape, buffer=num_steps)\n\n# Instantiate Monitors to record the voltage and the current of the LIF neurons.\nmonitor_v = Monitor()\nmonitor_u = Monitor()\n\nlif_network_balanced.outport.connect(outport_plug.a_in)\nmonitor_v.probe(target=lif_network_balanced.state,  num_steps=num_steps)\nmonitor_u.probe(target=lif_network_balanced.state_alt,  num_steps=num_steps)\n\nlif_network_balanced.run(condition=run_cond, run_cfg=rcfg)\n\n# Fetching spiking activity.\nspks_balanced = outport_plug.data.get()\ndata_v_balanced = monitor_v.get_data()[lif_network_balanced.name][lif_network_balanced.state.name]\ndata_u_balanced = monitor_u.get_data()[lif_network_balanced.name][lif_network_balanced.state_alt.name]\n\nlif_network_balanced.stop()\n``````\n\n``````\n[17]:\n``````\n\n``````\nfrom lava.utils.plots import raster_plot\nfig = raster_plot(spikes=spks_balanced)\n``````\n\n``````\n[18]:\n``````\n\n``````\nwindow_size = 25\nwindow = np.ones(window_size) # Window size of 25 time steps for binning.\nbinned_sps_balanced = np.asarray([np.convolve(spks_balanced[i], window) for i in range(dim)])[:, :-window_size + 1]\n``````\n\n``````\n[19]:\n``````\n\n``````\noffset = 50\ntimesteps = np.arange(0,1000, 1)[offset: -offset]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nax1.set_title('Mean rate of Rate network')\nax1.plot(timesteps,\n         (states_balanced - states_balanced.mean(axis=0)[np.newaxis,:]).mean(axis=1)[offset: -offset])\nax1.set_ylabel('Mean rate')\nax1.set_xlabel('Time Step')\nax2.set_title('Mean rate of LIF network')\nax2.plot(timesteps,\n         (binned_sps_balanced - np.mean(binned_sps_balanced, axis=1)[:, np.newaxis]).T.mean(axis=1)[offset: -offset])\nax2.set_xlabel('Time Step')\nplt.show()\n``````\n\n``````\n[20]:\n``````\n\n``````\nlags, ac_fct = auto_cov_fct(acts=binned_sps_balanced.T)\n\n# Plotting the auto-covariance function.\nplt.figure(figsize=(7,5))\nplt.xlabel('Lag')\nplt.ylabel('Covariance')\nplt.plot(lags, ac_fct)\n``````\n\n``````\n[20]:\n``````\n\n``````\n[<matplotlib.lines.Line2D at 0x7feff717d4b0>]\n``````\n\n``````\n[21]:\n``````\n\n``````\nnum_steps = 1000\nrcfg = CustomRunConfigFloat(select_tag='lif_neurons', select_sub_proc_model=True)\nrun_cond = RunSteps(num_steps=num_steps)\n\n# Creating new new network with changed weights.\nlif_network_critical = EINetwork(**network_params_critical, convert=True)\noutport_plug = io.sink.RingBuffer(shape=shape, buffer=num_steps)\n\n# Instantiate Monitors to record the voltage and the current of the LIF neurons.\nmonitor_v = Monitor()\nmonitor_u = Monitor()\n\nlif_network_critical.outport.connect(outport_plug.a_in)\nmonitor_v.probe(target=lif_network_critical.state,  num_steps=num_steps)\nmonitor_u.probe(target=lif_network_critical.state_alt,  num_steps=num_steps)\n\nlif_network_critical.run(condition=run_cond, run_cfg=rcfg)\n\n# Fetching spiking activity.\nspks_critical = outport_plug.data.get()\ndata_v_critical = monitor_v.get_data()[lif_network_critical.name][lif_network_critical.state.name]\ndata_u_critical = monitor_u.get_data()[lif_network_critical.name][lif_network_critical.state_alt.name]\n\nlif_network_critical.stop()\n``````\n\n``````\n[22]:\n``````\n\n``````\nfig = raster_plot(spikes=spks_critical)\n``````\n\n``````\n[23]:\n``````\n\n``````\nwindow = np.ones(window_size)\nbinned_sps_critical = np.asarray([np.convolve(spks_critical[i], window) for i in range(dim)])[:, :-window_size + 1]\nlags, ac_fct_lif_critical = auto_cov_fct(acts=binned_sps_critical.T)\n``````\n\n``````\n[24]:\n``````\n\n``````\noffset = 50\ntimesteps = np.arange(0,1000, 1)[offset: -offset]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nax1.set_title('Mean rate of Rate network')\nax1.plot(timesteps,\n         (states_critical - states_critical.mean(axis=0)[np.newaxis,:]).mean(axis=1)[offset: -offset])\nax1.set_ylabel('Mean rate')\nax1.set_xlabel('Time Step')\nax2.set_title('Mean rate of LIF network')\nax2.plot(timesteps,\n         (binned_sps_critical - np.mean(binned_sps_critical, axis=1)[:, np.newaxis]).T.mean(axis=1)[offset: -offset])\nax2.set_xlabel('Time Step')\nplt.show()\n``````\n\n``````\n[25]:\n``````\n\n``````\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nax1.plot(lags, ac_fct_lif_critical)\nax1.set_title('Auto-Cov function: LIF network')\nax1.set_xlabel('Lag')\nax1.set_ylabel('Covariance')\nax2.plot(lags, ac_fct_critical)\nax2.set_title('Auto-Cov function: Rate network')\nax2.set_xlabel('Lag')\nax2.set_ylabel('Covariance')\nplt.tight_layout()\n``````\n\n``````\n[26]:\n``````\n\n``````\ndef calculate_activation(weights, spks, num_exc_neurons):\n    \"\"\"Calculate excitatory, inhibitory and total activation to neurons.\n\n    Parameters\n    ----------\n\n    weights : np.ndarray (num_neurons, num_neurons)\n        Weights of recurrent connections\n    spks : np.ndarray (num_neurons, num_time_steps)\n        Spike times of neurons, 0 if neuron did not spike, 1 otherwise\n    num_exc_neurons : int\n        Number of excitatory neurons\n\n    Returns\n    -------\n\n    activation_exc : np.ndarray (num_neurons, num_time_steps)\n        Excitatory activation provided to neurons\n    activation_inh : np.ndarray (num_neurons, num_time_steps)\n        Inhibitory activation provided to neurons\n    activations_total : np.ndarray (num_neurons, num_time_steps)\n        Total activation provided to neurons\n    \"\"\"\n\n    weights_exc = weights[:, :num_exc_neurons]\n    weights_inh = weights[:, num_exc_neurons:]\n\n    spks_exc = spks[:num_exc_neurons]\n    spks_inh = spks[num_exc_neurons:]\n\n    activation_exc = np.matmul(weights_exc, spks_exc)\n    activation_inh = np.matmul(weights_inh, spks_inh)\n\n    activation_total = activation_exc + activation_inh\n\n    return activation_exc, activation_inh, activation_total\n``````\n\n``````\n[27]:\n``````\n\n``````\noffset = 200\n\nact_exc_balanced, act_inh_balanced, act_tot_balanced \\\n    = calculate_activation(lif_network_balanced.proc_params.get('weights'),\n                          spks_balanced[:,offset:],\n                          network_params_balanced['shape_exc'])\n\nact_exc_critical, act_inh_critical, act_tot_critical \\\n    = calculate_activation(lif_network_critical.proc_params.get('weights'),\n                          spks_critical[:,offset:],\n                          network_params_balanced['shape_exc'])\n``````\n\n``````\n[28]:\n``````\n\n``````\nrnd_neuron = 4\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nax1.set_title('Low weights')\nax1.set_xlabel('Activation')\nax1.set_ylabel('Density')\nax1.hist(act_exc_balanced[rnd_neuron], bins=10, alpha=0.5, density=True, label='E')\nax1.hist(act_inh_balanced[rnd_neuron], bins=10, alpha=0.5, density=True, label='I'),\nax1.hist(act_tot_balanced[rnd_neuron], bins=10, alpha=0.5, density=True, label='Total')\nax1.legend()\n\nax2.set_title('High weights')\nax2.set_xlabel('Activation')\nax2.set_ylabel('Density')\nax2.hist(act_exc_critical[rnd_neuron], bins=10, alpha=0.5, density=True, label='E')\nax2.hist(act_inh_critical[rnd_neuron], bins=10, alpha=0.5, density=True, label='I')\nax2.hist(act_tot_critical[rnd_neuron], bins=10, alpha=0.5, density=True, label='Total')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n``````\n\n``````\n[29]:\n``````\n\n``````\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nax1.set_title('Low weights')\nax1.set_xlabel('Activation')\nax1.set_ylabel('Density')\nax1.hist(act_exc_balanced.mean(axis=0), bins=10, alpha=0.5, density=True, label='E')\nax1.hist(act_inh_balanced.mean(axis=0), bins=10, alpha=0.5, density=True, label='I'),\nax1.hist(act_tot_balanced.mean(axis=0), bins=10, alpha=0.5, density=True, label='Total')\nax1.legend()\n\nax2.set_title('High weights')\nax2.set_xlabel('Activation')\nax2.set_ylabel('Density')\nax2.hist(act_exc_critical.mean(axis=0), bins=10, alpha=0.5, density=True, label='E')\nax2.hist(act_inh_critical.mean(axis=0), bins=10, alpha=0.5, density=True, label='I')\nax2.hist(act_tot_critical.mean(axis=0), bins=10, alpha=0.5, density=True, label='Total')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n``````\n\n``````\n[30]:\n``````\n\n``````\ntime_steps = np.arange(offset, num_steps, 1)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\nax1.set_title('Low weights')\nax1.set_xlabel('Time step')\nax1.set_ylabel('Activation')\nfor i in range(3):\n    ax1.plot(time_steps, act_tot_balanced[i], alpha=0.7)\n\n\nax2.set_title('High weights')\nax2.set_xlabel('Time step')\nax2.set_ylabel('Activation')\nfor i in range(3):\n    ax2.plot(time_steps, act_tot_critical[i], alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n``````\n\n``````\n[31]:\n``````\n\n``````\ndef _scaling_funct(params):\n    '''Find optimal scaling function for float- to fixed-point mapping.\n\n    Parameter\n    ---------\n    params : dict\n        Dictionary containing information required for float- to fixed-point mapping\n\n    Returns\n    ------\n    scaling_funct : callable\n        Optimal scaling function for float- to fixed-point conversion\n    '''\n    sorted_params = dict(sorted(params.items(), key=lambda x: np.max(np.abs(x[1]['val'])), reverse=True))\n\n    # Initialize scaling function.\n    scaling_funct = None\n\n    for key, val in sorted_params.items():\n        if val['signed'] == 's':\n            signed_shift = 1\n        else:\n            signed_shift = 0\n\n        if np.max(val['val']) == np.max(np.abs(val['val'])):\n            max_abs = True\n            max_abs_val = np.max(val['val'])\n        else:\n            max_abs = False\n            max_abs_val = np.max(np.abs(val['val']))\n\n        if max_abs:\n            rep_val = 2**(val['bits'] - signed_shift) - 1\n        else:\n            rep_val = 2**(val['bits'] - signed_shift)\n\n        max_shift = np.max(val['shift'])\n\n        max_rep_val = rep_val * 2**max_shift\n\n        if scaling_funct:\n            scaled_vals = scaling_funct(val['val'])\n\n            max_abs_scaled_vals = np.max(np.abs(scaled_vals))\n            if max_abs_scaled_vals <= max_rep_val:\n                continue\n            else:\n                p1 = max_rep_val\n                p2 = max_abs_val\n\n        else:\n            p1 = max_rep_val\n            p2 = max_abs_val\n\n        scaling_funct = lambda x: p1 / p2 * x\n\n    return scaling_funct\n\ndef float2fixed_lif_parameter(lif_params):\n    '''Float- to fixed-point mapping for LIF parameters.\n\n    Parameters\n    ---------\n    lif_params : dict\n        Dictionary with parameters for LIF network with floating-point ProcModel\n\n    Returns\n    ------\n    lif_params_fixed : dict\n        Dictionary with parameters for LIF network with fixed-point ProcModel\n    '''\n\n    scaling_funct = _scaling_funct(params)\n\n    bias_mant_bits = params['bias']['bits']\n    scaled_bias = scaling_funct(params['bias']['val'])[0]\n    bias_exp = int(np.ceil(np.log2(scaled_bias) - bias_mant_bits + 1))\n    if bias_exp <=0:\n        bias_exp = 0\n\n\n    weight_mant_bits = params['weights']['bits']\n    scaled_weights = np.round(scaling_funct(params['weights']['val']))\n    weight_exp = int(np.ceil(np.log2(scaled_bias) - weight_mant_bits + 1))\n    weight_exp = np.max(weight_exp) - 6\n    if weight_exp <=0:\n        diff = weight_exp\n        weight_exp = 0\n\n\n    bias_mant = int(scaled_bias // 2**bias_exp)\n    weights = scaled_weights.astype(np.int32)\n\n    lif_params_fixed = {'vth' : int(scaling_funct(params['vth']['val']) // 2**params['vth']['shift'][0]),\n                        'bias_mant': bias_mant,\n                        'bias_exp': bias_exp,\n                        'weights': np.round(scaled_weights / (2 ** params['weights']['shift'][0])).astype(np.int32),\n                        'weight_exp': weight_exp}\n\n    return lif_params_fixed\n\ndef scaling_funct_dudv(val):\n    '''Scaling function for du, dv in LIF\n    '''\n    assert val < 1, 'Passed value must be smaller than 1'\n\n    return np.round(val * 2 ** 12).astype(np.int32)\n``````\n\n``````\n[32]:\n``````\n\n``````\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\nax1.set_title('u')\nax1.set_xlabel('Current')\nax1.set_ylabel('Density')\nax1.hist(data_u_critical.flatten(), bins='auto', density=True)\nax1.legend()\n\nax2.set_title('v')\nax2.set_xlabel('Voltage')\nax2.set_ylabel('Density')\nax2.hist(data_v_critical.flatten(), bins='auto', density=True)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n``````\n\n``````\n[33]:\n``````\n\n``````\nu_low = np.quantile(data_u_critical.flatten(), 0.2)\nu_high = np.quantile(data_u_critical.flatten(), 0.8)\nv_low = np.quantile(data_v_critical.flatten(), 0.2)\nv_high = np.quantile(data_v_critical.flatten(), 0.8)\n\nlif_params_critical = convert_rate_to_lif_params(**network_params_critical)\nweights = lif_params_critical['weights']\nbias = lif_params_critical['bias_mant_exc']\n\nparams = {'vth': {'bits': 17, 'signed': 'u', 'shift': np.array([6]), 'val': np.array([1])},\n          'u': {'bits': 24, 'signed': 's', 'shift': np.array([0]), 'val': np.array([u_low, u_high])},\n          'v': {'bits': 24, 'signed': 's', 'shift': np.array([0]), 'val': np.array([v_low, v_high])},\n          'bias': {'bits': 13, 'signed': 's', 'shift': np.arange(0, 3, 1), 'val': np.array([bias])},\n          'weights' : {'bits': 8, 'signed': 's', 'shift': np.arange(6,22,1), 'val': weights}}\n\nmapped_params = float2fixed_lif_parameter(params)\n``````\n\n``````\n[34]:\n``````\n\n``````\n# Set up parameters for bit accurate model\nlif_params_critical_fixed = {'shape_exc': lif_params_critical['shape_exc'],\n                             'shape_inh': lif_params_critical['shape_inh'],\n                             'g_factor': lif_params_critical['g_factor'],\n                             'q_factor': lif_params_critical['q_factor'],\n                             'vth_exc': mapped_params['vth'],\n                             'vth_inh': mapped_params['vth'],\n                             'bias_mant_exc': mapped_params['bias_mant'],\n                             'bias_exp_exc': mapped_params['bias_exp'],\n                             'bias_mant_inh': mapped_params['bias_mant'],\n                             'bias_exp_inh': mapped_params['bias_exp'],\n                             'weights': mapped_params['weights'],\n                             'weight_exp': mapped_params['weight_exp'],\n                             'du_exc': scaling_funct_dudv(lif_params_critical['du_exc']),\n                             'dv_exc': scaling_funct_dudv(lif_params_critical['dv_exc']),\n                             'du_inh': scaling_funct_dudv(lif_params_critical['du_inh']),\n                             'dv_inh': scaling_funct_dudv(lif_params_critical['dv_inh'])}\n``````\n\n``````\n[35]:\n``````\n\n``````\n# Import bit accurate ProcessModels.\nfrom lava.proc.dense.models import PyDenseModelBitAcc\nfrom lava.proc.lif.models import PyLifModelBitAcc\n\n# Configurations for execution.\nnum_steps = 1000\nrun_cond = RunSteps(num_steps=num_steps)\n\n# Define custom Run Config for execution of bit accurate models.\nclass CustomRunConfigFixed(Loihi1SimCfg):\n    def select(self, proc, proc_models):\n        # Customize run config to always use float model for io.sink.RingBuffer.\n        if isinstance(proc, io.sink.RingBuffer):\n            return io.sink.PyReceiveModelFloat\n        if isinstance(proc, LIF):\n            return PyLifModelBitAcc\n        elif isinstance(proc, Dense):\n            return PyDenseModelBitAcc\n        else:\n            return super().select(proc, proc_models)\n\nrcfg = CustomRunConfigFixed(select_tag='lif_neurons', select_sub_proc_model=True)\n\nlif_network_critical_fixed = EINetwork(**lif_params_critical_fixed)\noutport_plug = io.sink.RingBuffer(shape=shape, buffer=num_steps)\n\nlif_network_critical_fixed.outport.connect(outport_plug.a_in)\n\nlif_network_critical_fixed.run(condition=run_cond, run_cfg=rcfg)\n\n# Fetching spiking activity.\nspks_critical_fixed = outport_plug.data.get()\n\nlif_network_critical_fixed.stop()\n``````\n\n``````\n[36]:\n``````\n\n``````\nfig = raster_plot(spikes=spks_critical, color='orange', alpha=0.3)\nraster_plot(spikes=spks_critical_fixed, fig=fig, alpha=0.3, color='b')\nplt.show()\n``````\n\n``````\n[37]:\n``````\n\n``````\nwindow = np.ones(window_size)\nbinned_sps_critical_fixed = np.asarray([\n    np.convolve(spks_critical_fixed[i], window) for i in range(dim)])[:,:-window_size +1]\nlags, ac_fct_lif_critical_fixed = auto_cov_fct(acts=binned_sps_critical_fixed.T)\n``````\n\n``````\n[38]:\n``````\n\n``````\noffset = 50\ntimesteps = np.arange(0,1000, 1)[offset: -offset]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nax1.set_title('Mean rate of floating-point LIF network')\nax1.plot(timesteps,\n         (binned_sps_critical - np.mean(binned_sps_critical, axis=1)[:, np.newaxis]).T.mean(axis=1)[offset: -offset])\nax1.set_ylabel('Mean rate')\nax1.set_xlabel('Time Step')\nax2.set_title('Mean rate of fixed-point LIF network')\nax2.plot(timesteps,\n         (binned_sps_critical_fixed\n          - np.mean(binned_sps_critical_fixed, axis=1)[:, np.newaxis]).T.mean(axis=1)[offset: -offset])\nax2.set_xlabel('Time Step')\nplt.show()\n``````\n\n``````\n[39]:\n``````\n\n``````\n# Plotting the auto-correlation function.\nplt.figure(figsize=(7,5))\nplt.xlabel('Lag')\nplt.ylabel('Covariance')\nplt.plot(lags, ac_fct_lif_critical_fixed, label='Bit accurate model')\nplt.plot(lags, ac_fct_lif_critical, label='Floating point model')\nplt.legend()\nplt.show()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/three_factor_learning/tutorial01_Reward_Modulated_STDP.html",
    "title": "Three Factor Learning with Lava — Lava  documentation",
    "content": "Copyright (C) 2022 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nMotivation: This tutorial demonstrates how Lava users can define and implement three factor learning rules using a software model of Loihi’s learning engine.\n\nhave the[Lava framework installed](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Lava framework installed\n\nare familiar with[Process interfaces in Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Process interfaces in Lava\n\nare familiar with[ProcessModel implementations in Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_process_models.ipynb)ProcessModel implementations in Lava\n\nare familiar with how to[connect Lava Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)connect Lava Processes\n\nare familiar with how to[implement a custom learning rule](https://lava-nc.org/lava/notebooks/in_depth/tutorial08_stdp.html)implement a custom learning rule\n\nThis tutorial demonstrates how the Lava Learning API can be used for simulation of three-factor learning rules. We first instantiate a high-level interface for the reward-modulated spike-timing dependent plasticity (R-STDP) rule defined in the Lava Process Library. We then execute a simulation of a spiking neural network in which localized, graded reward factors are mapped to specific plastic synapses, demonstrating Lava support for this new Loihi 2 learning capability. Finally, we demonstrate\nhow users can define their own custom, three-factor learning rule interfaces and implement custom post-synaptic trace dynamics in simulation, as supported on Loihi 2 through microcoded post-synaptic neuron instructions. With these capabilities, Lava now provides support for many of the latest neuro-inspired learning algorithms under study!\n\nThe Lava learning API can be used to represent a wide range of local learning rules that obey Loihi’s sum-of-product learning rule form. Users can define define custom, reusable learning rule interfaces atop the full Lava learning engine equation parser. Here, we import a reusable interface for reward-modulated spike-timing dependent plasticity (R-STDP) that is now part of the standard Lava Process Library.\n\nReward-modulated STDP is a three-factor learning rule that can explain how behaviourly relevant adaptive changes in a complex network of spiking neurons could be achieved in a self-organizing manner through local synaptic plasticity. A third, reward signal modulates the outcome of the pairwise two-factor learning rule STDP. The implementation of the R-STDP described below is adapted from[Neuromodulated Spike-Timing-Dependent\nPlasticity](https://www.frontiersin.org/articles/10.3389/fncir.2015.00085/full)Neuromodulated Spike-Timing-Dependent\nPlasticity.\n\nSynaptic weights,W, are modulated as a function of the eligibility traceEand reward termR:\n\n\\dot{W} = R \\cdot E\n\nThe synaptic eligibility trace stores a temporary memory of the STDP outcome that persists through the epoch when a delayed reward signal is received. Defining the learning window of a traditional Hebbian STDP asSTDP(pre, post), where “pre” represents the pre-synaptic activity of a synapse and “post” represents the state of the post-synaptic neuron, the synaptic eligibility trace dynamics have the form:\n\n\\dot{E} = - \\frac{E}{\\tau_e} + STDP(pre, post)\n\nNOTE: Learning parameters are adapted from online implemention of[Spike-Timing Dependent Plasticity (STDP)](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity)Spike-Timing Dependent Plasticity (STDP)and can vary based on implementation.\n\nWe can now define a simple spiking network in which connections are modified according to our R-STDP learning rule. The core of the learning network is comprised of one pre-synaptic leaky-integrate-and-fire (LIF) neuron and two post-synaptic LIF neurons, all of which are driven by binary spiking inputs. The pre- and post-synaptic neurons are connected via a LearningDense Process, which learns weights according to the R-STDP learning rule. The binary spiking inputs drive the pre- and post-\npopulations to affect the dynamics of the R-STDP eligibility trace.\n\nThe network’s post-synaptic population also receives localized, graded reward spikes. The post-synaptic neurons transform the graded reward spikes to localized reward traces, which act as targeted modulators to specific plastic synapses within the LearningDense process. This tutorials now includes a RSTDPLIF Process, used for post-synaptic neurons that can each compute up to two custom reward traces or “third factor” modulators.\n\nThe diagram below explains the structure of the spiking network used to demonstrate the learnable weights modified with the R-STDP learning rule.\n\nThe simple spiking network shown above translates to a connected Lava process architecture as shown below.\n\nNOTE : Though the RSTDPLIF Process can currently only be executed on CPU backend, it is modeled from Loihi 2’s ability to compute custom post-traces in microcoded neuron instructions. Lava support for on-chip execution will be available soon!\n\nThe spiking neural network is simulated for 200 time steps using the Loihi2 simulation configuration.\n\nThis plot shows the spike times at which the pre-synaptic neuron and the post-synaptic neurons ‘A’ and ‘B’ fired across time-steps. The co-relation between the spike timing of the pre-synaptic neuron and the post-synaptic neurons are used to do learning updates throughout the rest of the tutorial.\n\nThe first plot shows the spike times at which the pre-synaptic neuron fired. The pre-traces are updated based on the pre-synaptic spike train shown in the first plot. At the instant of a pre-synaptic spike, the pre-trace value is incremented by the ‘pre_trace_kernel_magnitude’ described in the learning rule. Subsequently, the trace value is decayed by a factor of ‘pre_trace_decay_tau’, with respect to the trace value at that instant, until the event of the next pre-synaptic spike. For example,\nthe first pre-synaptic spike happens at time step 15 as shown in the first plot by the first red dash. Therefore, the pre-trace value is increamented by a value of 16 (‘pre_trace_kernel_magnitude’), at time step 15, after which the decaying of the trace ensues until the next pre-synaptic spike at time step 46. Similar update dynamics are used to update the post-traces also.\n\nThe tag dynamics replicate the STDP learning rule with an additional decay behaviour to represent an eligibility trace. The update to the tag trace is based on the co-relation between pre and post-synaptic spikes. Consider the trace dynamics for ‘Neuron A’, the post-synaptic neuron A, fires at time-step 6, which is indicated by the first green dash in the ‘spike arrival’ plot. In Loihi 2, the update of the traces are done after both the pre-synaptic and post-synaptic neurons have fired. At the\nadvent of pre-synaptic spike at time step 15, the tag trace of the post-synaptic neuron A which fired before the pre-synaptic neuron, is decremented by the dot product of the pre-trace value and the post-trace value at time step 15. This behaviour can be seen in the plot shown above. The evolution of the tag dynamics follows the same principle to do both depression and potentiation with respect to the co-relation between pre and post-spikes.\n\nIn Loihi 2, individual post-synaptic neurons can be microcoded with reward traces that can be driven heterogeneously by synaptic inputs that differ in both magnitude and time. The reward trace plot shows the difference in third-factor trace received by the two-post synaptic neurons. The learnable synaptic weight is updated at every ‘t_epoch’ based on the instantanious value of the eligibility trace and the reward trace. In an ‘R-STDP’ rule, the synaptic weight is updated only when a\nreward/third-factor value is non-zero. The weight dynamics of ‘Neuron A’ can be seen only being modified during the interval from time step 25 to time step 50, after which the weight trace value remains constant. This behaviour explains the functionality of the Reward-Modulated Spike-Timing Dependent Plasticity rule we defined in this tutorial.\n\nThe RewardModulatedSTDP Class maps R-STDP parameters to terms understood by Loihi’s sum-of-product form learning engine. The eligibility traceEcan be stored in the tag variabletof the Loihi learning engine. Tag dynamicsdtevolve according to\n\ndt = STDP(pre, post) - t \\cdot \\tau_{tag} \\cdot u_0\n\nwhere\\tau_{tag}is the time constant of the tag evolution. As described in the[STDP tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial08_stdp.html)STDP tutorial, pairwise STDP dynamicsSTDP(pre, post)can be re-defined using Loihi learning engine variables as\n\ndt = ( A_{+} \\cdot x_0 \\cdot y_1 + A_{-} \\cdot y_0 \\cdot x_1 ) - t \\cdot \\tau_{tag} \\cdot u_0\n\nwherex_1andy_1are the pre and postsynaptic spikes,x_0andy_0describe a rule execution dependence on the timing of the pre and postsynaptic spikes, respectively, and the spike timing behavior is scaled by the constantsA_{+} < 0andA_{-} > 0.\n\nWeight evolution during learningdwis a function of the tag dynamicstand the reward signal, stored in Loihi’s post-synaptic trace variabley_2:\n\ndw = u_0 \\cdot t \\cdot y_2\n\nu_0is the Loihi learning engine’s way of defining that the weight update occurs at every learning epoch.\n\nThe class structure of the RewardModulatedSTDP which converts the user readable variables to Loihi specific parameters and variables is described below.\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\n[Connecting Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)Connecting Processes\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.proc.learning_rules.r_stdp_learning_rule import RewardModulatedSTDP\n\nR_STDP = RewardModulatedSTDP(learning_rate=1,\n                             A_plus=2,\n                             A_minus=-2,\n                             pre_trace_decay_tau=10,\n                             post_trace_decay_tau=10,\n                             pre_trace_kernel_magnitude=16,\n                             post_trace_kernel_magnitude=16,\n                             eligibility_trace_decay_tau=0.5,\n                             t_epoch=1\n                             )\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\n\n# Set this tag to \"fixed_pt\" or \"floating_pt\" to choose the corresponding models.\nSELECT_TAG = \"fixed_pt\"\n\n# LIF parameters\nif SELECT_TAG == \"fixed_pt\":\n    du = 4095\n    dv = 4095\nelif SELECT_TAG == \"floating_pt\":\n    du = 1\n    dv = 1\nvth = 240\n``````\n\n``````\n[3]:\n``````\n\n``````\n# Pre-synaptic neuron parameters\nnum_neurons_pre = 1\nshape_lif_pre = (num_neurons_pre, )\nshape_conn_pre = (num_neurons_pre, num_neurons_pre)\n\n# SpikeIn -> pre-synaptic LIF connection weight\nwgt_inp_pre = np.eye(num_neurons_pre) * 250\n\n\n# Post-synaptic neuron parameters\nnum_neurons_post = 2\nshape_lif_post = (num_neurons_post, )\nshape_conn_post = (num_neurons_post, num_neurons_pre)\n\n# SpikeIn -> post-synaptic LIF connection weight\nwgt_inp_post = np.eye(num_neurons_post) * 250\n\n\n# Third-factor input weights\n# Graded SpikeIn -> post-synaptic LIF connection weight\nwgt_inp_reward = np.eye(num_neurons_post) * 100\n\n\n# pre-LIF -> post-LIF connection initial weight (learning-enabled)\n# Plastic synapse\nwgt_plast_conn = np.full(shape_conn_post, 50)\n``````\n\n``````\n[4]:\n``````\n\n``````\nfrom utils import generate_post_spikes\n``````\n\n``````\n[5]:\n``````\n\n``````\n# Number of simulation time steps\nnum_steps = 200\ntime = list(range(1, num_steps + 1))\n\n# Spike times\nspike_prob = [0.03, 0.09]\n\n# Create random spike rasters\nnp.random.seed(156)\nspike_raster_pre = np.zeros((num_neurons_pre, num_steps))\nnp.place(spike_raster_pre, np.random.rand(num_neurons_pre, num_steps) < spike_prob[0], 1)\n\n# Create post spikes raster\nspike_raster_post = generate_post_spikes(spike_raster_pre, num_steps, spike_prob)\n\n# Create graded reward input spikes\ngraded_reward_spikes = np.zeros((num_neurons_post, num_steps))\nfor index in range(num_steps):\n    if index in range(50, 70):\n        graded_reward_spikes[0][index] = 6\n    if index in range(150, 170):\n        graded_reward_spikes[1][index] = 8\n``````\n\n``````\n[6]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.io.source import RingBuffer as SpikeIn\nfrom lava.proc.dense.process import LearningDense, Dense\nfrom utils import RSTDPLIF, RSTDPLIFModelFloat, RSTDPLIFBitAcc\n``````\n\n``````\n[7]:\n``````\n\n``````\n# Create input devices\npattern_pre = SpikeIn(data=spike_raster_pre.astype(int))\npattern_post = SpikeIn(data=spike_raster_post.astype(int))\n\n# Create graded reward input device\nreward_pattern_post = SpikeIn(data=graded_reward_spikes.astype(float))\n\n# Create input connectivity\nconn_inp_pre = Dense(weights=wgt_inp_pre)\nconn_inp_post = Dense(weights=wgt_inp_post)\nconn_inp_reward = Dense(weights=wgt_inp_reward, num_message_bits=5)\n\n# Create pre-synaptic neurons\nlif_pre = LIF(u=0,\n              v=0,\n              du=du,\n              dv=du,\n              bias_mant=0,\n              bias_exp=0,\n              vth=vth,\n              shape=shape_lif_pre,\n              name='lif_pre')\n\n# Create plastic connection\nplast_conn = LearningDense(weights=wgt_plast_conn,\n                           learning_rule=R_STDP,\n                           name='plastic_dense')\n\n# Create post-synaptic neuron\nlif_post = RSTDPLIF(u=0,\n                    v=0,\n                    du=du,\n                    dv=du,\n                    bias_mant=0,\n                    bias_exp=0,\n                    vth=vth,\n                    shape=shape_lif_post,\n                    name='lif_post',\n                    learning_rule=R_STDP)\n``````\n\n``````\n[8]:\n``````\n\n``````\n# Connect network\npattern_pre.s_out.connect(conn_inp_pre.s_in)\nconn_inp_pre.a_out.connect(lif_pre.a_in)\n\npattern_post.s_out.connect(conn_inp_post.s_in)\nconn_inp_post.a_out.connect(lif_post.a_in)\n\n# Reward ports\nreward_pattern_post.s_out.connect(conn_inp_reward.s_in)\nconn_inp_reward.a_out.connect(lif_post.a_third_factor_in)\n\nlif_pre.s_out.connect(plast_conn.s_in)\nplast_conn.a_out.connect(lif_post.a_in)\n\n# Connect reward trace callback\n# Sending the post synaptic spikes\nlif_post.s_out_bap.connect(plast_conn.s_in_bap)\n\nlif_post.s_out_y1.connect(plast_conn.s_in_y1)\n\n# Sending graded reward spikes\nlif_post.s_out_y2.connect(plast_conn.s_in_y2)\n``````\n\n``````\n[9]:\n``````\n\n``````\nfrom lava.proc.monitor.process import Monitor\n\n# Create monitors\nmon_pre_trace = Monitor()\nmon_post_trace = Monitor()\nmon_reward_trace = Monitor()\nmon_pre_spikes = Monitor()\nmon_post_spikes = Monitor()\nmon_weight = Monitor()\nmon_tag = Monitor()\nmon_s_in_y2 = Monitor()\nmon_y2 = Monitor()\n\n# Connect monitors\nmon_pre_trace.probe(plast_conn.x1, num_steps)\nmon_post_trace.probe(plast_conn.y1, num_steps)\nmon_reward_trace.probe(lif_post.s_out_y2, num_steps)\nmon_pre_spikes.probe(lif_pre.s_out, num_steps)\nmon_post_spikes.probe(lif_post.s_out, num_steps)\nmon_weight.probe(plast_conn.weights, num_steps)\nmon_tag.probe(plast_conn.tag_1, num_steps)\n``````\n\n``````\n[10]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi2SimCfg\n``````\n\n``````\n[11]:\n``````\n\n``````\n# Running\npattern_pre.run(condition=RunSteps(num_steps=num_steps), run_cfg=Loihi2SimCfg(select_tag=SELECT_TAG))\n``````\n\n``````\n[12]:\n``````\n\n``````\n# Get data from monitors\npre_trace = mon_pre_trace.get_data()['plastic_dense']['x1']\npost_trace = mon_post_trace.get_data()['plastic_dense']['y1']\nreward_trace = mon_reward_trace.get_data()['lif_post']['s_out_y2']\npre_spikes = mon_pre_spikes.get_data()['lif_pre']['s_out']\npost_spikes = mon_post_spikes.get_data()['lif_post']['s_out']\nweights_neuron_A = mon_weight.get_data()['plastic_dense']['weights'][:, 0, 0]\nweights_neuron_B = mon_weight.get_data()['plastic_dense']['weights'][:, 1, 0]\ntag_neuron_A = mon_tag.get_data()['plastic_dense']['tag_1'][:, 0, 0]\ntag_neuron_B = mon_tag.get_data()['plastic_dense']['tag_1'][:, 1, 0]\n``````\n\n``````\n[13]:\n``````\n\n``````\n# Stopping\npattern_pre.stop()\n``````\n\n``````\n[14]:\n``````\n\n``````\nfrom utils import plot_spikes, plot_time_series, plot_time_series_subplots, plot_spikes_time_series\n``````\n\n``````\n[15]:\n``````\n\n``````\n# Plot spikes\nplot_spikes(spikes=[np.where(post_spikes[:, 1])[0], np.where(post_spikes[:, 0])[0], np.where(pre_spikes[:, 0])[0]],\n            figsize=(14, 4),\n            legend=['Post-synaptic Neuron B', 'Post-synaptic Neuron A', 'Pre-synaptic'],\n            colors=['#ff9912',      '#458b00',  '#f14a16'],\n            title='Spike arrival',\n            num_steps=num_steps\n            )\n``````\n\n``````\n[16]:\n``````\n\n``````\nplot_spikes_time_series(time=time,\n                        time_series=pre_trace,\n                        spikes=[np.where(pre_spikes[:, 0])[0]],\n                        figsize=(14, 6),\n                        legend=['Pre-synaptic Neuron'],\n                        colors='#f14a16',\n                        title=['Pre-synaptic Trace Dynamics'],\n                        num_steps=num_steps\n                        )\n``````\n\n``````\n[17]:\n``````\n\n``````\nplot_spikes_time_series(time=time,\n                        time_series=post_trace[:, 0],\n                        spikes=[np.where(post_spikes[:, 0])[0]],\n                        figsize=(14, 6),\n                        legend=['Post-synaptic Neuron A'],\n                        colors='#458b00',\n                        title=['Post-synaptic Trace Dynamics (Neuron A)'],\n                        num_steps=num_steps\n                        )\n\nplot_spikes_time_series(time=time,\n                        time_series=post_trace[:, 1],\n                        spikes=[np.where(post_spikes[:, 1])[0]],\n                        figsize=(14, 6),\n                        legend=['Post-synaptic Neuron B'],\n                        colors='#ff9912',\n                        title=['Post-synaptic Trace Dynamics (Neuron B)'],\n                        num_steps=num_steps\n                        )\n``````\n\n``````\n[18]:\n``````\n\n``````\n# Plotting tag dynamics (dt)\nplot_time_series_subplots(time=time,\n                          time_series_y1=tag_neuron_A,\n                          time_series_y2=tag_neuron_B,\n                          ylabel=\"Trace value\",\n                          title=\"Tag Dynamics (dt) for Post-synaptic Neurons\",\n                          figsize=(14, 4),\n                          color=['#458b00', '#ff9912'],\n                          legend=['Neuron A', 'Neuron B'],\n                          leg_loc=\"lower left\"\n                         )\n``````\n\n``````\n[19]:\n``````\n\n``````\n# Plotting reward trace dynamics\nplot_time_series_subplots(time=time,\n                          time_series_y1=reward_trace[:, 0],\n                          time_series_y2=reward_trace[:, 1],\n                          ylabel=\"Trace value\",\n                          title=\"Reward Trace for Post-synaptic Neurons\",\n                          figsize=(14, 4),\n                          color=['#458b00', '#ff9912'],\n                          legend=['Neuron A', 'Neuron B']\n                         )\n\n# Plotting weight dynamics\nplot_time_series_subplots(time=time,\n                          time_series_y1=weights_neuron_A,\n                          time_series_y2=weights_neuron_B,\n                          ylabel=\"Trace value\",\n                          title=\"Weight Dynamics (dw) for Plastic Connections\",\n                          figsize=(14, 4),\n                          color=['#458b00', '#ff9912'],\n                          legend=['Neuron A', 'Neuron B']\n                         )\n``````\n\n``````\n[20]:\n``````\n\n``````\nfrom lava.magma.core.learning.learning_rule import Loihi3FLearningRule\nfrom lava.magma.core.learning.utils import float_to_literal\n\n\nclass RewardModulatedSTDP(Loihi3FLearningRule):\n    def __init__(\n            self,\n            learning_rate: float,\n            A_plus: float,\n            A_minus: float,\n            pre_trace_decay_tau: float,\n            post_trace_decay_tau: float,\n            pre_trace_kernel_magnitude: float,\n            post_trace_kernel_magnitude: float,\n            eligibility_trace_decay_tau: float,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\n        Reward-Modulated Spike-timing dependent plasticity (STDP)\n        as defined in Frémaux, Nicolas, and Wulfram Gerstner.\n        \"Neuromodulated spike-timing-dependent plasticity, and\n        theory of three-factor learning rules.\" Frontiers in\n        neural circuits 9 (2016).\n\n        Parameters\n        ==========\n\n        learning_rate: float\n            Overall learning rate scaling the intensity of weight changes.\n        A_plus:\n            Scaling the weight change on pre-synaptic spike times.\n        A_minus:\n            Scaling the weight change on post-synaptic spike times.\n        pre_trace_decay_tau:\n            Decay time constant of the pre-synaptic activity trace.\n        post_trace_decay_tau:\n            Decay time constant of the post-synaptic activity trace.\n        pre_trace_kernel_magnitude:\n            The magnitude of increase to the pre-synaptic trace value\n            at the instant of pre-synaptic spike.\n        post_trace_kernel_magnitude:\n            The magnitude of increase to the post-synaptic trace value\n            at the instant of post-synaptic spike.\n        eligibility_trace_decay_tau:\n            Decay time constant of the eligibility trace.\n\n        \"\"\"\n        self.learning_rate = float_to_literal(learning_rate)\n        self.A_plus = float_to_literal(A_plus)\n        self.A_minus = float_to_literal(A_minus)\n        self.pre_trace_decay_tau = pre_trace_decay_tau\n        self.post_trace_decay_tau = post_trace_decay_tau\n        self.pre_trace_kernel_magnitude = pre_trace_kernel_magnitude\n        self.post_trace_kernel_magnitude = post_trace_kernel_magnitude\n        self.eligibility_trace_decay_tau = \\\n            float_to_literal(eligibility_trace_decay_tau)\n\n        # Trace impulse values\n        x1_impulse = pre_trace_kernel_magnitude\n\n        # Trace decay constants\n        x1_tau = self.pre_trace_decay_tau\n\n        # Eligibility trace represented as dt\n        dt = f\"{self.learning_rate} * {self.A_minus} * x0 * y1 + \" \\\n             f\"{self.learning_rate} * {self.A_plus} * y0 * x1 - \" \\\n             f\"u0 * t * {self.eligibility_trace_decay_tau}\"\n\n        # Reward-modulated weight update\n        dw = \" u0 * t * y2 \"\n\n        super().__init__(\n            dw=dw,\n            dt=dt,\n            x1_impulse=x1_impulse,\n            x1_tau=x1_tau,\n            *args,\n            **kwargs\n        )\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html",
    "title": "Installing Lava — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nYou can run Lava on any 64-bit systems with the following configuration:\n\nLanguage Support - Python 3.8+\n\nOS Support - Ubuntu 20.04 or later - Windows 10 - MacOS\n\nWe recommend using a 4 core CPU and 8 GB RAM as your machine configuration setup for compiling and executing your Lava applications.\n\nWe highly recommend cloning the repository and using pybuilder to setup lava. You will need to install pybuilder for this purpose.\n\nOpen a python terminal and run based on the OS you are on:\n\ncd $HOME\n\npip install -U pip\n\npip install “poetry>=1.1.13”\n\ngit clone[git@github.com](mailto:git%40github.com)git@github.com:lava-nc/lava.git\n\ncd lava\n\npoetry config virtualenvs.in-project true\n\npoetry install\n\nsource .venv/bin/activate\n\npytest\n\nNote that you should install the core Lava repository (lava) before installing other Lava libraries such as lava-optimization or lava-dl.\n\nCommands in PowerShell\n\ncd $HOME\n\ngit clone[git@github.com](mailto:git%40github.com)git@github.com:lava-nc/lava.git\n\ncd lava\n\npython3 -m venv .venv\n\n.venv:nbsphinx-math:ScriptsScripts\\activate\n\npip install -U pip\n\npip install “poetry>=1.1.13”\n\npoetry config virtualenvs.in-project true\n\npoetry install\n\npytest\n\nNote that you should install the core Lava repository (lava) before installing other Lava libraries such as lava-optimization or lava-dl.\n\nIf you only need the lava package in your python environment, we will publish Lava releases via[GitHub Releases](https://github.com/lava-nc/lava/releases)GitHub Releases. Please download the package and install it.\n\nOpen a python terminal and run:\n\npython3 -m venv .venv\n\n.venv:nbsphinx-math:ScriptsScripts\\activate\n\npip install -U pip\n\npip install lava-nc-<version>.tar.gz\n\npython3 -m venv .venv\n\nsource .venv/bin/activate\n\npip install -U pip\n\npip install lava-nc-<version>.tar.gz\n\nIntel’s neuromorphic Loihi 1 or 2 research systems are currently not available commercially. Developers interested in using Lava with Loihi systems, need to join the[Intel Neuromorphic Research Community (INRC)](https://www.intel.com/content/www/us/en/research/neuromorphic-community.html)Intel Neuromorphic Research Community (INRC). Once a member of the INRC, developers will gain access to cloud-hosted Loihi systems or are able to obtain physical Loihi systems on a loan basis. In addition, Intel will provide further proprietary\ncomponents of the magma library which enable compiling processes for Loihi systems that need to be installed into the same Lava namespace.\n\nTo request INRC membership, please write an email to[inrc_interest@intel.com](mailto:inrc_interest%40intel.com)inrc_interest@intel.comwith a research proposal.\n\nInstructions how to run Lava on Loihi were sent to you during your INRC Account Setup. If you need support, email:[nrc_support@intel-research.net](mailto:nrc_support%40intel-research.net)nrc_support@intel-research.net\n\nWe have published a[Lava Developer Guide](https://lava-nc.org/developer_guide.html)Lava Developer Guidewhich will help you to understand how you can contribute to Lava by following some known best practices and CI/CD processes.\n\nLava helps you to get started by providing a range of[Tutorials](https://github.com/lava-nc/lava/tree/main/tutorials/)Tutorials.\n\nexport PYTHONPATH=~/lava/src\n\npip install jupyter\n\njupyter notebook\n\nOpen browser based on URL shown in the jupyter log\n\nNavigate in the notebook tolava ~> tutorials\n\nset PYTHONPATH=~/lava/src\n\npip install jupyter\n\njupyter notebook\n\nOpen browser based on URL shown in the jupyter log\n\nNavigate in the notebook tolava ~> tutorials\n\nTo dive into Lava, start by writing your own end-to-end[MNIST classifier](https://lava-nc.org/lava/notebooks/end_to_end/tutorial01_mnist_digit_classification.html)MNIST classifieror by diving into our series of in-depth tutorials, starting with a introduction to[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes.\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/%20%22Lava%20Documentation%22)Lava documentation.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb%20%22INRC%20Newsletter%22)INRC newsletter.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial02_process_models.ipynb",
    "title": "Page not found · GitHub Pages",
    "content": "File not found\n\nThe site configured at this address does not\n        contain the requested file.\n\nIf this is your site, make sure that the filename case matches the URL\n        as well as any file permissions.For root URLs (like`http://example.com/`http://example.com/) you must provide an`index.html`index.htmlfile.\n\n[Read the full documentation](https://help.github.com/pages/)Read the full documentationfor more information about usingGitHub Pages."
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html",
    "title": "Processes — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nLearn how to createProcesses, the fundamental computational units used in Lava to build algorithms and applications.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\nThis tutorial will show how to create aProcessthat simulates a group of leaky integrate-and-fire neurons. But in Lava, the concept ofProcessesapplies widely beyond this example. In general, aProcessdescribes an individual program unit which encapsulates\n\ndata that store its state,\n\nalgorithms that describe how to manipulate the data,\n\nports that share data with other Processes, and\n\nan API that facilitates user interaction.\n\nAProcesscan thus be as simple as a single neuron or a synapse, as complex as a full neural network, and as non-neuromorphic as a streaming interface for a peripheral device or an executed instance of regular program code.\n\nProcessesare independent from each other as they primarily operate on their own local memory while they pass messages between each other via channels. DifferentProcessesthus proceed their computations simultaneously and asynchronously, mirroring the high parallelism inherent in neuromorphic hardware. The parallelProcessesare furthermore safe against side effects from shared-memory interaction.\n\nOnce aProcesshas been coded in Python, Lava allows to run it seamlessly across different backends such as a CPU, a GPU, or neuromorphic cores. Developers can thus easily test and benchmark their applications on classical computing hardware and then deploy it to neuromorphic hardware. Furthermore, Lava takes advantage of distributed, heterogeneous hardware such as Loihi as it can run someProcesseson neuromorphic cores and in parallel others on embedded conventional CPUs and GPUs.\n\nWhile Lava provides a growing[library of Processes](https://github.com/lava-nc/lava/tree/main/src/lava/proc)library of Processes, you can easily write your own processes that suit your needs.\n\nAllProcessesin Lava share a universal architecture as they inherit from the sameAbstractProcessclass. EachProcessconsists of the following four key components.\n\nWhen you create your own new process, you need to inherit from the AbstractProcess class. As an example, we will implement theclass LIF, a group of leaky integrate-and-fire (LIF) neurons.\n\nComponent\n\nName\n\nPython\n\nPorts\n\na_{in}\n\nInport\n\nReceives spikes from upstream neurons.\n\ns_{out}\n\nOutport\n\nTransmits spikes to downstream neurons.\n\nState\n\nu\n\nVar\n\nSynaptic current of the LIF neurons.\n\nv\n\nVar\n\nMembrane voltage of the LIF neurons.\n\ndu\n\nVar\n\nA time constant describing the current leakage.\n\ndv\n\nVar\n\nA time constant describing the voltage leakage.\n\nbias\n\nVar\n\nA bias value.\n\nvth\n\nVar\n\nA constant threshold that the membrane voltage needs to exceed for a spike.\n\nAPI\n\nAll Vars\n\nVar\n\nAll publicVarsare considered part of theProcessAPI.\n\nAll Ports\n\nAbstractPort\n\nAllPortsare considered part of theProcessAPI.\n\nprint_vars\n\ndef\n\nA function that prints all internal variables to help the user see if the LIF neuron has correctly been set up.\n\nThe following code implements the classLIFthat you can also find in Lava’sProcesslibrary, but extends it by an additional API method that prints the state of the LIF neurons.\n\nYou may have noticed that most of theVarswere initialized by scalar integers. But the synaptic currentuillustrates thatVarscan in general be initialized with numeric objects that have a dimensionality equal or less than specified by itsshapeargument. The initial value will be scaled up to match theVardimension at run time.\n\nThere are two further important things to notice about theProcessclass:\n\nIt only defines the interface of the LIF neuron, but not its temporal behavior.\n\nIt is fully agnostic to the computing backend and will thus remain the same if you want to run your code, for example, once on a CPU and once on Loihi.\n\nThe behavior of aProcessis defined by itsProcessModel. For the specific example of LIF neuron, theProcessModeldescribes how their current and voltage react to a synaptic input, how these states evolve with time, and when the neurons should emit a spike.\n\nA singleProcesscan have severalProcessModels, one for each backend that you want to run it on.\n\nThe following code implements aProcessModelthat defines how a CPU should run the LIFProcess. Please do not worry about the precise implementation here—the code will be explained in detail in the next[Tutorial on ProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)Tutorial on ProcessModels.\n\nNow we can create an instance of ourProcess, in this case a group of 3 LIF neurons.\n\nOnce you have instantiated a group of LIF neurons, you can easily interact with them.\n\nYou can always read out the current values of the processVarsto determine theProcessstate. For example, all three neurons should have been initialized with a zero membrane voltage.\n\nAs described above, theVarvhas in this example been initialized as a scalar value that describes the membrane voltage of all three neurons simultaneously.\n\nTo facilitate how users can interact with yourProcess, they can use the custom APIs that you provide them with. For LIF neurons, you defined a custom function that allows the user to inspect the internalVarsof the LIFProcess. Have a look if allVarshave been set up correctly.\n\nOnce theProcessis instantiated and you are satisfied with its state, you can run theProcess. As long as aProcessModelhas been defined for the desired backend, theProcesscan run seamlessly across computing hardware. Do not worry about the details here—you will learn all about how Lava builds, compiles, and runsProcessesin a[separate tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)separate tutorial.\n\nTo run aProcess, specify the number of steps to run for and select the desired backend.\n\nThe voltage of each LIF neuron should now have increased by the bias value, 3, from their initial values of 0. Check if the neurons have evolved as expected.\n\nYou can furthermore update the internalVarsmanually. You may, for example, set the membrane voltage to new values between two runs.\n\nNote that theset()method becomes available once theProcesshas been run. Prior to the first run, use the__init__function of theProcessto setVars.\n\nLater tutorials will illustrate more sophisticated ways to access, store, and change variables during run time usingProcesscode.\n\nIn the end, stop the process to terminate its execution.\n\nLearn how to implement the behavior ofProcessesin the[next tutorial on ProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)next tutorial on ProcessModels.\n\nIf you want to find out more aboutProcesses, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/core/process/process.py)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nimport numpy as np\n\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n\n\nclass LIF(AbstractProcess):\n    \"\"\"Leaky-Integrate-and-Fire neural process with activation input and spike\n    output ports a_in and s_out.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__()\n        shape = kwargs.get(\"shape\", (1,))\n        self.a_in = InPort(shape=shape)\n        self.s_out = OutPort(shape=shape)\n        self.u = Var(shape=shape, init=0)\n        self.v = Var(shape=shape, init=0)\n        self.du = Var(shape=(1,), init=kwargs.pop(\"du\", 0))\n        self.dv = Var(shape=(1,), init=kwargs.pop(\"dv\", 0))\n        self.bias = Var(shape=shape, init=kwargs.pop(\"bias\", 0))\n        self.vth = Var(shape=(1,), init=kwargs.pop(\"vth\", 10))\n\n    def print_vars(self):\n        \"\"\"Prints all variables of a LIF process and their values.\"\"\"\n\n        sp = 3 * \"  \"\n        print(\"Variables of the LIF:\")\n        print(sp + \"u:    {}\".format(str(self.u.get())))\n        print(sp + \"v:    {}\".format(str(self.v.get())))\n        print(sp + \"du:   {}\".format(str(self.du.get())))\n        print(sp + \"dv:   {}\".format(str(self.dv.get())))\n        print(sp + \"bias: {}\".format(str(self.bias.get())))\n        print(sp + \"vth:  {}\".format(str(self.vth.get())))\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires, tag\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n@implements(proc=LIF, protocol=LoihiProtocol)\n@requires(CPU)\n@tag('floating_pt')\nclass PyLifModel(PyLoihiProcessModel):\n    a_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float)\n    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\n    u: np.ndarray = LavaPyType(np.ndarray, float)\n    v: np.ndarray = LavaPyType(np.ndarray, float)\n    bias: np.ndarray = LavaPyType(np.ndarray, float)\n    du: float = LavaPyType(float, float)\n    dv: float = LavaPyType(float, float)\n    vth: float = LavaPyType(float, float)\n\n    def run_spk(self):\n        a_in_data = self.a_in.recv()\n        self.u[:] = self.u * (1 - self.du)\n        self.u[:] += a_in_data\n        bias = self.bias\n        self.v[:] = self.v * (1 - self.dv) + self.u + bias\n        s_out = self.v >= self.vth\n        self.v[s_out] = 0  # Reset voltage to 0\n        self.s_out.send(s_out)\n``````\n\n``````\n[3]:\n``````\n\n``````\nn_neurons = 3\n\nlif = LIF(shape=(3,), du=0, dv=0, bias=3, vth=10)\n``````\n\n``````\n[4]:\n``````\n\n``````\nprint(lif.v.get())\n``````\n\n``````\n0\n``````\n\n``````\n[5]:\n``````\n\n``````\nlif.print_vars()\n``````\n\n``````\nVariables of the LIF:\n      u:    0\n      v:    0\n      du:   0\n      dv:   0\n      bias: 3\n      vth:  10\n``````\n\n``````\n[6]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\n\nlif.run(condition=RunSteps(num_steps=1), run_cfg=Loihi1SimCfg())\n``````\n\n``````\n[7]:\n``````\n\n``````\nprint(lif.v.get())\n``````\n\n``````\n[3. 3. 3.]\n``````\n\n``````\n[8]:\n``````\n\n``````\nlif.v.set(np.array([1, 2, 3]) )\nprint(lif.v.get())\n``````\n\n``````\n[1. 2. 3.]\n``````\n\n``````\n[9]:\n``````\n\n``````\nlif.stop()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html",
    "title": "ProcessModels — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nThis tutorial explains how LavaProcessModelsimplement the behavior of LavaProcesses. Each LavaProcessmust have one or moreProcessModels, which provide the instructions for how to execute a LavaProcess. LavaProcessModelsallow a user to specify a Process’s behavior in one or more languages (like Python, C, or the Loihi neurocore interface) and for various compute resources (like CPUs, GPUs, or Loihi chips). In this way,ProcessModelsenable seamles cross-platform execution\nofProcessesand allow users to build applications and algorithms agonostic of platform-specific implementations.\n\nThere are two broad classes ofProcessModels:LeafProcessModelandSubProcessModel.LeafProcessModels, which will be the focus of this tutorial, implement the behavior of a process directly.SubProcessModelsallow users to implement and compose the behavior of aProcessusing otherProcesses, thus enabling the creation of HierarchicalProcesses.\n\nIn this tutorial, we walk through the creation of multipleLeafProcessModelsthat could be used to implement the behavior of a Leaky Integrate-and-Fire (LIF) neuronProcess.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\nFirst, we will define our LIFProcessexactly as it is defined in the`Magma`Magmacore library of Lava. (For more information on defining Lava Processes, see the[previous tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)previous tutorial.) Here the LIF neuralProcessaccepts activity from synaptic inputs viaInPort`a_in`a_inand outputs spiking activity viaOutPort`s_out`s_out.\n\nNow, we will create a PythonProcessModel, orPyProcessModel, that runs on a CPU compute resource and implements the LIFProcessbehavior.\n\nWe begin by importing the required Lava classes. First, we setup our compute resources (CPU) and ourSyncProtocol. ASyncProtocoldefines how and when parallelProcessessynchronize. Here we use theLoihiProtoicolwhich defines the synchronization phases required for execution on the Loihi chip, but users may also specify a completely asynchronous protocol or define a customSyncProtocol. The decorators imported will be necessary to specify the resourceRequirementsandSyncProtocolof ourProcessModel.\n\nNow we import the parent class from which ourProcessModelinherits, as well as our requiredPortandVariabletypes.PyLoihiProcessModelis the abstract class for a PythonProcessModelthat implements theLoihiProtocol. OurProcessModelneedsPortsandVariablesthat mirror those the LIFProcess. The in-ports and out-ports of a PythonProcessModelhave typesPyInPortandPyOutPort, respectively, while variables have typeLavaPyType.\n\nWe now define aLeafProcessModel`PyLifModel`PyLifModelthat implements the behavior of the LIFProcess.\n\nThe`@implements`@implementsdecorator specifies theSyncProtocol(`protocol=LoihiProtocol`protocol=LoihiProtocol) and the class of theProcess(`proc=LIF`proc=LIF) corresponding to theProcessModel. The`@requires`@requiresdecorator specifies the CPU compute resource required by theProcessModel. The`@tag`@tagdecorator specifies the precision of theProcessModel. Here we illustrate aProcessModelwith standard, floating point precision.\n\nNext we define theProcessModelvariables and ports. The variables and ports defined in theProcessModelmust exactly match (by name and number) the variables and ports defined in the correspondingProcessfor compilation. Our LIF exampleProcessand`PyLifModel`PyLifModeleach have 1 input port, 1 output port, and variables for`u`u,`v`v,`du`du,`dv`dv,`bias`bias,`bias_exp`bias_exp, and`vth`vth. Variables and ports in aProcessModelmust be initialized withLavaTypeobjects specific to the\nlanguage of theLeafProcessModelimplementation. Here, variables are initialized with the`LavaPyType`LavaPyTypeto match our PythonLeafProcessModelimplementation. In general,LavaTypesspecify the class-types of variables and ports, including their numeric d_type, precision and dynamic range. The LavaCompilerreads theseLavaTypesto initialize concrete class objects from the initial values provided in theProcess.\n\nWe then fill in the`run_spk()`run_spk()method to execute the LIF neural dynamics.`run_spk()`run_spk()is a method specific toLeafProcessModelsof type`PyLoihiProcessModel`PyLoihiProcessModelthat executes user-defined neuron dynamics with correct handling of all phases our`LoihiProtocol`LoihiProtocolSyncProtocol. In this example,`run_spike`run_spikewill accept activity from synaptic inputs viaPyInPort`a_in`a_in, and, after integrating current and voltage according to current-based (CUBA) dynamics, output spiking activity viaPyOutPort`s_out`s_out.`recv()`recv()and`send()`send()are the methods that support the channel based communication of the inputs and outputs to ourProcessModel. For more detailed information about Ports and channel-based communication, see the[Connection Tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)Connection Tutorial.\n\nWe have demonstrated multipleProcessModelimplementations of a single LIFProcess. How is one of severalProcessModelsthen selected as the implementation of aProcessduring runtime? To answer that question, we take a deeper dive into the attributes of aLeafProcessModeland the relationship between aLeafProcessModel, aProcess, and aSyncProtocol.\n\nAs shown below, aLeafProcessModelimplements both a Process (in our example, LIF) and aSyncProtocol(in our example, theLoihiProtocol). ALeafProcessModelhas a singleType. In this tutorial`PyLifModel`PyLifModelhas Type`PyLoihiProcessModel`PyLoihiProcessModel. ALeafProcessModelalso has one or more resourceRequirementsthat specify the compute resources (for example, a CPU, a GPU, or Loihi Neurocores) or peripheral resources (like access to a camera) that are required for execution. Finally, aLeafProcessModelcan have one and more user-defineableTags.Tagscan be used, among other customizable reasons, to group multipleProcessModelsfor a multi-Processapplication or to distinguish between multipleLeafProcessModelimplementations with the sameTypeandSyncProtocol. As an example, we illustrated above a`PyLoihiProcessModel`PyLoihiProcessModelfor LIF that uses floating point precision and has the tag`@tag('floating_pt')`@tag('floating_pt'). There also exists a`PyLoihiProcessModel`PyLoihiProcessModelthat uses\nfixed point precision and has behavior that is bit-accurate with LIF execution on a Loihi chip; thisProcessModelis distinguished by the tag`@tag('fixed_pt')`@tag('fixed_pt'). Together, theType,TagandRequirementattributes of aLeafProcessModelallow users to define aRunConfigthat chooses which of severalLeafProcessModelsis used to implement aProcessat runtime. The Core Lava Library will also provide several preconfiguredRunConfigs.\n\nLearn how to execute singleProcessesand networks ofProcessesin the[next tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)next tutorial.\n\nIf you want to find out more aboutProcessModels, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/core/model/model.py)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to[our newsletter](http://eepurl.com/hJCyhb)our newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n\nclass LIF(AbstractProcess):\n    \"\"\"Leaky-Integrate-and-Fire (LIF) neural Process.\n\n    LIF dynamics abstracts to:\n    u[t] = u[t-1] * (1-du) + a_in                              # neuron current\n    v[t] = v[t-1] * (1-dv) + u[t] + bias_mant * 2 ** bias_exp  # neuron voltage\n    s_out = v[t] > vth                                         # spike if threshold is exceeded\n    v[t] = 0                                                   # reset at spike\n\n    Parameters\n    ----------\n    du: Inverse of decay time-constant for current decay.\n    dv: Inverse of decay time-constant for voltage decay.\n    bias_mant: Mantissa part of neuron bias.\n    bias_exp: Exponent part of neuron bias, if needed. Mostly for fixed point\n              implementations. Unnecessary for floating point\n              implementations. If specified, bias = bias_mant * 2**bias_exp.\n    vth: Neuron threshold voltage, exceeding which, the neuron will spike.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__()\n        shape = kwargs.get(\"shape\", (1,))\n        du = kwargs.pop(\"du\", 0)\n        dv = kwargs.pop(\"dv\", 0)\n        bias_mant = kwargs.pop(\"bias_mant\", 0)\n        bias_exp = kwargs.pop(\"bias_exp\", 0)\n        vth = kwargs.pop(\"vth\", 10)\n\n        self.shape = shape\n        self.a_in = InPort(shape=shape)\n        self.s_out = OutPort(shape=shape)\n        self.u = Var(shape=shape, init=0)\n        self.v = Var(shape=shape, init=0)\n        self.du = Var(shape=(1,), init=du)\n        self.dv = Var(shape=(1,), init=dv)\n        self.bias_mant = Var(shape=shape, init=bias_mant)\n        self.bias_exp = Var(shape=shape, init=bias_exp)\n        self.vth = Var(shape=(1,), init=vth)\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\nfrom lava.magma.core.decorator import implements, requires\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n``````\n\n``````\n[3]:\n``````\n\n``````\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\n``````\n\n``````\n[4]:\n``````\n\n``````\nimport numpy as np\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires, tag\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n@implements(proc=LIF, protocol=LoihiProtocol)\n@requires(CPU)\n@tag('floating_pt')\nclass PyLifModel1(PyLoihiProcessModel):\n    a_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float)\n    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\n    u: np.ndarray = LavaPyType(np.ndarray, float)\n    v: np.ndarray = LavaPyType(np.ndarray, float)\n    bias_mant: np.ndarray = LavaPyType(np.ndarray, float)\n    bias_exp: np.ndarray = LavaPyType(np.ndarray, float)\n    du: float = LavaPyType(float, float)\n    dv: float = LavaPyType(float, float)\n    vth: float = LavaPyType(float, float)\n\n    def run_spk(self):\n        a_in_data = self.a_in.recv()\n        self.u[:] = self.u * (1 - self.du)\n        self.u[:] += a_in_data\n        bias = self.bias_mant * (2 ** self.bias_exp)\n        self.v[:] = self.v * (1 - self.dv) + self.u + bias\n        s_out = self.v >= self.vth\n        self.v[s_out] = 0  # Reset voltage to 0\n        self.s_out.send(s_out)\n``````\n\n``````\n[5]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\n\nlif = LIF(shape=(3,), du=0, dv=0, bias_mant=3, vth=10)\n\nrun_cfg = Loihi1SimCfg()\nlif.run(condition=RunSteps(num_steps=10), run_cfg=run_cfg)\nprint(lif.v.get())\n``````\n\n``````\n[6. 6. 6.]\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html",
    "title": "Execution — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nThis tutorial covers how to execute singleProcessesand networks ofProcesses, how to configure execution, how to pause, resume, and stop execution, and how to manually set up aCompilerandRunTimefor more fine-grained control.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\nTo start executing aProcesscall its method`run(condition=...,run_cfg=...)`run(condition=...,run_cfg=...). The execution must be configured by passing in both aRunConditionand aRunConfiguration.\n\nARunConditionspecifies how long aProcessis executed.\n\nThe run conditionRunStepsexecutes aProcessfor a specified number time steps, here 42 in the example below. The execution will automatically pause after the specified number of time steps. You can also specify whether or not the call to`run()`run()will block the program flow.\n\nThe run conditionRunContinuousenables you to run aProcesscontinuously. In this case, theProcesswill run indefinitely until you explicitly call`pause()`pause()or`stop()`stop()(see below). This call never blocks the program flow (blocking=False).\n\nARunConfigspecifies on what devices theProcessesshould be executed. Based on theRunConfig, aProcessselects and initializes exactly one of its associated[ProcessModels](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModels, which implement the behavior of theProcessin a particular programming language and for a particular computing resource. If theProcesshas aSubProcessModelcomposed of otherProcesses, theRunConfigchooses the appropriateProcessModelimplementation of the\nchildProcess.\n\nLava currently supports execution on Loihi 2 using the predefined RunConfig Loihi2HwCfg. Simulation of Loihi executed on a single CPU is possible using the RunConfig Loihi1SimCfg. We will make more predefined run configurations available with the upcoming support for Loihi 1 and other devices such as GPUs.\n\nThe example below specifies that theProcess(and all its connectedProcessesandSubProcesses) are executed in Python on a CPU.\n\nWe can now use both aRunConditionand aRunConfigto execute a simple leaky integrate-and-fire (LIF) neuron.\n\nCalling`run()`run()on aProcesswill also execute allProcessesthat are connected to it. In the example below, threeProcesseslif1,dense, andlif2are connected in a sequence. We call`run()`run()onProcesslif2. Sincelif2is connected todenseanddenseis connected tolif1, all threeProcesseswill be executed. As demonstrated here, the execution will cover the entire connected network ofProcesses, irrespective of the direction in which theProcessesare\nconnected.\n\nAnother way to executeProcessesis to use theRunConditionRunContinuouswhich runs the network non-blocking until the the execution is paused or stopped by the user.\n\nCalling the`pause()`pause()method of aProcesspauses execution but preserves its state. TheProcesscan then be inspected and manipulated by the user, as shown in the example below.\n\nAfterward, execution can be resumed by calling`run()`run()again.\n\nCalling the`stop()`stop()method of aProcesscompletely terminates its execution. Contrary to pausing execution,`stop()`stop()does not preserve the state of theProcess. If aProcessexecuted on a hardware device, the connection between theProcessand the device is terminated as well.\n\nIn many cases, creating an instance of aProcessand calling its`run()`run()method is all you need to do. Calling`run()`run()internally first compiles theProcessand then starts execution. These steps can also be manually invoked in sequence, for instance to inspect or manipulate theProcessbefore starting execution.\n\nInstantiation stage: This is the call to the init-method of aProcess, which instantiates an object of theProcess.\n\nConfiguration stage: After aProcesshas been instantiated, it can be configured further through its public API and connected to otherProcessesvia itsPorts. In addition, probes can be defined for LavaVarsin order to record a time series of its evolution during execution.\n\nCompile stage: After aProcesshas been configured, it needs to be compiled to become executable. After the compilation stage, the state of theProcesscan still be manipulated and inspected.\n\nExecution stage: When compilation is complete,Processescan be executed. The execution stage ensures that the (prior) compilation stage has been completed and otherwise invokes it.\n\nThe following does all of the above automatically:\n\nLearn more about Lava in the next tutorial about how to[transfer data between Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)transfer data between Processes.\n\nIf you want to find out more about how to compile and executeProcesses, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[Compiler](https://github.com/lava-nc/lava/tree/main/src/lava/magma/compiler/)Compilerand[RunTime source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/runtime/)RunTime source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\n\nrun_condition = RunSteps(num_steps=42, blocking=False)\n``````\n\n``````\n[2]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunContinuous\n\nrun_condition = RunContinuous()\n``````\n\n``````\n[3]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\nrun_cfg = Loihi1SimCfg()\n``````\n\n``````\n[4]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\n# create a Process for a LIF neuron\nlif = LIF(shape=(1,))\n\n# execute that Process for 42 time steps in simulation\nlif.run(condition=RunSteps(num_steps=42), run_cfg=Loihi1SimCfg())\n``````\n\n``````\n[5]:\n``````\n\n``````\nimport numpy as np\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.dense.process import Dense\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\n# create processes\nlif1 = LIF(shape=(1,))\ndense = Dense(weights=np.eye(1))\nlif2 = LIF(shape=(1,))\n\n# connect the OutPort of lif1 to the InPort of dense\nlif1.s_out.connect(dense.s_in)\n# connect the OutPort of dense to the InPort of lif2\ndense.a_out.connect(lif2.a_in)\n\n# execute Process lif2 and all Processes connected to it (dense, lif1)\nlif2.run(condition=RunSteps(num_steps=42), run_cfg=Loihi1SimCfg())\n``````\n\n``````\n[6]:\n``````\n\n``````\nimport numpy as np\nfrom lava.proc.lif.process import LIF\nfrom lava.magma.core.run_conditions import RunContinuous\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\nlif3 = LIF(shape=(1, ))\n\n# start continuous execution\nlif3.run(condition=RunContinuous(), run_cfg=Loihi1SimCfg())\n\n# pause execution\nlif3.pause()\n\n# inspect the state of the Process, here, the voltage variable 'v'\nprint(lif3.v.get())\n\n# manipulate the state of the Process, here, resetting the voltage to zero\nlif3.v.set(np.array([0]))\n\n# resume continuous execution\nlif3.run(condition=RunContinuous(), run_cfg=Loihi1SimCfg())\n\n# terminate execution;\n# after this, you no longer have access to the state of lif\nlif3.stop()\n``````\n\n``````\n[0.]\n``````\n\n``````\n[7]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.dense.process import Dense\n\nlif1 = LIF(shape=(1,))\ndense = Dense(weights=np.eye(1))\nlif2 = LIF(shape=(1,))\n``````\n\n``````\n[8]:\n``````\n\n``````\n# connect the processes\nlif1.s_out.connect(dense.s_in)\ndense.a_out.connect(lif2.a_in)\n``````\n\n``````\n[9]:\n``````\n\n``````\nfrom lava.magma.compiler.compiler import Compiler\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\n# create a compiler\ncompiler = Compiler()\n\n# compile the Process (and all connected Processes) into an executable\nexecutable = compiler.compile(lif2, run_cfg=Loihi1SimCfg())\n``````\n\n``````\n[10]:\n``````\n\n``````\nfrom lava.magma.runtime.runtime import Runtime\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.process.message_interface_enum import ActorType\n\n# create and initialize a runtime\nmp = ActorType.MultiProcessing\nruntime = Runtime(exe=executable,\n                  message_infrastructure_type=mp)\nruntime.initialize()\n\n# start execution\nruntime.start(run_condition=RunSteps(num_steps=42))\n\n# stop execution\nruntime.stop()\n``````\n\n``````\n[11]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.dense.process import Dense\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi1SimCfg\n\n# create Processes\nlif = LIF(shape=(1,))\ndense = Dense(weights=np.eye(1))\n\n# connect Processes\nlif.s_out.connect(dense.s_in)\n\n# execute Processes\nlif.run(condition=RunSteps(num_steps=42), run_cfg=Loihi1SimCfg())\n\n# stop Processes\nlif.stop()\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html",
    "title": "Connect processes — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nThis tutorial gives an introduction in how to connectProcessesto build a network of asynchronously operating and interactingProcesses.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\nProcessesare the main building blocks of Lava. EachProcesscan exercise different computations and usually depends on some input data and/or creates output data. Transfering I/O data betweenProcessesis a key element of Lava. AProcesscan have various input and outputPortswhich are then connected via channels to correspondingPortsof anotherProcess. This allows to build networks of asynchronously operating and interactingProcesses.\n\nThe objective is to connectProcessP1withProcessP2.P1has an outputPortOutPortcalledoutandP2has an input portInPortcalledinp. Data fromP1provided to thePortoutshould be transfered toP2and received fromPortinp.\n\nAs first step we define theProcessesP1andP2with their respectivePortsoutandinp.\n\nProcessP1andP2require a correspondingProcessModelwhich implements theirPortsand a simple RunConfig for sending and receiving data.\n\nTheProcessModelscan be written in Python and should be exectued on a CPU. The input and outputPortshould be able to receive/send a vector of integers and print the transferred data.\n\nSo theProcessModelinherits fromAbstractPyProcessModelin order to execute Python code and the configuredComputeResourceis a CPU. ALavaPyTypeis used for thePorts. TheLavaPyTypespecifies the expected data format for thePort. A dense vector of type integer is chosen with the parameters _PyOutPort._VEC_DENSE andint. ThePortscan be used to send and receive data by callingsendorrecv. The sent and received data is afterwards printed out.\n\nNext, the processesP1andP2are instantiated and the outputPortoutfromProcessP1is connected with the inputPortinpofProcessP2.\n\nCalling`run()`run()on either of theseProcesseswill first call theCompiler. During compilation the specified connection is setup by creating a channel betweenP1andP2. Now data can be transfered during execution as seen by the output print statements.\n\nThe instance`sender`senderof P1 sent the data`[12]`[12]via itsOutPort`out`outto theInPort`in`inof the instance`recv`recvof P2, where the data is received.\n\nThis first example was very simple. In principle,Processescan have multiple input and outputPortswhich can be freely connected with each other. Also,Processeswhich execute on different compute resources can be connected in the same way.\n\nInPortscannot connect toOutPorts\n\nShape and datatype of connectPortsmust match\n\nAnInPortmight get data from multipleOutPorts- default behavior is a summation of the incoming data\n\nAnOutPortmight send data to multipleInPorts- allInPortsreceive the same data\n\nThe instance`sender`senderof P1 sent the data`[12]`[12]to the 3 instances`recv1,recv2,recv3`recv1,recv2,recv3of P2.\n\nIf multiple inputPortsconnect to the same outputPortthe default behavior is that the data from each inputPortis added up at the outputPort.\n\nThe 3 instances`sender1,sender2,sender3`sender1,sender2,sender3of P1 sent the data`[12]`[12]to the instance`recv`recvof P2, where the data was summed up to`[36]`[36].\n\nLearn how to implement and compose the behavior of a process using other processes in the[next tutorial on hierarchical Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)next tutorial on hierarchical Processes.\n\nIf you want to find out more about connecting processes, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/core/process/ports/ports.py)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n``````\n\n``````\n[2]:\n``````\n\n``````\n# Minimal process with an OutPort\nclass P1(AbstractProcess):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        shape = kwargs.get('shape', (2,))\n        self.out = OutPort(shape=shape)\n\n\n# Minimal process with an InPort\nclass P2(AbstractProcess):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        shape = kwargs.get('shape', (2,))\n        self.inp = InPort(shape=shape)\n``````\n\n``````\n[3]:\n``````\n\n``````\nimport numpy as np\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\nfrom lava.magma.core.decorator import implements, requires, tag\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n``````\n\n``````\n[4]:\n``````\n\n``````\n# A minimal PyProcModel implementing P1\n@implements(proc=P1, protocol=LoihiProtocol)\n@requires(CPU)\n@tag('floating_pt')\nclass PyProcModelA(PyLoihiProcessModel):\n    out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, int)\n\n    def run_spk(self):\n        data = np.array([1, 2])\n        self.out.send(data)\n        print(\"Sent output data of P1: {}\".format(data))\n\n\n\n# A minimal PyProcModel implementing P2\n@implements(proc=P2, protocol=LoihiProtocol)\n@requires(CPU)\n@tag('floating_pt')\nclass PyProcModelB(PyLoihiProcessModel):\n    inp: PyInPort = LavaPyType(PyInPort.VEC_DENSE, int)\n\n    def run_spk(self):\n        in_data = self.inp.recv()\n        print(\"Received input data for P2: {}\".format(in_data))\n``````\n\n``````\n[5]:\n``````\n\n``````\nsender = P1()\nrecv = P2()\n\n# Connecting output port to an input port\nsender.out.connect(recv.inp)\n\nsender = P1()\nrecv = P2()\n\n# ... or connecting an input port from an output port\nrecv.inp.connect_from(sender.out)\n``````\n\n``````\n[6]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\n``````\n\n``````\n[7]:\n``````\n\n``````\nsender.run(RunSteps(num_steps=1), Loihi1SimCfg())\nsender.stop()\n``````\n\n``````\nSent output data of P1: [1 2]\nReceived input data for P2: [1 2]\n``````\n\n``````\n[8]:\n``````\n\n``````\nsender = P1()\nrecv1 = P2()\nrecv2 = P2()\nrecv3 = P2()\n\n# An OutPort can connect to multiple InPorts\n# Either at once...\nsender.out.connect([recv1.inp, recv2.inp, recv3.inp])\n\nsender = P1()\nrecv1 = P2()\nrecv2 = P2()\nrecv3 = P2()\n\n# ... or consecutively\nsender.out.connect(recv1.inp)\nsender.out.connect(recv2.inp)\nsender.out.connect(recv3.inp)\n``````\n\n``````\n[9]:\n``````\n\n``````\nsender.run(RunSteps(num_steps=1), Loihi1SimCfg())\nsender.stop()\n``````\n\n``````\nReceived input data for P2: [1 2]\nSent output data of P1: [1 2]\nReceived input data for P2: [1 2]\nReceived input data for P2: [1 2]\n``````\n\n``````\n[10]:\n``````\n\n``````\nsender1 = P1()\nsender2 = P1()\nsender3 = P1()\nrecv = P2()\n\n# An InPort can connect to multiple OutPorts\n# Either at once...\nrecv.inp.connect_from([sender1.out, sender2.out, sender3.out])\n\nsender1 = P1()\nsender2 = P1()\nsender3 = P1()\nrecv = P2()\n\n# ... or consecutively\nsender1.out.connect(recv.inp)\nsender2.out.connect(recv.inp)\nsender3.out.connect(recv.inp)\n``````\n\n``````\n[11]:\n``````\n\n``````\nsender1.run(RunSteps(num_steps=1), Loihi1SimCfg())\nsender1.stop()\n``````\n\n``````\nSent output data of P1: [1 2]\nReceived input data for P2: [3 6]\nSent output data of P1: [1 2]\nSent output data of P1: [1 2]\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html",
    "title": "Hierarchical Processes and SubProcessModels — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nPrevious tutorials have briefly covered that there are two categories ofProcessModels:LeafProcessModelsandSubProcessModels. The[ProcessModel Tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel TutorialexplainedLeafProcessModelsin detail. These implement the behavior of aProcessdirectly, in the language (for example, Python or Loihi Neurocore API) required for a particular compute resource (for example, a CPU or Loihi Neurocores).SubProcessModels, by contrast, allow users to implement\nand compose the behavior of a processusing other processes. This enables the creation ofHierarchical Processesand reuse of primitiveProcessModelsto realize more complexProcessModels.SubProcessModelsinherit all compute resource requirements from the subProcessesthey instantiate.\n\nIn this tutorial, we will create a Dense Layer HierarchicalProcessthat has the behavior of Leaky-Integrate-and-Fire (LIF) neurons. The Dense LayerProcessModelimplements this behavior via the primitive LIF and Dense ConnectionProcessesand their respectivePyLoihiProcessModels.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\n[Connecting Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)Connecting Processes\n\nThe[ProcessModel Tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel Tutorialwalks through the creation of a LIFProcessand an implementingPyLoihiProcessModel. Our DenseLayerProcessadditionally requires a Dense LavaProcessandProcessModelthat have the behavior of a dense set of synaptic connections and weights. The Dense ConnectionProcesscan be used to connect neuralProcesses. For completeness, we’ll first briefly show an example LIF and DenseProcessandPyLoihiProcessModel.\n\nNow, we create a DenseLayerHierarchical Processcombining LIF neuralProcessesand Dense connectionProcesses. OurHierarchical Processcontains all of the variables (`u`u,`v`v,`bias`bias,`du`du,`dv`dvand`vth`vth) native to the LIFProcessplus the`weights`weightsvariable native to the DenseProcess. The InPort to ourHierarchical Processis`s_in`s_in, which represents the spike inputs to our Dense synaptic connections. These Dense connections synapse onto a population of LIF\nneurons. The OutPort of ourHierarchical Processis`s_out`s_out, which represents the spikes output by the layer of LIF neurons. We do not have to define thePortOutof theDenseProcess nor thePortInof theLIFProcess in theDenseLayerProcess, as they are only used internally and won’t be exposed.\n\nNow, we will create theSubProcessModelthat implements our DenseLayerProcess. This inherits from theAbstractSubProcessModelclass. Recall thatSubProcessModelsalso inherit the compute resource requirements from theProcessModelsof their childProcesses. In this example, we will use the LIF and DenseProcessModelsrequiring a CPU compute resource that were defined earlier in the tutorial, and`SubDenseLayerModel`SubDenseLayerModelwill therefore implicitly require the CPU compute resource.\n\nThe`__init__()`__init__()constructor of`SubDenseLayerModel`SubDenseLayerModelbuilds the subProcessstructure of the`DenseLayer`DenseLayerProcess. The`DenseLayer`DenseLayerProcessgets passed to the`__init__()`__init__()method via the`proc`procattribute. The`__init__()`__init__()constructor first instantiates the child LIF and DenseProcesses. Initial conditions of the`DenseLayer`DenseLayerProcess, which are required to instantiate the child LIF and DenseProcesses, are accessed through`proc.proc_params`proc.proc_params.\n\nWe then`connect()`connect()the in-port of the Dense childProcessto the in-port of the`DenseLayer`DenseLayerparentProcessand the out-port of the LIF childProcessto the out-port of the`DenseLayer`DenseLayerparentProcess. Note that ports of the`DenseLayer`DenseLayerparent process are accessed using`proc.in_ports`proc.in_portsor`proc.out_ports`proc.out_ports, while ports of a childProcesslike LIF are accessed using`self.lif.in_ports`self.lif.in_portsand`self.lif.out_ports`self.lif.out_ports. OurProcessModelalso internally`connect()`connect()s the\nout-port of the Dense connection childProcessto the in-port of the LIF neural childProcess.\n\nThe`alias()`alias()method exposes the variables of the LIF and Dense childProcessesto the`DenseLayer`DenseLayerparentProcess. Note that the variables of the`DenseLayer`DenseLayerparentProcessare accessed using`proc.vars`proc.vars, while the variables of a childProcesslike LIF are accessed using`self.lif.vars`self.lif.vars. Note that unlike aLeafProcessModel, aSubProcessModeldoes not require variables to be initialized with a specified data type or precision. This is because the data types and precisions\nof all`DenseLayer`DenseLayerProcessvariables (`proc.vars`proc.vars) are determined by the particularProcessModelschosen by the Run Configuration to implement the LIF and Dense childProcesses. This allows the sameSubProcessModelto be used flexibly across multiple languages and compute resources when the childProcesseshave multipleProcessModelimplementations.SubProcessModelsthus enable the composition of complex applications agnostic of platform-specific implementations. In this\nexample, we will implement the LIF and DenseProcesseswith thePyLoihiProcessModelsdefined earlier in the tutorial, so the`DenseLayer`DenseLayervariables aliased from LIF and Dense implicity have type`LavaPyType`LavaPyTypeand precisions as specified in`PyLifModel`PyLifModeland`PyDenseModel`PyDenseModel.\n\nLearn how to access memory from other Processes in the[Remote Memory Access Tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial07_remote_memory_access.html)Remote Memory Access Tutorial.\n\nIf you want to find out more aboutSubProcessModels, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/core/model/sub/model.py)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to[our newsletter](http://eepurl.com/hJCyhb)our newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n\n\nclass Dense(AbstractProcess):\n    \"\"\"Dense connections between neurons.\n    Realizes the following abstract behavior:\n    a_out = W * s_in\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        shape = kwargs.get(\"shape\", (1, 1))\n        self.s_in = InPort(shape=(shape[1],))\n        self.a_out = OutPort(shape=(shape[0],))\n        self.weights = Var(shape=shape, init=kwargs.pop(\"weights\", 0))\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\n\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n@implements(proc=Dense, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyDenseModel(PyLoihiProcessModel):\n    s_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, bool)\n    a_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)\n    weights: np.ndarray = LavaPyType(np.ndarray, float)\n\n    def run_spk(self):\n        s_in = self.s_in.recv()\n        a_out = self.weights[:, s_in].sum(axis=1)\n        self.a_out.send(a_out)\n``````\n\n``````\n[3]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import InPort, OutPort\n\n\nclass LIF(AbstractProcess):\n    \"\"\"Leaky-Integrate-and-Fire (LIF) neural Process.\n    LIF dynamics abstracts to:\n    u[t] = u[t-1] * (1-du) + a_in              # neuron current\n    v[t] = v[t-1] * (1-dv) + u[t] + bias_mant  # neuron voltage\n    s_out = v[t] > vth                         # spike if threshold is exceeded\n    v[t] = 0                                   # reset at spike\n    Parameters\n    ----------\n    du: Inverse of decay time-constant for current decay.\n    dv: Inverse of decay time-constant for voltage decay.\n    bias: Neuron bias.\n    vth: Neuron threshold voltage, exceeding which, the neuron will spike.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        shape = kwargs.get(\"shape\", (1,))\n        du = kwargs.pop(\"du\", 0)\n        dv = kwargs.pop(\"dv\", 0)\n        bias_mant = kwargs.pop(\"bias_mant\", 0)\n        vth = kwargs.pop(\"vth\", 10)\n\n        self.shape = shape\n        self.a_in = InPort(shape=shape)\n        self.s_out = OutPort(shape=shape)\n        self.u = Var(shape=shape, init=0)\n        self.v = Var(shape=shape, init=0)\n        self.du = Var(shape=(1,), init=du)\n        self.dv = Var(shape=(1,), init=dv)\n        self.bias_mant = Var(shape=shape, init=bias_mant)\n        self.vth = Var(shape=(1,), init=vth)\n``````\n\n``````\n[4]:\n``````\n\n``````\nimport numpy as np\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.ports import PyInPort, PyOutPort\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n@implements(proc=LIF, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyLifModel(PyLoihiProcessModel):\n    a_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float)\n    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1)\n    u: np.ndarray = LavaPyType(np.ndarray, float)\n    v: np.ndarray = LavaPyType(np.ndarray, float)\n    bias_mant: np.ndarray = LavaPyType(np.ndarray, float)\n    du: float = LavaPyType(float, float)\n    dv: float = LavaPyType(float, float)\n    vth: float = LavaPyType(float, float)\n\n    def run_spk(self):\n        a_in_data = self.a_in.recv()\n        self.u[:] = self.u * (1 - self.du)\n        self.u[:] += a_in_data\n        self.v[:] = self.v * (1 - self.dv) + self.u + self.bias_mant\n        s_out = self.v >= self.vth\n        self.v[s_out] = 0  # Reset voltage to 0\n        self.s_out.send(s_out)\n``````\n\n``````\n[5]:\n``````\n\n``````\nclass DenseLayer(AbstractProcess):\n    \"\"\"Combines Dense and LIF Processes.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        shape = kwargs.get(\"shape\", (1, 1))\n        du = kwargs.pop(\"du\", 0)\n        dv = kwargs.pop(\"dv\", 0)\n        bias_mant = kwargs.pop(\"bias_mant\", 0)\n        bias_exp = kwargs.pop(\"bias_exp\", 0)\n        vth = kwargs.pop(\"vth\", 10)\n        weights = kwargs.pop(\"weights\", 0)\n\n        self.s_in = InPort(shape=(shape[1],))\n        self.s_out = OutPort(shape=(shape[0],))\n\n        self.weights = Var(shape=shape, init=weights)\n        self.u = Var(shape=(shape[0],), init=0)\n        self.v = Var(shape=(shape[0],), init=0)\n        self.bias_mant = Var(shape=(shape[0],), init=bias_mant)\n        self.du = Var(shape=(1,), init=du)\n        self.dv = Var(shape=(1,), init=dv)\n        self.vth = Var(shape=(1,), init=vth)\n``````\n\n``````\n[6]:\n``````\n\n``````\nimport numpy as np\n\nfrom lava.proc.dense.process import Dense\nfrom lava.magma.core.model.sub.model import AbstractSubProcessModel\n\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.decorator import implements\n\n@implements(proc=DenseLayer, protocol=LoihiProtocol)\nclass SubDenseLayerModel(AbstractSubProcessModel):\n\n    def __init__(self, proc):\n        \"\"\"Builds sub Process structure of the Process.\"\"\"\n\n        # Instantiate child processes\n        # The input shape is a 2D vector (shape of the weight matrix).\n        shape = proc.proc_params.get(\"shape\", (1, 1))\n        weights = proc.proc_params.get(\"weights\", (1, 1))\n        bias_mant = proc.proc_params.get(\"bias_mant\", (1, 1))\n        vth = proc.proc_params.get(\"vth\", (1, 1))\n\n        self.dense = Dense(weights=weights)\n        self.lif = LIF(shape=(shape[0], ), bias_mant=bias_mant, vth=vth)\n\n        # Connect the parent InPort to the InPort of the Dense child-Process.\n        proc.in_ports.s_in.connect(self.dense.in_ports.s_in)\n\n        # Connect the OutPort of the Dense child-Process to the InPort of the\n        # LIF child-Process.\n        self.dense.out_ports.a_out.connect(self.lif.in_ports.a_in)\n\n        # Connect the OutPort of the LIF child-Process to the OutPort of the\n        # parent Process.\n        self.lif.out_ports.s_out.connect(proc.out_ports.s_out)\n\n        proc.vars.u.alias(self.lif.vars.u)\n        proc.vars.v.alias(self.lif.vars.v)\n        proc.vars.bias_mant.alias(self.lif.vars.bias_mant)\n        proc.vars.du.alias(self.lif.vars.du)\n        proc.vars.dv.alias(self.lif.vars.dv)\n        proc.vars.vth.alias(self.lif.vars.vth)\n        proc.vars.weights.alias(self.dense.vars.weights)\n``````\n\n``````\n[7]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import RunConfig, Loihi1SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.proc.io import sink, source\n\ndim = (3, 3)\n# Create the weight matrix.\nweights0 = np.zeros(shape=dim)\nweights0[1,1]=1\nweights1 = weights0\n# Instantiate two DenseLayers.\nlayer0 = DenseLayer(shape=dim, weights=weights0, bias_mant=4, vth=10)\nlayer1 = DenseLayer(shape=dim, weights=weights1, bias_mant=4, vth=10)\n# Connect the first DenseLayer to the second DenseLayer.\nlayer0.s_out.connect(layer1.s_in)\n\nprint('Layer 1 weights: \\n', layer1.weights.get(),'\\n')\nprint('\\n ----- \\n')\n\nrcfg = Loihi1SimCfg(select_tag='floating_pt', select_sub_proc_model=True)\n\nfor t in range(9):\n    # Run the entire network of Processes.\n    layer1.run(condition=RunSteps(num_steps=1), run_cfg=rcfg)\n    print('t: ',t)\n    print('Layer 0 v: ', layer0.v.get())\n    print('Layer 1 u: ', layer1.u.get())\n    print('Layer 1 v: ', layer1.v.get())\n    #print('Layer 1 spikes: ', layer1.spikes.get())\n    print('\\n ----- \\n')\n\nlayer1.stop()\n``````\n\n``````\nLayer 1 weights:\n [[0. 0. 0.]\n [0. 1. 0.]\n [0. 0. 0.]]\n\n\n -----\n\nt:  0\nLayer 0 v:  [4. 4. 4.]\nLayer 1 u:  [0. 0. 0.]\nLayer 1 v:  [4. 4. 4.]\n\n -----\n\nt:  1\nLayer 0 v:  [8. 8. 8.]\nLayer 1 u:  [0. 0. 0.]\nLayer 1 v:  [8. 8. 8.]\n\n -----\n\nt:  2\nLayer 0 v:  [0. 0. 0.]\nLayer 1 u:  [0. 0. 0.]\nLayer 1 v:  [0. 0. 0.]\n\n -----\n\nt:  3\nLayer 0 v:  [4. 4. 4.]\nLayer 1 u:  [0. 1. 0.]\nLayer 1 v:  [4. 5. 4.]\n\n -----\n\nt:  4\nLayer 0 v:  [8. 8. 8.]\nLayer 1 u:  [0. 1. 0.]\nLayer 1 v:  [8. 0. 8.]\n\n -----\n\nt:  5\nLayer 0 v:  [0. 0. 0.]\nLayer 1 u:  [0. 1. 0.]\nLayer 1 v:  [0. 5. 0.]\n\n -----\n\nt:  6\nLayer 0 v:  [4. 4. 4.]\nLayer 1 u:  [0. 2. 0.]\nLayer 1 v:  [4. 0. 4.]\n\n -----\n\nt:  7\nLayer 0 v:  [8. 8. 8.]\nLayer 1 u:  [0. 2. 0.]\nLayer 1 v:  [8. 6. 8.]\n\n -----\n\nt:  8\nLayer 0 v:  [0. 0. 0.]\nLayer 1 u:  [0. 2. 0.]\nLayer 1 v:  [0. 0. 0.]\n\n -----\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial07_remote_memory_access.html",
    "title": "Remote Memory Access — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nThe goal of this tutorial is to show how to enable remote memory access between processes using LavaRefPorts. In previous tutorials you have been introduced toProcesseswhich define behavior andProcessModelswhich implement the behavior for specific compute resources, e.g., CPU or Loihi Neurocores.\n\nIn general, processes have only access to its own state and communicate with the enviornment only through messages using ports. Lava also allows certain processes (e.g. those on CPUs) to perform remote memory access of internal states on other processes. Remote memory access between processes is potentially unsafe and should be used with care, but can be very useful in defined cases. One such case would be accessing (read/write) aVarof aProcesson a Loihi NeuroCore from anotherProcesson the embedded CPU.\n\nIn Lava, even remote memory access between Processes is realized via message-passing to remain true to the overall event-based message passing concept. The read/write is implemented via channels and message passing between processes and the remote process modifies its memory itself based on instructions from another process. However, as a convenience feature,RefPortsandVarPortssyntactically simplify the act of interacting with remote Vars.\n\nThus,RefPortsallow in Lava oneProcessto access the internalVarsof anotherProcess.RefPortsgive access to otherVarsas if it was an internalVar.\n\nIn this tutorial, we will create minimal Processes and ProcessModels to demonstrate reading and writing of Vars using RefPorts and VarPorts. Furthermore, we will explain the possibilities to connect RefPorts with VarPorts and Vars as well as the difference of explicitly and implicitly created VarPorts.\n\n[Installing Lava](https://lava-nc.org/lava/notebooks/in_depth/tutorial01_installing_lava.html)Installing Lava\n\n[Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial02_processes.html)Processes\n\n[ProcessModel](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel\n\n[Execution](https://lava-nc.org/lava/notebooks/in_depth/tutorial04_execution.html)Execution\n\n[Connecting Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial05_connect_processes.html)Connecting Processes\n\n[Hierarchical Processes](https://lava-nc.org/lava/notebooks/in_depth/tutorial06_hierarchical_processes.html)Hierarchical Processes\n\nThe[ProcessModel Tutorial](https://lava-nc.org/lava/notebooks/in_depth/tutorial03_process_models.html)ProcessModel Tutorialwalks through the creation ofProcessesand correspondingProcessModels. In order to demonstrate RefPorts we create a minimal process P1 with aRefPort`ref`refand a minimal process P2 with aVar`var`var.\n\nWe also create the correspondingProcessModelsPyProcModel1 and PyProcModel2 which implement the process P1 and P2. The value of theVarof P2`var`varis initialized with the value 5. The behavior we implement prints out the value of the`var`varin P1 every time step, demonstrating thereadability of aRefPort`ref`ref. Afterwards we set the value of`var`varby adding the current time step to it and write it with`ref`ref, demonstrating thewriteabiltity of aRefPort.\n\nTheRefPort`ref`refneeds to be connected with theVar`var`var, before execution. The expected output will be the initial value 5 of`var`varat the beginning, followed by 6 (5+1), 8 (6+2), 11 (8+3), 15 (11+4).\n\nIn the example above we demonstrated the read and write ability of aRefPortwhich used animplicitVarPortto connect to theVar. An implicitVarPortis created when`connect_var(..)`connect_var(..)is used to connect aRefPortwith aVar. ARefPortcan also be connected to aVarPortexplicitlydefined in aProcessusing`connect(..)`connect(..). In order to demonstrate explicitVarPortswe redefineProcessP2 and the correspondingProcessModel.\n\nThis time theRefPort`ref`refis connected to the explicitly definedVarPort`var_port`var_port. The output is the same as before.\n\nRefPortscan be connected in different ways toVarsandVarPorts.RefPortsandVarPortscan also be connected to themselves in case of hierarchical processes.\n\nRefPortscan be connected toRefPortsorVarPortsusing`connect(..)`connect(..)\n\nRefPortscan be connected toVarsusing`connect_var(..)`connect_var(..)\n\nRefPortscan receive connections fromRefPortsusing`connect_from(..)`connect_from(..)\n\nVarPortscan be connected toVarPortsusing`connect(..)`connect(..)\n\nVarPortscan receive connections fromVarPortsorRefPortsusing`connect_from(..)`connect_from(..)\n\nIf you want to find out more aboutRemote Memory Access, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/tree/main/src/lava/magma/core/process/ports/ports.py)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to[our newsletter](http://eepurl.com/hJCyhb)our newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.process.process import AbstractProcess\nfrom lava.magma.core.process.variable import Var\nfrom lava.magma.core.process.ports.ports import RefPort\n\n\n# A minimal process with a Var and a RefPort\nclass P1(AbstractProcess):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.ref = RefPort(shape=(1,))\n\n\n# A minimal process with a Var\nclass P2(AbstractProcess):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.var = Var(shape=(1,), init=5)\n``````\n\n``````\n[2]:\n``````\n\n``````\nimport numpy as np\n\nfrom lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\nfrom lava.magma.core.model.py.ports import PyRefPort\nfrom lava.magma.core.model.py.type import LavaPyType\nfrom lava.magma.core.resources import CPU\nfrom lava.magma.core.decorator import implements, requires\nfrom lava.magma.core.model.py.model import PyLoihiProcessModel\n\n\n# A minimal PyProcModel implementing P1\n@implements(proc=P1, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyProcModel1(PyLoihiProcessModel):\n    ref: PyRefPort = LavaPyType(PyRefPort.VEC_DENSE, int)\n\n    def post_guard(self):\n        return True\n\n    def run_post_mgmt(self):\n        # Retrieve current value of the Var of P2\n        cur_val = self.ref.read()\n        print(\"Value of var: {} at time step: {}\".format(cur_val, self.time_step))\n\n        # Add the current time step to the current value\n        new_data = cur_val + self.time_step\n        # Write the new value to the Var of P2\n        self.ref.write(new_data)\n\n\n# A minimal PyProcModel implementing P2\n@implements(proc=P2, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyProcModel2(PyLoihiProcessModel):\n    var: np.ndarray = LavaPyType(np.ndarray, np.int32)\n``````\n\n``````\n[3]:\n``````\n\n``````\nfrom lava.magma.core.run_configs import Loihi1SimCfg\nfrom lava.magma.core.run_conditions import RunSteps\n\n# Create process P1 and P2\nproc1 = P1()\nproc2 = P2()\n\n# Connect RefPort 'ref' of P1 with Var 'var' of P2 using an implicit VarPort\nproc1.ref.connect_var(proc2.var)\n\n# Run the network for 5 time steps\nproc1.run(condition=RunSteps(num_steps=5), run_cfg=Loihi1SimCfg())\nproc1.stop()\n``````\n\n``````\nValue of var: [5] at time step: 1\nValue of var: [6] at time step: 2\nValue of var: [8] at time step: 3\nValue of var: [11] at time step: 4\nValue of var: [15] at time step: 5\n``````\n\n``````\n[4]:\n``````\n\n``````\nfrom lava.magma.core.process.ports.ports import VarPort\n\n# A minimal process with a Var and an explicit VarPort\nclass P2(AbstractProcess):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.var = Var(shape=(1,), init=5)\n        self.var_port = VarPort(self.var)\n``````\n\n``````\n[5]:\n``````\n\n``````\nfrom lava.magma.core.model.py.ports import PyVarPort\n\n# A minimal PyProcModel implementing P2\n@implements(proc=P2, protocol=LoihiProtocol)\n@requires(CPU)\nclass PyProcModel2(PyLoihiProcessModel):\n    var: np.ndarray = LavaPyType(np.ndarray, np.int32)\n    var_port: PyVarPort = LavaPyType(PyVarPort.VEC_DENSE, int)\n``````\n\n``````\n[6]:\n``````\n\n``````\n# Create process P1 and P2\nproc1 = P1()\nproc2 = P2()\n\n# Connect RefPort 'ref' of P1 with VarPort 'var_port' of P2\nproc1.ref.connect(proc2.var_port)\n\n# Run the network for 5 time steps\nproc1.run(condition=RunSteps(num_steps=5), run_cfg=Loihi1SimCfg())\nproc1.stop()\n``````\n\n``````\nValue of var: [5] at time step: 1\nValue of var: [6] at time step: 2\nValue of var: [8] at time step: 3\nValue of var: [11] at time step: 4\nValue of var: [15] at time step: 5\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial08_stdp.html",
    "title": "Spike-timing Dependent Plasticity (STDP) — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nMotivation: In this tutorial, we will demonstrate usage of a software model of Loihi’s learning engine, exposed in Lava. This involves the LearningRule object for learning rule and other learning-related information encapsulation and the LearningDense Lava Process modelling learning-enabled connections.\n\nhave the[Lava framework installed](https://lava-nc.org/lava/in_depth/tutorial01_installing_lava.ipynb)Lava framework installed\n\nare familiar with the[Process concept in Lava](https://lava-nc.org/lava/in_depth/tutorial02_processes.ipynb)Process concept in Lava\n\nare familiar with the[ProcessModel concept in Lava](https://lava-nc.org/lava/in_depth/tutorial02_process_models.ipynb)ProcessModel concept in Lava\n\nare familiar with how to[connect Lava Processes](https://lava-nc.org/lava/in_depth/tutorial05_connect_processes.ipynb)connect Lava Processes\n\nThis tutorial gives a bird’s-eye view of how to make use of the available learning rules in Lavas Process Library. For this purpose, we will create a network of LIF and Dense processes with one plastic connection and generate frozen patterns of activity. We can easily choose between a floating point simulation of the learning engine and a fixed point simulation, which approximates the behavior on the Loihi neuromorphic hardware. We also will create monitors to observe the behavior of the weights\nand activity traces of the neurons and learning rules.\n\nLet’s first generate the random, frozen input and define all parameters for the network.\n\nNext, lets instatiate the STDP learning rule from the Lava Process Library. The STDPLoihi learning rule provides the parameters as described in Gerstner and al. 1996 (see also[http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity)http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity).\n\nThe following diagram depics the Lava Process architecture used in this tutorial. It consists of: - 2 Constant pattern generators for injection spike trains to LIF neurons. - 2LIFProcesses representing pre- and post-synaptic Leaky Integrate-and-Fire neurons. - 1DenseProcess representing learning-enable connection between LIF neurons.\n\nNote:All neuronal population (spike generator, LIF) are composed of only 1 neuron in this tutorial.\n\nWe now instantiate our plastic Dense process. The Dense Process provides the following Vars and Ports relevant for plasticity:\n\nComponent\n\nName\n\nDescription\n\nInPort\n\n`s_in_bap`s_in_bap\n\nReceives spikes from post-synaptic neurons.\n\nVar\n\n`tag_2`tag_2\n\nDelay synaptic variable.\n\n`tag_1`tag_1\n\nTag synaptic variable.\n\n`x0`x0\n\nState ofx_0dependency.\n\n`tx`tx\n\nWithin-epoch spike times of pre-synaptic neurons.\n\n`x1`x1\n\nState ofx_1trace.\n\n`x2`x2\n\nState ofx_2trace.\n\n`y0`y0\n\nState ofy_0dependency.\n\n`ty`ty\n\nWithin-epoch spike times of post-synaptic neurons.\n\n`y1`y1\n\nState ofy_1trace.\n\n`y2`y2\n\nState ofy_2trace.\n\n`y3`y3\n\nState ofy_3trace.\n\nNow, we can take a look at the results of the simulation.\n\nAs can be seen, the actual weight changes follow the defined STDP with a certain amout of noise. If the tag is set to`fixed_pt`fixed_pt, the weight changes get more quantized, but still follow the correct trend.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[130]:\n``````\n\n``````\nimport numpy as np\n\n# Set this tag to \"fixed_pt\" or \"floating_pt\" to choose the corresponding models.\nSELECT_TAG = \"floating_pt\"\n\n# LIF parameters\nif SELECT_TAG == \"fixed_pt\":\n    du = 4095\n    dv = 4095\nelif SELECT_TAG == \"floating_pt\":\n    du = 1\n    dv = 1\nvth = 240\n\n# Number of neurons per layer\nnum_neurons = 1\nshape_lif = (num_neurons, )\nshape_conn = (num_neurons, num_neurons)\n\n# Connection parameters\n\n# SpikePattern -> LIF connection weight\nwgt_inp = np.eye(num_neurons) * 250\n\n# LIF -> LIF connection initial weight (learning-enabled)\nwgt_plast_conn = np.full(shape_conn, 50)\n\n# Number of simulation time steps\nnum_steps = 200\ntime = list(range(1, num_steps + 1))\n\n# Spike times\nspike_prob = 0.03\n\n# Create spike rasters\nnp.random.seed(123)\nspike_raster_pre = np.zeros((num_neurons, num_steps))\nnp.place(spike_raster_pre, np.random.rand(num_neurons, num_steps) < spike_prob, 1)\n\nspike_raster_post = np.zeros((num_neurons, num_steps))\nnp.place(spike_raster_post, np.random.rand(num_neurons, num_steps) < spike_prob, 1)\n``````\n\n``````\n[131]:\n``````\n\n``````\nfrom lava.proc.learning_rules.stdp_learning_rule import STDPLoihi\n``````\n\n``````\n[132]:\n``````\n\n``````\nstdp = STDPLoihi(learning_rate=1,\n                 A_plus=1,\n                 A_minus=-1,\n                 tau_plus=10,\n                 tau_minus=10,\n                 t_epoch=4)\n``````\n\n``````\n[133]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.io.source import RingBuffer\nfrom lava.proc.dense.process import LearningDense, Dense\n``````\n\n``````\n[134]:\n``````\n\n``````\n# Create input devices\npattern_pre = RingBuffer(data=spike_raster_pre.astype(int))\npattern_post = RingBuffer(data=spike_raster_post.astype(int))\n\n# Create input connectivity\nconn_inp_pre = Dense(weights=wgt_inp)\nconn_inp_post = Dense(weights=wgt_inp)\n\n# Create pre-synaptic neurons\nlif_pre = LIF(u=0,\n              v=0,\n              du=du,\n              dv=dv,\n              bias_mant=0,\n              bias_exp=0,\n              vth=vth,\n              shape=shape_lif,\n              name='lif_pre')\n\n# Create plastic connection\nplast_conn = LearningDense(weights=wgt_plast_conn,\n                           learning_rule=stdp,\n                           name='plastic_dense')\n\n# Create post-synaptic neuron\nlif_post = LIF(u=0,\n               v=0,\n               du=du,\n               dv=dv,\n               bias_mant=0,\n               bias_exp=0,\n               vth=vth,\n               shape=shape_lif,\n               name='lif_post')\n\n# Connect network\npattern_pre.s_out.connect(conn_inp_pre.s_in)\nconn_inp_pre.a_out.connect(lif_pre.a_in)\n\npattern_post.s_out.connect(conn_inp_post.s_in)\nconn_inp_post.a_out.connect(lif_post.a_in)\n\nlif_pre.s_out.connect(plast_conn.s_in)\nplast_conn.a_out.connect(lif_post.a_in)\n\n# Connect back-propagating action potential (BAP)\nlif_post.s_out.connect(plast_conn.s_in_bap)\n``````\n\n``````\n[135]:\n``````\n\n``````\nfrom lava.proc.monitor.process import Monitor\n``````\n\n``````\n[136]:\n``````\n\n``````\n# Create monitors\nmon_pre_trace = Monitor()\nmon_post_trace = Monitor()\nmon_pre_spikes = Monitor()\nmon_post_spikes = Monitor()\nmon_weight = Monitor()\n\n# Connect monitors\nmon_pre_trace.probe(plast_conn.x1, num_steps)\nmon_post_trace.probe(plast_conn.y1, num_steps)\nmon_pre_spikes.probe(lif_pre.s_out, num_steps)\nmon_post_spikes.probe(lif_post.s_out, num_steps)\nmon_weight.probe(plast_conn.weights, num_steps)\n``````\n\n``````\n[137]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi2SimCfg\n``````\n\n``````\n[138]:\n``````\n\n``````\n# Running\npattern_pre.run(condition=RunSteps(num_steps=num_steps), run_cfg=Loihi2SimCfg(select_tag=SELECT_TAG))\n``````\n\n``````\n[139]:\n``````\n\n``````\n# Get data from monitors\npre_trace = mon_pre_trace.get_data()['plastic_dense']['x1']\npost_trace = mon_post_trace.get_data()['plastic_dense']['y1']\npre_spikes = mon_pre_spikes.get_data()['lif_pre']['s_out']\npost_spikes = mon_post_spikes.get_data()['lif_post']['s_out']\nweights = mon_weight.get_data()['plastic_dense']['weights'][:, :, 0]\n``````\n\n``````\n[140]:\n``````\n\n``````\n# Stopping\npattern_pre.stop()\n``````\n\n``````\n[141]:\n``````\n\n``````\nimport matplotlib.pyplot as plt\n``````\n\n``````\n[142]:\n``````\n\n``````\n# Plotting pre- and post- spike arrival\ndef plot_spikes(spikes, legend, colors):\n    offsets = list(range(1, len(spikes) + 1))\n\n    plt.figure(figsize=(10, 3))\n\n    spikes_plot = plt.eventplot(positions=spikes,\n                                lineoffsets=offsets,\n                                linelength=0.9,\n                                colors=colors)\n\n    plt.title(\"Spike arrival\")\n    plt.xlabel(\"Time steps\")\n    plt.ylabel(\"Neurons\")\n    plt.yticks(ticks=offsets, labels=legend)\n\n    plt.show()\n\n# Plot spikes\nplot_spikes(spikes=[np.where(post_spikes[:, 0])[0], np.where(pre_spikes[:, 0])[0]],\n            legend=['Post', 'Pre'],\n            colors=['#370665', '#f14a16'])\n``````\n\n``````\n[143]:\n``````\n\n``````\n# Plotting trace dynamics\n\ndef plot_time_series(time, time_series, ylabel, title):\n    plt.figure(figsize=(10, 1))\n\n    plt.step(time, time_series)\n\n    plt.title(title)\n    plt.xlabel(\"Time steps\")\n    plt.ylabel(ylabel)\n\n    plt.show()\n\n# Plotting pre trace dynamics\nplot_time_series(time=time, time_series=pre_trace, ylabel=\"Trace value\", title=\"Pre trace\")\n# Plotting post trace dynamics\nplot_time_series(time=time, time_series=post_trace, ylabel=\"Trace value\", title=\"Post trace\")\n# Plotting weight dynamics\nplot_time_series(time=time, time_series=weights, ylabel=\"Weight value\", title=\"Weight dynamics\")\n``````\n\n``````\n[144]:\n``````\n\n``````\ndef extract_stdp_weight_changes(time, spikes_pre, spikes_post, wgt):\n    # Compute the weight changes for every weight change event\n    w_diff = np.zeros(wgt.shape)\n    w_diff[1:] = np.diff(wgt)\n\n    w_diff_non_zero = np.where(w_diff != 0)\n    dw = w_diff[w_diff_non_zero].tolist()\n\n    # Find the absolute time of every weight change event\n    time = np.array(time)\n    t_non_zero = time[w_diff_non_zero]\n\n    # Compute the difference between post and pre synaptic spike time for every weight change event\n    spikes_pre = np.array(spikes_pre)\n    spikes_post = np.array(spikes_post)\n    dt = []\n    for i in range(0, len(dw)):\n        time_stamp = t_non_zero[i]\n        t_post = (spikes_post[np.where(spikes_post <= time_stamp)])[-1]\n        t_pre = (spikes_pre[np.where(spikes_pre <= time_stamp)])[-1]\n        dt.append(t_post-t_pre)\n\n    return np.array(dt), np.array(dw)\n\ndef plot_stdp(time, spikes_pre, spikes_post, wgt,\n              on_pre_stdp, y1_impulse, y1_tau,\n              on_post_stdp, x1_impulse, x1_tau):\n    # Derive weight changes as a function of time differences\n    diff_t, diff_w = extract_stdp_weight_changes(time, spikes_pre, spikes_post, wgt)\n\n    # Derive learning rule coefficients\n    on_pre_stdp = eval(str(on_pre_stdp).replace(\"^\", \"**\"))\n    a_neg = on_pre_stdp * y1_impulse\n    on_post_stdp = eval(str(on_post_stdp).replace(\"^\", \"**\"))\n    a_pos = on_post_stdp * x1_impulse\n\n    # Derive x-axis limit (absolute value)\n    max_abs_dt = np.maximum(np.abs(np.max(diff_t)), np.abs(np.min(diff_t)))\n\n    # Derive x-axis for learning window computation (negative part)\n    x_neg = np.linspace(-max_abs_dt, 0, 1000)\n    # Derive learning window (negative part)\n    w_neg = a_neg * np.exp(x_neg / y1_tau)\n\n    # Derive x-axis for learning window computation (positive part)\n    x_pos = np.linspace(0, max_abs_dt, 1000)\n    # Derive learning window (positive part)\n    w_pos = a_pos * np.exp(- x_pos / x1_tau)\n\n    plt.figure(figsize=(10, 5))\n\n    plt.scatter(diff_t, diff_w, label=\"Weight changes\", color=\"b\")\n\n    plt.plot(x_neg, w_neg, label=\"W-\", color=\"r\")\n    plt.plot(x_pos, w_pos, label=\"W+\", color=\"g\")\n\n    plt.title(\"STDP weight changes - Learning window\")\n    plt.xlabel('t_post - t_pre')\n    plt.ylabel('Weight change')\n    plt.legend()\n    plt.grid()\n\n    plt.show()\n\n# Plot STDP window\nplot_stdp(time, np.where(pre_spikes[:, 0]), np.where(post_spikes[:, 0]), weights[:, 0],\n          stdp.A_minus, stdp.y1_impulse, stdp.tau_minus,\n          stdp.A_plus, stdp.x1_impulse, stdp.tau_plus)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava/notebooks/in_depth/tutorial09_custom_learning_rules.html",
    "title": "Custom Learning Rules — Lava  documentation",
    "content": "Copyright (C) 2021 Intel CorporationSPDX-License-Identifier: BSD-3-ClauseSee: https://spdx.org/licenses/\n\nMotivation: In this tutorial, we will demonstrate usage of a software model of Loihi’s learning engine, exposed in Lava. This involves the LearningRule object for learning rule and other learning-related information encapsulation and the LearningDense Lava Process modelling learning-enabled connections.\n\nhave the[Lava framework installed](https://lava-nc.org/lava/in_depth/tutorial01_installing_lava.ipynb)Lava framework installed\n\nare familiar with the[Process concept in Lava](https://lava-nc.org/lava/in_depth/tutorial02_processes.ipynb)Process concept in Lava\n\nare familiar with the[ProcessModel concept in Lava](https://lava-nc.org/lava/in_depth/tutorial02_process_models.ipynb)ProcessModel concept in Lava\n\nare familiar with how to[connect Lava Processes](https://lava-nc.org/lava/in_depth/tutorial05_connect_processes.ipynb)connect Lava Processes\n\nThis tutorial gives a bird’s-eye view of how to develop custom learning rules for Loihi. For this purpose, we will create a network of LIF and Dense processes with one plastic connection and generate frozen patterns of activity. We can easily choose between a floating point simulation of the learning engine and a fixed point simulation, which approximates the behavior on the Loihi neuromorphic hardware. We also will create monitors to observe the behavior of the weights and activity traces of\nthe neurons and learning rules.\n\nLoihi provides a programmable learning engine that can evolve synaptic state variables over time as a function of several locally available parameters and the equations relating input terms to output synaptic target variables are calledlearning rules. These learning rule equations are highly configurable but are constrained to a sum of products form.\n\nFor efficiency reasons, trace and synaptic variable updates proceed in learning epochs with a length oft_{epoch}time steps. Within an epoch, spike events are recorded but trace and synaptic variable updates are only computed and applied with a slight delay at the end of the epoch. This delayed application will theoretically not have any impact as long as there is only one spike per synapse and per epoch.\n\nFor each synapse, Loihi’s computational model defines a set of three synaptic state variables that can be modified by the learning engine. These are : - WeightW, representingsynaptic efficacy. - DelayD, representingsynaptic delay. - TagT, which is an additional synaptic variable that allows for constructing more complex learning dynamics.\n\nThe amount of change by which a target synaptic variable is updated at the end of a learning epoch is given by the learning rule associated with said variable. The rules are specified in sum-of-products form:\n\nZ \\in \\{W, D, T\\}\n\ndZ = \\sum_{i = 1}^{N_P} D_i \\cdot \\left[ S_i \\cdot \\prod_{j = 1}^{N_F^i} F_{i, j} \\right]\n\nThe learning rule consists in asumofN_Pproducts. Eachi’th product is composed of a dependency operatorD_i, a scaling factorS_iand a sub-product ofN_F^ifactors withF_{i, j}denoting thej’th factor of the current product.\n\nEach product is associated with adependencyoperatorD_ithat conditions the evaluation of a product on the presence of a pre- or post-synaptic spike during the past epoch or evaluates a product unconditionally every other epoch.D_ialso determines at what time step during an epoch, all trace variables in the associated product are evaluated. The table below lists the various dependency operators and their behavior:\n\nDependency\n\nt_{eval}\n\nDescription\n\nx_0\n\nt_x\n\nConditioned on at least one pre-synaptic spike during epoch.\n\ny_0\n\nt_y\n\nConditioned on at least one post-synaptic spike during epoch.\n\nu_{\\kappa}\n\nt_{epoch}\n\nUnconditionally executed every\\kappa \\cdot t_{epoch}time steps.\n\nEach product is also associated with ascaling factor(constant literal) that is given in mantissa/exponent form :\n\nS_i = S_i^{mant} \\cdot 2^{S_i^{exp}}\n\nFurthermore, Loihi provides a set of locally available quantities which can be used in learning rule to derive synaptic variable updates. The table below lists the various types of variables whose valueF_{i, j}can assume:\n\nFactor\n\nDescription\n\nx_0 + C\n\nPre-synaptic spike.\n\nx_1(t_{eval}) + C\n\nPre-synaptic tracex_1.\n\nx_2(t_{eval}) + C\n\nPre-synaptic tracex_2.\n\ny_0 + C\n\nPost-synaptic spike.\n\ny_1(t_{eval}) + C\n\nPost-synaptic tracey_1.\n\ny_2(t_{eval}) + C\n\nPost-synaptic tracey_2.\n\ny_3(t_{eval}) + C\n\nPost-synaptic tracey_3.\n\nW + C\n\nWeight synaptic variableW.\n\nD + C\n\nDelay synaptic variableD.\n\nT + C\n\nTag synaptic variableT.\n\nsgn(W + C)\n\nSign ofW.\n\nsgn(D + C)\n\nSign ofD.\n\nsgn(T + C)\n\nSign ofT.\n\nC\n\nConstant term(variant 1).\n\nC^{mant} \\cdot 2^{C^{exp}}\n\nConstant term(variant 2).\n\nTraces are low-pass filtered versions of spike train that are typically used in online implementations of[Spike-Timing Dependent Plasticity (STDP)](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity)Spike-Timing Dependent Plasticity (STDP). For each synapse, Loihi provides a set of2 pre-synaptic traces\\{x_1, x_2\\}and3 post-synaptic traces\\{y_1, y_2, y_3\\}. The dynamics of an ideal spike trace is given by :\n\nz \\in \\{x_1, x_2, y_1, y_2, y_3\\}\n\nz(t) = z(t_{k-1}) \\cdot exp(- \\frac{t-t_{k-1}}{\\tau^z}) + \\xi^z \\cdot \\delta^{z}(t - t_k)\n\nHere, the set\\{t_k\\}are successive spike times at which the trace accumulates the spike impulse value\\xi^{z}while\\tau^zgoverns the speed of exponential decay between spike events. Finally,\\delta^zdenotes the raw spike train associated with the tracez.\n\ndW = S_1 \\cdot x_0 \\cdot y_1 + S_2 \\cdot y_0 \\cdot x_1\n\nwhereS_1 < 0andS_2 > 0.\n\nNext, we define a learning rule (dw) for theweightsynaptic variable. The learning rule is first written in string format and passed to the LearningRule object as instantiation argument. This string learning rule will get internally parsed, transformed into and stored as aProductSeries, which is a custom data structure that is particularly well-suited for sum-of-products representation.\n\nHere, we use the basic pair-based STDP learning rule defined by :\n\ndw = -2 \\cdot x_0 \\cdot y_1 + 2 \\cdot y_0 \\cdot x_1\n\nAs a reminder, the main function of the LearningRule object is not only to encapsulate learning rules, but also other learning-related such as trace impulse values and decay constants for all of the traces as well as the length of the learning epoch. The following table lists the different fields of the LearningRule class:\n\nField\n\nPython type\n\nDescription\n\n`dw`dw\n\nProductSeries\n\nLearning rule targetting the synaptic variableW.\n\n`dd`dd\n\nProductSeries\n\nLearning rule targetting the synaptic variableD.\n\n`dt`dt\n\nProductSeries\n\nLearning rule targetting the synaptic variableT.\n\n`x1_impulse`x1_impulse\n\nfloat\n\nTrace impulse value associated withx_1trace.\n\n`x1_tau`x1_tau\n\nint\n\nTrace decay constant associated withx_1trace.\n\n`x2_impulse`x2_impulse\n\nfloat\n\nTrace impulse value associated withx_2trace.\n\n`x2_tau`x2_tau\n\nint\n\nTrace decay constant associated withx_2trace.\n\n`y1_impulse`y1_impulse\n\nfloat\n\nTrace impulse value associated withy_1trace.\n\n`y1_tau`y1_tau\n\nint\n\nTrace decay constant associated withy_1trace.\n\n`y2_impulse`y2_impulse\n\nfloat\n\nTrace impulse value associated withy_2trace.\n\n`y2_tau`y2_tau\n\nint\n\nTrace decay constant associated withy_2trace.\n\n`y3_impulse`y3_impulse\n\nfloat\n\nTrace impulse value associated withy_3trace.\n\n`y3_tau`y3_tau\n\nint\n\nTrace decay constant associated withy_3trace.\n\n`t_epoch`t_epoch\n\nint\n\nLearning epoch length.\n\nThe following diagram depics the Lava Process architecture used in this tutorial. It consists of: - 2 Constant pattern generators for injection spike trains to LIF neurons. - 2LIFProcesses representing pre- and post-synaptic Leaky Integrate-and-Fire neurons. - 1DenseProcess representing learning-enable connection between LIF neurons.\n\nNote:All neuronal population (spike generator, LIF) are composed of only 1 neuron in this tutorial.\n\nWe now instantiate our plastic Dense process. The Dense Process provides the following Vars and Ports relevant for plasticity:\n\nComponent\n\nName\n\nDescription\n\nInPort\n\n`s_in_bap`s_in_bap\n\nReceives spikes from post-synaptic neurons.\n\nVar\n\n`tag_2`tag_2\n\nDelay synaptic variable.\n\n`tag_1`tag_1\n\nTag synaptic variable.\n\n`x0`x0\n\nState ofx_0dependency.\n\n`tx`tx\n\nWithin-epoch spike times of pre-synaptic neurons.\n\n`x1`x1\n\nState ofx_1trace.\n\n`x2`x2\n\nState ofx_2trace.\n\n`y0`y0\n\nState ofy_0dependency.\n\n`ty`ty\n\nWithin-epoch spike times of post-synaptic neurons.\n\n`y1`y1\n\nState ofy_1trace.\n\n`y2`y2\n\nState ofy_2trace.\n\n`y3`y3\n\nState ofy_3trace.\n\nNow, we can take a look at the results of the simulation.\n\nFind out how to use STDP from the Lava ProcessLibrary in the[STDP Tutorial](https://lava-nc.org/lava/in_depth/tutorial09_custom_learning_rules.ipynb)STDP Tutorial.\n\n[Processes](https://lava-nc.org/lava/in_depth/tutorial02_processes.ipynb)Processes\n\n[ProcessModel](https://lava-nc.org/lava/in_depth/tutorial03_process_models.ipynb)ProcessModel\n\n[Execution](https://lava-nc.org/lava/in_depth/tutorial04_execution.ipynb)Execution\n\n[Connecting Processes](https://lava-nc.org/lava/in_depth/tutorial05_connect_processes.ipynb)Connecting Processes\n\nIf you want to find out more about Lava, have a look at the[Lava documentation](https://lava-nc.org/)Lava documentationor dive into the[source code](https://github.com/lava-nc/lava/)source code.\n\nTo receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the[INRC newsletter](http://eepurl.com/hJCyhb)INRC newsletter.\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\n[1]:\n``````\n\n``````\nfrom lava.magma.core.learning.learning_rule import Loihi2FLearningRule\n\n# Learning rule coefficient\non_pre_stdp = -2\non_post_stdp = 2\n\nlearning_rate = 1\n\n# Trace decay constants\nx1_tau = 10\ny1_tau = 10\n\n# Impulses\nx1_impulse = 16\ny1_impulse = 16\n\n# Epoch length\nt_epoch = 2\n\n# Define dw as string\ndw = f\"{learning_rate} * ({on_pre_stdp}) * x0 * y1 +\" \\\n     f\"{learning_rate} * {on_post_stdp} * y0 * x1\"\n``````\n\n``````\n[2]:\n``````\n\n``````\n# Create custom LearningRule\nstdp = Loihi2FLearningRule(dw=dw,\n                           x1_impulse=x1_impulse,\n                           x1_tau=x1_tau,\n                           y1_impulse=y1_impulse,\n                           y1_tau=y1_tau,\n                           t_epoch=t_epoch)\n``````\n\n``````\n[3]:\n``````\n\n``````\nimport numpy as np\n\n# Set this tag to \"fixed_pt\" or \"floating_pt\" to choose the corresponding models.\nSELECT_TAG = \"fixed_pt\"\n\n# LIF parameters\nif SELECT_TAG == \"fixed_pt\":\n    du = 4095\n    dv = 4095\nelif SELECT_TAG == \"floating_pt\":\n    du = 1\n    dv = 1\nvth = 240\n\n# Number of neurons per layer\nnum_neurons = 1\nshape_lif = (num_neurons, )\nshape_conn = (num_neurons, num_neurons)\n\n# Connection parameters\n\n# SpikePattern -> LIF connection weight\nwgt_inp = np.eye(num_neurons) * 250\n\n# LIF -> LIF connection initial weight (learning-enabled)\nwgt_plast_conn = np.full(shape_conn, 50)\n\n# Number of simulation time steps\nnum_steps = 200\ntime = list(range(1, num_steps + 1))\n\n# Spike times\nspike_prob = 0.03\n\n# Create spike rasters\nnp.random.seed(123)\nspike_raster_pre = np.zeros((num_neurons, num_steps))\nnp.place(spike_raster_pre, np.random.rand(num_neurons, num_steps) < spike_prob, 1)\n\nspike_raster_post = np.zeros((num_neurons, num_steps))\nnp.place(spike_raster_post, np.random.rand(num_neurons, num_steps) < spike_prob, 1)\n``````\n\n``````\n[4]:\n``````\n\n``````\nfrom lava.proc.lif.process import LIF\nfrom lava.proc.io.source import RingBuffer\nfrom lava.proc.dense.process import LearningDense, Dense\n``````\n\n``````\n[5]:\n``````\n\n``````\n# Create input devices\npattern_pre = RingBuffer(data=spike_raster_pre.astype(int))\npattern_post = RingBuffer(data=spike_raster_post.astype(int))\n\n# Create input connectivity\nconn_inp_pre = Dense(weights=wgt_inp)\nconn_inp_post = Dense(weights=wgt_inp)\n\n# Create pre-synaptic neurons\nlif_pre = LIF(u=0,\n              v=0,\n              du=du,\n              dv=du,\n              bias_mant=0,\n              bias_exp=0,\n              vth=vth,\n              shape=shape_lif,\n              name='lif_pre')\n\n# Create plastic connection\nplast_conn = LearningDense(weights=wgt_plast_conn,\n                           learning_rule=stdp,\n                           name='plastic_dense')\n\n# Create post-synaptic neuron\nlif_post = LIF(u=0,\n               v=0,\n               du=du,\n               dv=du,\n               bias_mant=0,\n               bias_exp=0,\n               vth=vth,\n               shape=shape_lif,\n               name='lif_post')\n\n# Connect network\npattern_pre.s_out.connect(conn_inp_pre.s_in)\nconn_inp_pre.a_out.connect(lif_pre.a_in)\n\npattern_post.s_out.connect(conn_inp_post.s_in)\nconn_inp_post.a_out.connect(lif_post.a_in)\n\nlif_pre.s_out.connect(plast_conn.s_in)\nplast_conn.a_out.connect(lif_post.a_in)\n\n# Connect back-propagating actionpotential (BAP)\nlif_post.s_out.connect(plast_conn.s_in_bap)\n``````\n\n``````\n[6]:\n``````\n\n``````\nfrom lava.proc.monitor.process import Monitor\n``````\n\n``````\n[7]:\n``````\n\n``````\n# Create monitors\nmon_pre_trace = Monitor()\nmon_post_trace = Monitor()\nmon_pre_spikes = Monitor()\nmon_post_spikes = Monitor()\nmon_weight = Monitor()\n\n# Connect monitors\nmon_pre_trace.probe(plast_conn.x1, num_steps)\nmon_post_trace.probe(plast_conn.y1, num_steps)\nmon_pre_spikes.probe(lif_pre.s_out, num_steps)\nmon_post_spikes.probe(lif_post.s_out, num_steps)\nmon_weight.probe(plast_conn.weights, num_steps)\n``````\n\n``````\n[8]:\n``````\n\n``````\nfrom lava.magma.core.run_conditions import RunSteps\nfrom lava.magma.core.run_configs import Loihi2SimCfg\n``````\n\n``````\n[9]:\n``````\n\n``````\n# Running\npattern_pre.run(condition=RunSteps(num_steps=num_steps), run_cfg=Loihi2SimCfg(select_tag=SELECT_TAG))\n``````\n\n``````\n[10]:\n``````\n\n``````\n# Get data from monitors\npre_trace = mon_pre_trace.get_data()['plastic_dense']['x1']\npost_trace = mon_post_trace.get_data()['plastic_dense']['y1']\npre_spikes = mon_pre_spikes.get_data()['lif_pre']['s_out']\npost_spikes = mon_post_spikes.get_data()['lif_post']['s_out']\nweights = mon_weight.get_data()['plastic_dense']['weights'][:, :, 0]\n``````\n\n``````\n[11]:\n``````\n\n``````\n# Stopping\npattern_pre.stop()\n``````\n\n``````\n[12]:\n``````\n\n``````\nimport matplotlib.pyplot as plt\n``````\n\n``````\n[13]:\n``````\n\n``````\n# Plotting pre- and post- spike arrival\ndef plot_spikes(spikes, legend, colors):\n    offsets = list(range(1, len(spikes) + 1))\n\n    plt.figure(figsize=(10, 3))\n\n    spikes_plot = plt.eventplot(positions=spikes,\n                                lineoffsets=offsets,\n                                linelength=0.9,\n                                colors=colors)\n\n    plt.title(\"Spike arrival\")\n    plt.xlabel(\"Time steps\")\n    plt.ylabel(\"Neurons\")\n    plt.yticks(ticks=offsets, labels=legend)\n\n    plt.show()\n\n# Plot spikes\nplot_spikes(spikes=[np.where(post_spikes[:, 0])[0], np.where(pre_spikes[:, 0])[0]],\n            legend=['Post', 'Pre'],\n            colors=['#370665', '#f14a16'])\n``````\n\n``````\n[14]:\n``````\n\n``````\n# Plotting trace dynamics\n\ndef plot_time_series(time, time_series, ylabel, title):\n    plt.figure(figsize=(10, 1))\n\n    plt.step(time, time_series)\n\n    plt.title(title)\n    plt.xlabel(\"Time steps\")\n    plt.ylabel(ylabel)\n\n    plt.show()\n\n# Plotting pre trace dynamics\nplot_time_series(time=time, time_series=pre_trace, ylabel=\"Trace value\", title=\"Pre trace\")\n# Plotting post trace dynamics\nplot_time_series(time=time, time_series=post_trace, ylabel=\"Trace value\", title=\"Post trace\")\n# Plotting weight dynamics\nplot_time_series(time=time, time_series=weights, ylabel=\"Weight value\", title=\"Weight dynamics\")\n``````\n\n``````\n[15]:\n``````\n\n``````\ndef extract_stdp_weight_changes(time, spikes_pre, spikes_post, wgt):\n    # Compute the weight changes for every weight change event\n    w_diff = np.zeros(wgt.shape)\n    w_diff[1:] = np.diff(wgt)\n\n    w_diff_non_zero = np.where(w_diff != 0)\n    dw = w_diff[w_diff_non_zero].tolist()\n\n    # Find the absolute time of every weight change event\n    time = np.array(time)\n    t_non_zero = time[w_diff_non_zero]\n\n    # Compute the difference between post and pre synaptic spike time for every weight change event\n    spikes_pre = np.array(spikes_pre)\n    spikes_post = np.array(spikes_post)\n    dt = []\n    for i in range(0, len(dw)):\n        time_stamp = t_non_zero[i]\n        t_post = (spikes_post[np.where(spikes_post <= time_stamp)])[-1]\n        t_pre = (spikes_pre[np.where(spikes_pre <= time_stamp)])[-1]\n        dt.append(t_post-t_pre)\n\n    return np.array(dt), np.array(dw)\n\ndef plot_stdp(time, spikes_pre, spikes_post, wgt,\n              on_pre_stdp, y1_impulse, y1_tau,\n              on_post_stdp, x1_impulse, x1_tau):\n    # Derive weight changes as a function of time differences\n    diff_t, diff_w = extract_stdp_weight_changes(time, spikes_pre, spikes_post, wgt)\n\n    # Derive learning rule coefficients\n    on_pre_stdp = eval(str(on_pre_stdp).replace(\"^\", \"**\"))\n    a_neg = on_pre_stdp * y1_impulse\n    on_post_stdp = eval(str(on_post_stdp).replace(\"^\", \"**\"))\n    a_pos = on_post_stdp * x1_impulse\n\n    # Derive x-axis limit (absolute value)\n    max_abs_dt = np.maximum(np.abs(np.max(diff_t)), np.abs(np.min(diff_t)))\n\n    # Derive x-axis for learning window computation (negative part)\n    x_neg = np.linspace(-max_abs_dt, 0, 1000)\n    # Derive learning window (negative part)\n    w_neg = a_neg * np.exp(x_neg / y1_tau)\n\n    # Derive x-axis for learning window computation (positive part)\n    x_pos = np.linspace(0, max_abs_dt, 1000)\n    # Derive learning window (positive part)\n    w_pos = a_pos * np.exp(- x_pos / x1_tau)\n\n    plt.figure(figsize=(10, 5))\n\n    plt.scatter(diff_t, diff_w, label=\"Weight changes\", color=\"b\")\n\n    plt.plot(x_neg, w_neg, label=\"W-\", color=\"r\")\n    plt.plot(x_pos, w_pos, label=\"W+\", color=\"g\")\n\n    plt.title(\"STDP weight changes - Learning window\")\n    plt.xlabel('t_post - t_pre')\n    plt.ylabel('Weight change')\n    plt.legend()\n    plt.grid()\n\n    plt.show()\n\n# Plot STDP window\nplot_stdp(time, np.where(pre_spikes[:, 0]), np.where(post_spikes[:, 0]), weights[:, 0],\n          on_pre_stdp, stdp.y1_impulse, stdp.x1_tau,\n          on_post_stdp, stdp.x1_impulse, stdp.y1_tau)\n``````"
  },
  {
    "url": "https://lava-nc.org/lava_api_documentation.html",
    "title": "Lava API Documentation — Lava  documentation",
    "content": "Welcome to Lava API documentation.\n\nContents:\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/lava_architecture_overview.html",
    "title": "Lava Architecture — Lava  documentation",
    "content": "Lava is an emerging open-source software framework for neuromorphic computing. Many parts of Lava are still under development. Therefore the following sections describe Lava’s key attributes and fundamental architectural concepts that are partly implemented and partly envisioned today. A deeper dive into these architectural concepts can be found in the ‘Getting started with Lava’ section. We hope to inspire and invite developers to contribute to this open-source effort to solve some of the key challenges of neuromorphic hardware, software and application development.\n\nAsynchronous parallelism:Lava is a software framework for asynchronous event-based processing for distributed, neuromorphic, heterogeneous, and massively parallel hardware platforms supporting edge to cloud based systems.\n\nRefinement:Lava facilitates iterative software development through refinement of abstract computational processes into architecture-specific implementations. This allows application development to start from high-level behavioral models that can be broken down successively into lower-level models optimized for different platforms.\n\nCross-platform:Lava supports flexible cross-platform execution of computational processes on novel neuromorphic architectures such as Intel Loihi as well as conventional CPU/GPU architectures. It is flexible in the dual sense of allowing the same computational processes to execute on different platforms while also allowing different processes to execute and interact across architectures through message passing.\n\nModular and composable:Lava’s computational processes follow a consistent architecture, to make them interoperable which allows to compose modular systems from other computational processes.\n\nExtensible:Lava is open and extensible to support use cases of increasing breadth over time and to interact with other third party frameworks such as TensorFlow, ROS, Brian and more.\n\nTrainable:Lava comes with powerful training algorithms to train models offline and continually online in real-time in the future.\n\nAccessible:Lava provides an intuitive Python API to quickly build and execute models on distributed parallel systems.\n\nAt a micro-scale, neuromorphic systems are brain-inspired. Brains consist of a large number of neurons and synapses that operate in parallel and communicate with each other with sparse asynchronous messages (spikes), which lead to large gains in computational efficiency.\n\nAt a macro-scale, neuromorphic hardware systems often involve multiple physical computing elements, ranging from special purpose neural accelerators to conventional CPU/GPUs, sensors, or actuator devices.\nNeuromorphic hardware systems and Lava mirror this general, massively parallel, heterogenous architecture from the ground up. All algorithms built in Lava are built from independent, modular computational processes that may execute on different hardware platforms and communicate through generalized message types.\n\nSo far, there is no single open software framework today that combines all of these architectural aspects in a coherent, easy to use, and performant fashion to pave the way for broader adoption of neuromorphic technologies.\n\nProcessesare the fundamental building block in the Lava architecture from which all algorithms and applications are built.Processesare stateful objects with internal variables, input and output ports for message-based communication via channels.Processescome in different forms. AProcesscan be as simple as a single neuron or as complex an entire neural network like a ResNet architecture, or an excitatory/inhibitory network.\nAProcesscould also represent regular program code like a search algorithm, a system management process or be used for data pre and post processing.\nIn addition, peripheral devices such as sensors or actuators can also be wrapped into theProcessabstraction to integrate them seamlessly into a Lava application alongside other computational processes.\nIn short, everything in Lava is aProcess, which has its own private memory and communicates with its environment solely via messages. This makes LavaProcessesa recursive programming abstraction from which modular, large-scale parallel applications can be built.\n\nProcessclasses themselves only define the interface of aProcessin terms of state variables, ports, and other class methods for otherProcessesto use or interact with any givenProcess. However,Processesdo not provide a behavioral implementation to make aProcessexecutable on a particular hardware architecture.\n\nThe behavioral implementation - i.e. a concrete implementation of variables, ports, channels and internal business logic - is provided via separateProcessModelclasses.Processescan have one or moreProcessModelsof the same or different types. We distinguish between two categories ofProcessModeltypes:\n\nSubProcessModels:The first wayProcessModelscan be implemented is in terms of otherProcesses. With aSubProcessModel, other sub-Processescan be instantiated, configured, and connected to define the behavior of the parent-Process. The ports of aSubProcessModelcan be connected to the ports of its sub-Processesand variables of sub-Processescan be exposed as variables of the parent-Processas needed to make them accessible in its environment. This type of refinement of a parent-Processby means of sub-Processescan be applied recursively and will result in a tree-structuredProcess/ProcessModelhierarchy which allows to build sophisticated and reusable application modules.\n\nLeafProcessModels:ProcessModelscan also be implemented directly as opposed to the composition of other sub-Processes. In this case, we refer to them asLeafProcessModelsbecause they form the leaves of the tree-structuredProcesshierarchy. SuchLeafProcessescan be implemented in a programming language such asPythonorC. For different hardware architectures, theLeafProcessModelcode either describes computations directly in terms of executable instructions for a von-Neumann processor, possibly using external libraries such as Numpy, TensorFlow or PyTorch. In contrast, compute resources like neuromorphic cores do not execute arbitrary sequences of instructions but gain computational advantages by executing through collective dynamics specified by structural descriptions. In this case, the code of correspondingNcProcessModelsfor such cores is responsible for the structural allocation and configuration of neural network resources such as axons, synapses, and neurons in a neuro core. In the future, LavaProcessesmay also model and specify the operation of analog neuromorphic chips with behavioral models that will only approximately match their real-time execution.\n\nIn general, in neuromorphic architectures computation emerges from collective dynamical processes that are often approximate, nondeterministic, and may vary in detailed execution from platform to platform. Therefore Lava views behavior asmodeledrather thanprogrammedorspecified, and this perspective motivates the nameProcessModel.\n\nFundamentally, allProcesseswithin a system or network operate in parallel and communicate asynchronously with each other through the exchange of message tokens. But many use cases require synchronization amongProcesses, for instance to implement a discrete-time dynamical system representing a particular neuron model progressing from one algorithmic time step to the next.\nLava allows developers to define synchronization protocols that describe howProcessesin the same synchronization domain synchronize with each other. ThisSyncProtocolis orchestrated by aSynchronizerwithin aSyncDomainwhich exchanges synchronization message tokens with allProcessesin aSyncDomain. The compiler either assignsProcessesautomatically to aSyncDomainbased on theSyncProtocolit implements but also allows users to assignProcessesmanually toSyncDomainsto customize synchronization amongProcessesin more detail.\n\nBesides implementing a specificProcessandSyncProtocol, aProcessModelhas a specific type (such asPyProcessModel,CProcessModel,SubProcessModel, etc.), has one or more resource requirements, and has one or more tags. Resource requirements specify what compute or peripheral resources aProcessModelmay require in order to execute, such as CPU, neuromorphic core, hard disk, or access to a camera. Furthermore, tags specify additional behavioral attributes of aProcessModel.\n\nIn order to execute a specific instance of aProcess, the compiler will select one of theProcessModelsthat implement a givenProcessclass. In order to allow for different selection strategies, the compiler delegates thisProcessModelselection to instances of a separateRunConfigclass. SuchRunConfigscorrespond to a set of rules that determine whichProcessModelto select for aProcessgiven user preferences and the resource requirements and tags of aProcessModel. For instance, a particularRunConfigmay always select the highest-level Python-based implementation of aProcessfor quick application prototyping on a conventional CPU or GPU architecture without physical access to neuromorphic systems like Intel’s Kapoho Bay or Pohohiki Springs. AnotherRunConfigmight prefer to mapProcessesto neuro cores or embedded CPUs whenever such neuromorphic systems are available. Lava will provide several pre-configuredRunConfigsbut allows users to customize them or create their own.\n\nIn summary, whileProcessesonly provide a universal interface to interact with their environment via message passing, one or moreProcessModelsare what implement the behavioral model of aProcessin different languages and ways tailored to different hardware architectures. This implementation can either be provided directly or by refining a givenProcessiteratively by implementing its behavior via sub-Processeswhich allows for code reuse and greater modularity.\n\nOverall, this programming model enables quick application prototyping at a high level, agnostic of the intricate constraints and complexities that are often associated with neuromorphic architectures, in the language of choice of a user, while deferring specifics about the hardware architecture for later. Once ready, high-level behavioral models can be replaced or refined by more efficient lower-level implementations (often provided by the Lava process library). At the same time, the availability of different behavioral implementations allows users to run the same application on different hardware platforms such as a CPU or a neuromorphic system when only one of them is available.\n\nProcessesare connected via ports with each other for message-based communication over channels. For hierarchicalProcesses, ports of a parent process can also be internally connected to ports of a childProcessand vice versa within aSubProcessModel.\nIn general, connections betweenProcessescan be feed-forward or recurrent and support branching and joining of connections.\nWhile ports at theProcess-level are only responsible for establishing connectivity betweenProcessesbefore compilation, the port implementation atProcessModel-level is responsible for actual message-passing at runtime.\n\nThe different kinds of ports and variables are part of the same formal class hierarchy. Both ports and variables are members of aProcess. As such, both ports and variables communicate or represent numeric data, characterized by a data type that must be specified when allocating them in aProcess. Additionally, port and variable implementations have an associated type and support a configurable precision of the numeric data they communicate or represent.\nThere are four main types of ports that can be grouped in two different ways:\n\nOutPortsconnect to and send messages toInPortsby value; meaning that theInPortreceives a copy of the data sent via theOutPort. Thus changes to that data on the sender or receiver side will not affect the other side, enabling safe parallel processing without side-effects.\n\nOn the other hand,RefPortsconnect toVarPortswhich act as a proxy to internal state variables of aProcess.RefPortsenable oneProcessto directly access a variable or internal memory of anotherProcessby reference as if it was its own. Such direct-memory access is generally very powerful, but also more dangerous as it can lead to unforeseen side effect in parallel programming and should therefore only be used with caution. Yet, sometimes it is necessary to achieve certain behaviors.\n\nAside from these main port types, there are additional virtual ports that effectively act as directives to the compiler to transform the shape of ports or how to combine multiple ports. Currently, Lava supportsReshapePortsandConcatPortsto change the shape of a port or to concatenate multiple ports into one.\nFinally, system-level communication betweenProcessessuch as for synchronization is also implemented via ports and channels, but those are not managed directly by – and  therefore are hidden from – the user.\n\nLava supports cross-platform execution of processes on a distributed set of compute nodes. Nodes in Lava have compute resources associated with them such as CPUs or neuro cores, and peripheral resources such as sensors, actuators or hard-disks. Given a graph ofProcesses, theirProcessModelsand aRunConfig, this allows the compiler to map theProcessesdefined in the user system process to one or moreNodeConfigurations. Depending on which type of node aProcessis mapped to, a differentProcessModelwith a node-specific implementation of its variables, ports, channels and behavior is chosen.\nIn the end, each node in aNodeConfigurationcan host one or moreSyncDomainswith one or moreProcModelsin it. Each suchSyncDomainalso contains a localRuntimeServiceprocess. TheRuntimeServiceis responsible for system management and includes theSynchronizerfor orchestrating theSyncProtocolof theSyncDomain. Irrespective of the presence of multipleSyncDomainson multiple nodes, all user-defined and system processes communicate seamlessly via one asynchronous message-passing backend with each other and the global Runtime within the user system process.\n\nThe core components of the Lava software stack are comprised of theCommunicating Sequential ProcessAPI, a powerful compiler, and a runtime. In combination, these components form theMagmalayer of Lava, which is the foundation on which newProcessesandProcessModelsare built.\nThe Lava process library provides a growing collection of generic, low-level, reusable, and widely applicableProcessesfrom which higher-level algorithm and application libraries are built.\n\nThe first libraries, to be released as part of the Lava software framework, are libraries for deep learning (lava-dl), optimization (lava-optim), and dynamic neural fields (lava-dnf). Future libraries will add support for vector symbolic architectures (lava-vsa) and evolutionary optimization (lava-evo).\n\nBesides these components, future releases of Lava will offer several utilities for application profiling, automatic float to fixed-point model conversion, network visualization, and more.\nFinally, Lava is open for extension to other third-party frameworks such as TensorFlow, ROS or Nengo.\nWe welcome open-source contributions to any of these future libraries and utilities.\n\n© Copyright 2021, Intel Corporation."
  },
  {
    "url": "https://lava-nc.org/optimization.html",
    "title": "Neuromorphic Constrained Optimization Library — Lava  documentation",
    "content": "A library of solvers that leverage neuromorphic hardware for\nconstrained optimization.\n\nConstrained optimization searches for the values of input variables that\nminimize or maximize a given objective function, while the variables are\nsubject to constraints. This kind of problem is ubiquitous throughout\nscientific domains and industries. Constrained optimization is a\npromising application for neuromorphic computing as it[naturally aligns\nwith the dynamics of spiking neural\nnetworks](https://doi.org/10.1109/JPROC.2021.3067593)naturally aligns\nwith the dynamics of spiking neural\nnetworks. When\nindividual neurons represent states of variables, the neuronal\nconnections can directly encode constraints between the variables: in\nits simplest form, recurrent inhibitory synapses connect neurons that\nrepresent mutually exclusive variable states, while recurrent excitatory\nsynapses link neurons representing reinforcing states. Implemented on\nmassively parallel neuromorphic hardware, such a spiking neural network\ncan simultaneously evaluate conflicts and cost functions involving many\nvariables, and update all variables accordingly. This allows a quick\nconvergence towards an optimal state. In addition, the fine-scale timing\ndynamics of SNNs allow them to readily escape from local minima.\n\nThis Lava repository currently supports solvers for the following\nconstrained optimization problems:\n\nQuadratic Programming (QP)\n\nQuadratic Unconstrained Binary Optimization (QUBO)\n\nAs we continue development, the library will support more constrained\noptimization problems that are relevant for robotics and operations\nresearch. We currently plan the following development order in such a\nway that new solvers build on the capabilities of existing ones:\n\nConstraint Satisfaction Problems (CSP) [problem interface already\navailable]\n\nInteger Linear Programming (ILP)\n\nMixed-Integer Linear Programming (MILP)\n\nMixed-Integer Quadratic Programming (MIQP)\n\nLinear Programming (LP)\n\nOverview_Solvers[](https://lava-nc.org/optimization.html#id1)\n\nMore formally, the general form of a constrained optimization problem\nis:\n\n\\displaystyle{\\min_{x} \\lbrace f(x) | g_i(x)    \\leq  b,    h_i(x)  = c.\\rbrace}\n\nWheref(x)is the objective function to be optimized whileg(x)andh(x)constrain the validity off(x)to\nregions in the state space satisfying the respective equality and\ninequality constraints. The vectorxcan be continuous, discrete\nor a mixture of both. We can then construct the following taxonomy of\noptimization problems according to the characteristics of the variable\ndomain and off,g, andh:\n\nimage[](https://lava-nc.org/optimization.html#id2)\n\nIn the long run, lava-optimization aims to offer support to solve all of\nthe problems in the figure with a neuromorphic backend.\n\nThe figure below shows the general architecture of the library. We\nharness the general definition of constraint optimization problems to\ncreate`OptimizationProblem`OptimizationProbleminstances by composing`Constraints`Constraints,`Variables`Variables, and`Cost`Costclasses which describe the characteristics\nof every subproblem class. Note that while a quadratic problem (QP) will\nbe described by linear equality and inequality constraints with\nvariables on the continuous domain and a quadratic function. A\nconstraint satisfaction problem (CSP) will be described by discrete\nconstraints, defined by variable subsets and a binary relation\ndescribing the mutually allowed values for such discrete variables and\nwill have a constant cost function with the pure goal of satisfying\nconstraints.\n\nAn API for every problem class can be created by inheriting from`OptimizationSolver`OptimizationSolverand composing particular flavors of`Constraints`Constraints,`Variables`Variables, and`Cost`Cost.\n\nimage[](https://lava-nc.org/optimization.html#id3)\n\nThe instance of an`Optimizationproblem`Optimizationproblemis the valid input for\ninstantiating the generic`OptimizationSolver`OptimizationSolverclass. In this way, the`OptimizationSolver`OptimizationSolverinterface is left fixed and the`OptimizationProblem`OptimizationProblemallows the greatest flexibility for creating new\nAPIs. Under the hood, the`OptimizationSolver`OptimizationSolverunderstands the\ncomposite structure of the`OptimizationProblem`OptimizationProblemand will in turn\ncompose the required solver components and Lava processes.\n\n[Solving\nLASSO.](https://github.com/lava-nc/lava-optimization/blob/release/v0.2.0/tutorials/tutorial_01_solving_lasso.ipynb)Solving\nLASSO.\n\n[Solving Maximum Independent\nSet.](https://github.com/lava-nc/lava-optimization/blob/release/v0.2.0/tutorials/tutorial_02_solving_qubos.ipynb)Solving Maximum Independent\nSet.\n\nCurrently, QP problems can be solved using the specific`QPSolver`QPSolver. In\nfuture releases, this will be merged with the generic API of`OptimizationSolver`OptimizationSolver(used in the next example).\n\nWorking installation of Lava, installed automatically with poetry\nbelow.[For custom installs see Lava installation\ntutorial.](https://github.com/lava-nc/lava/blob/main/tutorials/in_depth/tutorial01_installing_lava.ipynb)For custom installs see Lava installation\ntutorial.\n\nIf you use the Conda package manager, you can simply install the Lava\npackage via:\n\nAlternatively with intel numpy and scipy:\n\n© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nimportnumpyasnpfromlava.lib.optimization.problems.problemsimportQPfromlava.lib.optimization.solvers.qp.solverimportQPSolver# Define QP problemQ=np.array([[100,0,0],[0,15,0],[0,0,5]])p=np.array([[1,2,1]]).TA=-np.array([[1,2,2],[2,100,3]])k=-np.array([[-50,50]]).Tproblem=QP(Q,p,A,k)# Define hyper-parametersalpha,beta=0.001,1alpha_d,beta_g=10000,10000iterations=400# Solve using QPSolversolver=QPSolver(alpha=alpha,beta=beta,alpha_decay_schedule=alpha_d,beta_growth_schedule=beta_g)solver.solve(problem,iterations=iterations)\n``````\n\n``````\nimport numpy as np\nfrom lava.lib.optimization.problems.problems import QUBO\nfrom lava.lib.optimization.solvers.generic.solver import OptimizationSolver\n\n# Define QUBO problem\nq = np.array([[-5, 2, 4, 0],\n              [ 2,-3, 1, 0],\n              [ 4, 1,-8, 5],\n              [ 0, 0, 5,-6]]))\n\nqubo = QUBO(q)\n\n# Solve using generic OptimizationSolver\nsolver = OptimizationSolver(problem=qubo1)\nsolution = solver.solve(timeout=3000, target_cost=-50, backend=“Loihi2”)\n``````\n\n``````\ncd$HOMEgitclonegit@github.com:lava-nc/lava-optimization.gitcdlava-optimization\ncurl-sSLhttps://install.python-poetry.org|python3-\npoetryconfigvirtualenvs.in-projecttruepoetryinstallsource.venv/bin/activate\npytest\n``````\n\n``````\n# Commands using PowerShellcd$HOMEgitclonegit@github.com:lava-nc/lava-optimization.gitcdlava-optimizationpython3-mvenv.venv.venv\\Scripts\\activatepipinstall-Upipcurl-sSLhttps://install.python-poetry.org|python3-poetryconfigvirtualenvs.in-projecttruepoetryinstallpytest\n``````\n\n``````\ncondainstalllava-optimization-cconda-forge\n``````\n\n``````\ncondacreate-nlava-optimizationpython=3.9-cintel\ncondaactivatelava-optimization\ncondainstall-nlava-optimization-cintelnumpyscipy\ncondainstall-nlava-optimization-cconda-forgelava-optimization--freeze-installed\n``````"
  },
  {
    "url": "https://lava-nc.org/py-modindex.html",
    "title": "Python Module Index — Lava  documentation",
    "content": "© Copyright 2021, Intel Corporation.\n\n\n\n🛠️ Code Snippets:\n\n``````\nlava\n``````\n\n``````\nlava.lib\n``````\n\n``````\nlava.lib.dl.bootstrap.ann_sampler\n``````\n\n``````\nlava.lib.dl.bootstrap.block\n``````\n\n``````\nlava.lib.dl.bootstrap.block.base\n``````\n\n``````\nlava.lib.dl.bootstrap.block.cuba\n``````\n\n``````\nlava.lib.dl.bootstrap.routine\n``````\n\n``````\nlava.lib.dl.netx.blocks\n``````\n\n``````\nlava.lib.dl.netx.blocks.models\n``````\n\n``````\nlava.lib.dl.netx.blocks.process\n``````\n\n``````\nlava.lib.dl.netx.hdf5\n``````\n\n``````\nlava.lib.dl.netx.utils\n``````\n\n``````\nlava.lib.dl.slayer.auto\n``````\n\n``````\nlava.lib.dl.slayer.axon\n``````\n\n``````\nlava.lib.dl.slayer.axon.delay\n``````\n\n``````\nlava.lib.dl.slayer.axon.delta\n``````\n\n``````\nlava.lib.dl.slayer.block\n``````\n\n``````\nlava.lib.dl.slayer.block.adrf\n``````\n\n``````\nlava.lib.dl.slayer.block.adrf_iz\n``````\n\n``````\nlava.lib.dl.slayer.block.alif\n``````\n\n``````\nlava.lib.dl.slayer.block.base\n``````\n\n``````\nlava.lib.dl.slayer.block.cuba\n``````\n\n``````\nlava.lib.dl.slayer.block.rf\n``````\n\n``````\nlava.lib.dl.slayer.block.rf_iz\n``````\n\n``````\nlava.lib.dl.slayer.block.sigma_delta\n``````\n\n``````\nlava.lib.dl.slayer.classifier\n``````\n\n``````\nlava.lib.dl.slayer.dendrite\n``````\n\n``````\nlava.lib.dl.slayer.dendrite.sigma\n``````\n\n``````\nlava.lib.dl.slayer.io\n``````\n\n``````\nlava.lib.dl.slayer.loss\n``````\n\n``````\nlava.lib.dl.slayer.neuron\n``````\n\n``````\nlava.lib.dl.slayer.neuron.adrf\n``````\n\n``````\nlava.lib.dl.slayer.neuron.adrf_iz\n``````\n\n``````\nlava.lib.dl.slayer.neuron.alif\n``````\n\n``````\nlava.lib.dl.slayer.neuron.base\n``````\n\n``````\nlava.lib.dl.slayer.neuron.cuba\n``````\n\n``````\nlava.lib.dl.slayer.neuron.dropout\n``````\n\n``````\nlava.lib.dl.slayer.neuron.norm\n``````\n\n``````\nlava.lib.dl.slayer.neuron.rf\n``````\n\n``````\nlava.lib.dl.slayer.neuron.rf_iz\n``````\n\n``````\nlava.lib.dl.slayer.neuron.sigma_delta\n``````\n\n``````\nlava.lib.dl.slayer.spike\n``````\n\n``````\nlava.lib.dl.slayer.spike.complex\n``````\n\n``````\nlava.lib.dl.slayer.spike.spike\n``````\n\n``````\nlava.lib.dl.slayer.synapse\n``````\n\n``````\nlava.lib.dl.slayer.synapse.complex\n``````\n\n``````\nlava.lib.dl.slayer.synapse.layer\n``````\n\n``````\nlava.lib.dl.slayer.utils\n``````\n\n``````\nlava.lib.dl.slayer.utils.assistant\n``````\n\n``````\nlava.lib.dl.slayer.utils.filter\n``````\n\n``````\nlava.lib.dl.slayer.utils.int_utils\n``````\n\n``````\nlava.lib.dl.slayer.utils.quantize\n``````\n\n``````\nlava.lib.dl.slayer.utils.stats\n``````\n\n``````\nlava.lib.dl.slayer.utils.time\n``````\n\n``````\nlava.lib.dl.slayer.utils.utils\n``````\n\n``````\nlava.lib.dnf\n``````\n\n``````\nlava.lib.dnf.connect\n``````\n\n``````\nlava.lib.dnf.connect.connect\n``````\n\n``````\nlava.lib.dnf.connect.exceptions\n``````\n\n``````\nlava.lib.dnf.inputs\n``````\n\n``````\nlava.lib.dnf.inputs.gauss_pattern\n``````\n\n``````\nlava.lib.dnf.inputs.gauss_pattern.models\n``````\n\n``````\nlava.lib.dnf.inputs.gauss_pattern.process\n``````\n\n``````\nlava.lib.dnf.inputs.rate_code_spike_gen\n``````\n\n``````\nlava.lib.dnf.inputs.rate_code_spike_gen.models\n``````\n\n``````\nlava.lib.dnf.inputs.rate_code_spike_gen.process\n``````\n\n``````\nlava.lib.dnf.kernels\n``````\n\n``````\nlava.lib.dnf.kernels.kernels\n``````\n\n``````\nlava.lib.dnf.operations\n``````\n\n``````\nlava.lib.dnf.operations.enums\n``````\n\n``````\nlava.lib.dnf.operations.exceptions\n``````\n\n``````\nlava.lib.dnf.operations.operations\n``````\n\n``````\nlava.lib.dnf.operations.shape_handlers\n``````\n\n``````\nlava.lib.dnf.utils\n``````\n\n``````\nlava.lib.dnf.utils.convenience\n``````\n\n``````\nlava.lib.dnf.utils.math\n``````\n\n``````\nlava.lib.dnf.utils.plotting\n``````\n\n``````\nlava.lib.dnf.utils.validation\n``````\n\n``````\nlava.lib.optimization\n``````\n\n``````\nlava.lib.optimization.problems\n``````\n\n``````\nlava.lib.optimization.problems.bayesian\n``````\n\n``````\nlava.lib.optimization.problems.bayesian.models\n``````\n\n``````\nlava.lib.optimization.problems.bayesian.processes\n``````\n\n``````\nlava.lib.optimization.problems.coefficients\n``````\n\n``````\nlava.lib.optimization.problems.constraints\n``````\n\n``````\nlava.lib.optimization.problems.cost\n``````\n\n``````\nlava.lib.optimization.problems.problems\n``````\n\n``````\nlava.lib.optimization.problems.variables\n``````\n\n``````\nlava.lib.optimization.solvers\n``````\n\n``````\nlava.lib.optimization.solvers.bayesian\n``````\n\n``````\nlava.lib.optimization.solvers.bayesian.models\n``````\n\n``````\nlava.lib.optimization.solvers.bayesian.processes\n``````\n\n``````\nlava.lib.optimization.solvers.bayesian.solver\n``````\n\n``````\nlava.lib.optimization.solvers.generic\n``````\n\n``````\nlava.lib.optimization.solvers.generic.builder\n``````\n\n``````\nlava.lib.optimization.solvers.generic.cost_integrator\n``````\n\n``````\nlava.lib.optimization.solvers.generic.cost_integrator.models\n``````\n\n``````\nlava.lib.optimization.solvers.generic.cost_integrator.process\n``````\n\n``````\nlava.lib.optimization.solvers.generic.dataclasses\n``````\n\n``````\nlava.lib.optimization.solvers.generic.hierarchical_processes\n``````\n\n``````\nlava.lib.optimization.solvers.generic.monitoring_processes\n``````\n\n``````\nlava.lib.optimization.solvers.generic.monitoring_processes.solution_readout\n``````\n\n``````\nlava.lib.optimization.solvers.generic.monitoring_processes.solution_readout.models\n``````\n\n``````\nlava.lib.optimization.solvers.generic.monitoring_processes.solution_readout.process\n``````\n\n``````\nlava.lib.optimization.solvers.generic.processes\n``````\n\n``````\nlava.lib.optimization.solvers.generic.read_gate\n``````\n\n``````\nlava.lib.optimization.solvers.generic.read_gate.models\n``````\n\n``````\nlava.lib.optimization.solvers.generic.read_gate.process\n``````\n\n``````\nlava.lib.optimization.solvers.generic.scif\n``````\n\n``````\nlava.lib.optimization.solvers.generic.scif.models\n``````\n\n``````\nlava.lib.optimization.solvers.generic.scif.process\n``````\n\n``````\nlava.lib.optimization.solvers.generic.solver\n``````\n\n``````\nlava.lib.optimization.solvers.generic.sub_process_models\n``````\n\n``````\nlava.lib.optimization.solvers.qp\n``````\n\n``````\nlava.lib.optimization.utils\n``````\n\n``````\nlava.lib.optimization.utils.generators\n``````\n\n``````\nlava.lib.optimization.utils.generators.mis\n``````\n\n``````\nlava.lib.optimization.utils.solver_tuner\n``````\n\n``````\nlava.magma\n``````\n\n``````\nlava.magma.compiler\n``````\n\n``````\nlava.magma.compiler.builders\n``````\n\n``````\nlava.magma.compiler.builders.channel_builder\n``````\n\n``````\nlava.magma.compiler.builders.interfaces\n``````\n\n``````\nlava.magma.compiler.builders.py_builder\n``````\n\n``````\nlava.magma.compiler.builders.runtimeservice_builder\n``````\n\n``````\nlava.magma.compiler.channel_map\n``````\n\n``````\nlava.magma.compiler.channels\n``````\n\n``````\nlava.magma.compiler.channels.interfaces\n``````\n\n``````\nlava.magma.compiler.channels.pypychannel\n``````\n\n``````\nlava.magma.compiler.compiler\n``````\n\n``````\nlava.magma.compiler.compiler_graphs\n``````\n\n``````\nlava.magma.compiler.compiler_utils\n``````\n\n``````\nlava.magma.compiler.exceptions\n``````\n\n``````\nlava.magma.compiler.executable\n``````\n\n``````\nlava.magma.compiler.mappable_interface\n``````\n\n``````\nlava.magma.compiler.mapper\n``````\n\n``````\nlava.magma.compiler.node\n``````\n\n``````\nlava.magma.compiler.subcompilers\n``````\n\n``````\nlava.magma.compiler.subcompilers.address\n``````\n\n``````\nlava.magma.compiler.subcompilers.channel_builders_factory\n``````\n\n``````\nlava.magma.compiler.subcompilers.channel_map_updater\n``````\n\n``````\nlava.magma.compiler.subcompilers.constants\n``````\n\n``````\nlava.magma.compiler.subcompilers.exceptions\n``````\n\n``````\nlava.magma.compiler.subcompilers.interfaces\n``````\n\n``````\nlava.magma.compiler.subcompilers.py\n``````\n\n``````\nlava.magma.compiler.subcompilers.py.pyproc_compiler\n``````\n\n``````\nlava.magma.compiler.utils\n``````\n\n``````\nlava.magma.compiler.var_model\n``````\n\n``````\nlava.magma.core\n``````\n\n``````\nlava.magma.core.callback_fx\n``````\n\n``````\nlava.magma.core.decorator\n``````\n\n``````\nlava.magma.core.learning\n``````\n\n``````\nlava.magma.core.learning.constants\n``````\n\n``````\nlava.magma.core.learning.learning_rule\n``````\n\n``````\nlava.magma.core.learning.learning_rule_applier\n``````\n\n``````\nlava.magma.core.learning.product_series\n``````\n\n``````\nlava.magma.core.learning.random\n``````\n\n``````\nlava.magma.core.learning.string_symbols\n``````\n\n``````\nlava.magma.core.learning.symbolic_equation\n``````\n\n``````\nlava.magma.core.learning.utils\n``````\n\n``````\nlava.magma.core.model\n``````\n\n``````\nlava.magma.core.model.interfaces\n``````\n\n``````\nlava.magma.core.model.model\n``````\n\n``````\nlava.magma.core.model.py\n``````\n\n``````\nlava.magma.core.model.py.connection\n``````\n\n``````\nlava.magma.core.model.py.model\n``````\n\n``````\nlava.magma.core.model.py.neuron\n``````\n\n``````\nlava.magma.core.model.py.ports\n``````\n\n``````\nlava.magma.core.model.py.type\n``````\n\n``````\nlava.magma.core.model.spike_type\n``````\n\n``````\nlava.magma.core.model.sub\n``````\n\n``````\nlava.magma.core.model.sub.model\n``````\n\n``````\nlava.magma.core.process\n``````\n\n``````\nlava.magma.core.process.connection\n``````\n\n``````\nlava.magma.core.process.interfaces\n``````\n\n``````\nlava.magma.core.process.message_interface_enum\n``````\n\n``````\nlava.magma.core.process.neuron\n``````\n\n``````\nlava.magma.core.process.ports\n``````\n\n``````\nlava.magma.core.process.ports.exceptions\n``````\n\n``````\nlava.magma.core.process.ports.ports\n``````\n\n``````\nlava.magma.core.process.ports.reduce_ops\n``````\n\n``````\nlava.magma.core.process.process\n``````\n\n``````\nlava.magma.core.process.variable\n``````\n\n``````\nlava.magma.core.resources\n``````\n\n``````\nlava.magma.core.run_conditions\n``````\n\n``````\nlava.magma.core.run_configs\n``````\n\n``````\nlava.magma.core.sync\n``````\n\n``````\nlava.magma.core.sync.domain\n``````\n\n``````\nlava.magma.core.sync.protocol\n``````\n\n``````\nlava.magma.core.sync.protocols\n``````\n\n``````\nlava.magma.core.sync.protocols.async_protocol\n``````\n\n``````\nlava.magma.core.sync.protocols.loihi_protocol\n``````\n\n``````\nlava.magma.runtime\n``````\n\n``````\nlava.magma.runtime.message_infrastructure\n``````\n\n``````\nlava.magma.runtime.message_infrastructure.factory\n``````\n\n``````\nlava.magma.runtime.message_infrastructure.message_infrastructure_interface\n``````\n\n``````\nlava.magma.runtime.message_infrastructure.multiprocessing\n``````\n\n``````\nlava.magma.runtime.message_infrastructure.nx\n``````\n\n``````\nlava.magma.runtime.mgmt_token_enums\n``````\n\n``````\nlava.magma.runtime.runtime\n``````\n\n``````\nlava.magma.runtime.runtime_services\n``````\n\n``````\nlava.magma.runtime.runtime_services.channel_broker\n``````\n\n``````\nlava.magma.runtime.runtime_services.channel_broker.channel_broker\n``````\n\n``````\nlava.magma.runtime.runtime_services.enums\n``````\n\n``````\nlava.magma.runtime.runtime_services.interfaces\n``````\n\n``````\nlava.magma.runtime.runtime_services.runtime_service\n``````\n\n``````\nlava.proc\n``````\n\n``````\nlava.proc.conv\n``````\n\n``````\nlava.proc.conv.models\n``````\n\n``````\nlava.proc.conv.process\n``````\n\n``````\nlava.proc.conv.utils\n``````\n\n``````\nlava.proc.dense\n``````\n\n``````\nlava.proc.dense.models\n``````\n\n``````\nlava.proc.dense.process\n``````\n\n``````\nlava.proc.io\n``````\n\n``````\nlava.proc.io.dataloader\n``````\n\n``````\nlava.proc.io.encoder\n``````\n\n``````\nlava.proc.io.reset\n``````\n\n``````\nlava.proc.io.sink\n``````\n\n``````\nlava.proc.io.source\n``````\n\n``````\nlava.proc.learning_rules\n``````\n\n``````\nlava.proc.learning_rules.r_stdp_learning_rule\n``````\n\n``````\nlava.proc.learning_rules.stdp_learning_rule\n``````\n\n``````\nlava.proc.lif\n``````\n\n``````\nlava.proc.lif.models\n``````\n\n``````\nlava.proc.lif.process\n``````\n\n``````\nlava.proc.monitor\n``````\n\n``````\nlava.proc.monitor.models\n``````\n\n``````\nlava.proc.monitor.process\n``````\n\n``````\nlava.proc.receiver\n``````\n\n``````\nlava.proc.receiver.models\n``````\n\n``````\nlava.proc.receiver.process\n``````\n\n``````\nlava.proc.sdn\n``````\n\n``````\nlava.proc.sdn.models\n``````\n\n``````\nlava.proc.sdn.process\n``````\n\n``````\nlava.proc.spiker\n``````\n\n``````\nlava.proc.spiker.models\n``````\n\n``````\nlava.proc.spiker.process\n``````\n\n``````\nlava.utils\n``````\n\n``````\nlava.utils.dataloader\n``````\n\n``````\nlava.utils.dataloader.mnist\n``````\n\n``````\nlava.utils.profiler\n``````\n\n``````\nlava.utils.system\n``````\n\n``````\nlava.utils.weightutils\n``````"
  },
  {
    "url": "https://lava-nc.org/search.html",
    "title": "Search — Lava  documentation",
    "content": "Please activate JavaScript to enable the search functionality.\n\n© Copyright 2021, Intel Corporation."
  }
]